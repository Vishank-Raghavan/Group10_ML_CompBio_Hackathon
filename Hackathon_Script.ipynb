{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpYk4dHcJyxN"
   },
   "source": [
    "   # Group 10 Traditional Machine Learning Notebook\n",
    "Within each cycle of active learning, you can:\n",
    "\n",
    "1. Collect training data (original training data + your query data).\n",
    "\n",
    "2. Train a prediction model to predict the DMS_score for each mutant (e.g., M0A).\n",
    "\n",
    "3. Use the trained model to predict the score for all mutant in the test set.\n",
    "\n",
    "4. Select query mutants for next round based on certain criteria. You may want to make sure you don't query the same mutant twice as you only have a limited chances of making queries in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PDuz5mihLReY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import rankdata\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9WI5oTTKdIY"
   },
   "source": [
    "## 1. collect training data\n",
    "\n",
    "Upload `sequence.fasta`, `train.csv`, and `test.csv` to the current runtime:\n",
    "\n",
    "1. click the folder icon on the left\n",
    "\n",
    "2. click the upload icon and upload the files to the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "Tj-TUAeZLEUz",
    "outputId": "88a181fb-fa9f-4954-caab-f7342a9d0b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLREKMRRRLESGDKWFSLEFFPPRTAEGAVNLISRFDRMAAGGPLYIDVTWHPAGDPGSDKETSSMMIASTAVNYCGLETILHMTCCRQRLEEITGHLHKAKQLGLKNIMALRGDPIGDQWEEEEGGFNYAVDLVKHIRSEFGDYFDICVAGYPKGHPEAGSFEADLKHLKEKVSAGADFIITQLFFEADTFFRFVKACTDMGITCPIVPGIFPIQGYHSLRQLVKLSKLEVPQEIKDVIEPIKDNDAAIRNYGIELAVSLCQELLASGLVPGLHFYTLNREMATTEVLKRLGMWTEDPRRPLPWALSAHPKRREEDVRPIFWASRPKSYIYRTQEWDEFPNGRWGNSSSPAFGELKDYYLFYLKSKSPKEELLKMWGEELTSEESVFEVFVLYLSGEPNRNGHKVTCLPWNDEPLAAETSLLKEELLRVNRQGILTINSQPNINGKPSSDPIVGWGPSGGYVFQKAYLEFFTSRETAEALLQVLKKYELRVNYHLVNVKGENITNAPELQPNAVTWGIFPGREIIQPTVVDPVSFMFWKDEAFALWIERWGKLYEEESPSRTIIQYIHDNYFLVNLVDNDFPLDNCLWQVVEDTLELLNRPTQNARETEAP'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sequence.fasta', 'r') as f:\n",
    "  data = f.readlines()\n",
    "\n",
    "sequence_wt = data[1].strip()\n",
    "sequence_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dewLzhLYMUSJ",
    "outputId": "f2c11453-a976-4778-f30d-57321d41e5d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N-tkTaqtK9AD"
   },
   "outputs": [],
   "source": [
    "def get_mutated_sequence(mut, sequence_wt):\n",
    "  wt, pos, mt = mut[0], int(mut[1:-1]), mut[-1]\n",
    "\n",
    "  sequence = deepcopy(sequence_wt)\n",
    "\n",
    "  return sequence[:pos]+mt+sequence[pos+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "bZH3YKNVyR-m",
    "outputId": "efff26ca-ec20-4bdc-9473-c3ebe01e996d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>DMS_score</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0Y</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0W</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0V</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M0T</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M0S</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>R593P</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>K596A</td>\n",
       "      <td>0.879648</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>Y610A</td>\n",
       "      <td>0.721494</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>Y610T</td>\n",
       "      <td>0.783082</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>N619A</td>\n",
       "      <td>0.885302</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mutant  DMS_score                                           sequence\n",
       "0       M0Y   0.273000  YVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1       M0W   0.285700  WVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "2       M0V   0.215300  VVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "3       M0T   0.312200  TVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "4       M0S   0.218000  SVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "...     ...        ...                                                ...\n",
       "1335  R593P   0.806500  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1336  K596A   0.879648  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1337  Y610A   0.721494  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1338  Y610T   0.783082  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1339  N619A   0.885302  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "\n",
       "[1340 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['sequence'] = df_train.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tqfIASlMLQe4",
    "outputId": "8a13634d-f9cd-4e3d-e7fb-efa11438e6a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1D</td>\n",
       "      <td>MDNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V1Y</td>\n",
       "      <td>MYNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V1C</td>\n",
       "      <td>MCNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1A</td>\n",
       "      <td>MANEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V1E</td>\n",
       "      <td>MENEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11319</th>\n",
       "      <td>P655S</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11320</th>\n",
       "      <td>P655T</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>P655V</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>P655A</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>P655W</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11324 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant                                           sequence\n",
       "0        V1D  MDNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1        V1Y  MYNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "2        V1C  MCNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "3        V1A  MANEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "4        V1E  MENEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "...      ...                                                ...\n",
       "11319  P655S  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11320  P655T  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11321  P655V  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11322  P655A  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11323  P655W  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "\n",
       "[11324 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test['sequence'] = df_test.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8hiStmfLXz6"
   },
   "outputs": [],
   "source": [
    "# TODO: integrate the query data that you acquired each round into df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cty7BGBLdgp"
   },
   "source": [
    "## 2. Train a prediction model\n",
    "\n",
    "Here, we provided a linear regression model and used one-hot encoding to encode each variant. You would need to build your own model to achieve better performances.\n",
    "\n",
    "Hint: you can perform cross-validation on the training set to evaluate your predictor before making predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N4rvH-HzOmN_"
   },
   "outputs": [],
   "source": [
    "'''hyperparameters'''\n",
    "\n",
    "seq_length = 656\n",
    "seed = 0 # seed for splitting the validation set\n",
    "val_ratio = 0.2 # proportion of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bg2fQKEKLsTJ"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df, istrain=True):\n",
    "\n",
    "        alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        map_a2i = {j:i for i,j in enumerate(alphabet)}\n",
    "        map_i2a = {i:j for i,j in enumerate(alphabet)}\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        self.num_samples = len(self.df)\n",
    "        self.seq_length = len(self.df.sequence.values[0])\n",
    "        self.num_channels = 20\n",
    "\n",
    "        # TODO: replace one-hot encodings with your own encodings\n",
    "        self.encodings = np.zeros((self.num_samples, self.num_channels, self.seq_length)).astype(np.float32)\n",
    "        self.targets = np.zeros(self.num_samples).astype(np.float32)\n",
    "\n",
    "        if istrain:\n",
    "          for it, (seq,target) in enumerate(self.df[['sequence', 'DMS_score']].values):\n",
    "              for i,aa in enumerate(seq):\n",
    "                  self.encodings[it,map_a2i[aa],i] = 1\n",
    "              self.targets[it] = target\n",
    "\n",
    "          self.encodings = self.encodings.astype(np.float32)\n",
    "          self.targets = self.targets.astype(np.float32)\n",
    "        else:\n",
    "          for it, seq in enumerate(self.df['sequence'].values):\n",
    "              for i,aa in enumerate(seq):\n",
    "                  self.encodings[it,map_a2i[aa],i] = 1\n",
    "\n",
    "          self.encodings = self.encodings.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encodings[idx]), torch.tensor(self.targets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#OHE with the entire sequence on one line(every 20 positions the AA position changes)\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df, istrain=True):\n",
    "\n",
    "        alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        map_a2i = {j:i for i,j in enumerate(alphabet)}\n",
    "        map_i2a = {i:j for i,j in enumerate(alphabet)}\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        self.num_samples = len(self.df)\n",
    "        self.seq_length = len(self.df.sequence.values[0])\n",
    "        self.num_channels = 20\n",
    "\n",
    "        # TODO: replace one-hot encodings with your own encodings\n",
    "        self.encodings = np.zeros((self.num_samples, self.num_channels * self.seq_length)).astype(np.float32)\n",
    "        self.targets = np.zeros(self.num_samples).astype(np.float32)\n",
    "\n",
    "        if istrain:\n",
    "          for it, (seq,target) in enumerate(self.df[['sequence', 'DMS_score']].values):\n",
    "              for i,aa in enumerate(seq):\n",
    "                  self.encodings[it,map_a2i[aa] + (20*i)] = 1\n",
    "              self.targets[it] = target\n",
    "\n",
    "          self.encodings = self.encodings.astype(np.float32)\n",
    "          self.targets = self.targets.astype(np.float32)\n",
    "        else:\n",
    "          for it, seq in enumerate(self.df['sequence'].values):\n",
    "              for i,aa in enumerate(seq):\n",
    "                  self.encodings[it,map_a2i[aa] + (20*i)] = 1\n",
    "\n",
    "          self.encodings = self.encodings.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encodings[idx]), torch.tensor(self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UERT_WBPOgOk"
   },
   "outputs": [],
   "source": [
    "train_dataset = ProteinDataset(df_train)\n",
    "test_dataset = ProteinDataset(df_test, istrain=False)\n",
    "\n",
    "# split validation set\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=val_ratio, random_state=seed, shuffle=True)\n",
    "\n",
    "# TODO: revise according to your own model\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, embed_dim=20000, num_heads=4, num_layers=3, ff_dim=10000):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.embedding = nn.Linear(13120, embed_dim)  # Project one-hot input to embedding space\n",
    "        # TODO: transformer block using TransformerEncoderLayer\n",
    "        #####\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        #####\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        #self.linear = nn.Linear(embed_dim,1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        # TODO: forward part of the transformer block\n",
    "        #####\n",
    "        x = self.encoder(x)\n",
    "        #####\n",
    "        #x = x.permute(0, 2, 1)\n",
    "        x = self.pooling(x).squeeze(-1)  # (batch, embed_dim)\n",
    "        #x = self.linear(x)\n",
    "        #x = x.max(dim=1).values\n",
    "        return (self.tanh(x) + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFRegressor(nn.Module):\n",
    "    def __init__(self, start_dim=13120):\n",
    "        super(FFRegressor, self).__init__()\n",
    "        self.linear1 = nn.Linear(start_dim, 1312)\n",
    "        self.linear2 = nn.Linear(1312, 1)\n",
    "        self.relu= nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        return (x)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(model, data):\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for row in data:\n",
    "        seq,fitness = row\n",
    "        seq = seq.to(device)\n",
    "        fitness = fitness.to(device)\n",
    "        preds.append(model(seq))\n",
    "        truths.append(ranks)\n",
    "    mse_list = [(x - y)**2 for x,y in zip(truths, preds)]\n",
    "    mse = sum(mse_list)/len(mse_list)\n",
    "    return (mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor(model, data):\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for row in data:\n",
    "        seq,fitness,ranks = row\n",
    "        seq = seq.to(device)\n",
    "        fitness = fitness.to(device)\n",
    "        preds.append(model(seq))\n",
    "        truths.append(fitness)\n",
    "    cor = spearmanr(preds, truths)\n",
    "    return (cor.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs=100, batch_size=256, lr=1e-3, patience=10, device='cuda:0'):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_mse = 10000\n",
    "    patience_counter = 0\n",
    "    best_ckpt = None\n",
    "    best_cor = -1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_epoch = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        #truths, preds = [], []\n",
    "\n",
    "        for sequences, fitness, ranks in train_loader:\n",
    "            # TODO: backpropagation\n",
    "            #####\n",
    "            model = model.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            fitness = fitness.to(device)\n",
    "            ranks = ranks.to(device)\n",
    "            outputs = model(ranks)\n",
    "            outputs = outputs.to(device)\n",
    "            loss = criterion(outputs, ranks)\n",
    "             #zero's grad for backprogation\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #backpropagates\n",
    "            loss.backward()\n",
    "\n",
    "            #steps forward\n",
    "            optimizer.step()\n",
    "            #####\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        #avg_loss = total_loss/batch_size\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "#         truth, preds = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for sequences, fitness in val_loader:\n",
    "#                 # TODO: model inference\n",
    "#                 #####\n",
    "#                 model = model.to(device)\n",
    "#                 sequences = sequences.to(device)\n",
    "#                 fitness = fitness.to(device)\n",
    "#                 outputs = model(sequences)\n",
    "#                 outputs = outputs.to(device)\n",
    "#                 truth.append(fitness)\n",
    "#                 preds.append(outputs)\n",
    "#                 #####\n",
    "\n",
    "        model = model.to(device)\n",
    "        val_cor = cor(model, val_dataset)\n",
    "        val_cor  = float(val_cor)\n",
    "        end_epoch = time.time()\n",
    "        #print(total_loss, train_corr.statistic, val_corr.statistic)\n",
    "        print(f'Epoch [{epoch+1} / {epochs}]: Train Loss={avg_loss:.4f}, Val MSE={val_mse:.4f}, Time={end_epoch - start_epoch:.4f} sec')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_cor > best_cor:\n",
    "            best_cor = val_cor\n",
    "            best_ckpt = deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model, best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinDataset(df_train)\n",
    "test_dataset = ProteinDataset(df_test, istrain=False)\n",
    "\n",
    "# split validation set\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=val_ratio, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FFRegressor().to(device)\n",
    "model, best_ckpt = train_model(model, train_dataset, val_dataset, epochs=500, batch_size=256, lr=1e-3, patience=10, device=device)\n",
    "model.load_state_dict(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 13120])\n",
      "Epoch [1 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0216 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [2 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0205 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [3 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0202 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [4 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0197 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [5 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0197 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [6 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0196 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [7 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0195 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [8 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0193 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [9 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0192 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [10 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0191 sec\n",
      "torch.Size([256, 13120])\n",
      "Epoch [11 / 500]: Train Loss=0.0000, Val MSE=0.1688, Time=0.0194 sec\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FFRegressor().to(device)\n",
    "model, best_ckpt = train_model(model, train_dataset, val_dataset, epochs=500, batch_size=256, lr=1e-3, patience=10, device=device)\n",
    "model.load_state_dict(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007279735479372176"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(preds, truths).statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060791107887777106"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(train_preds, train_truths).statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/vraghavan40/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import esm\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "#from autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "def gen_emb(fasta_file, out_dir='esm_embeddings_test', device='cuda:0'):\n",
    "    records = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "    names = [rec.id for rec in records]\n",
    "    sequences = [str(rec.seq) for rec in records]\n",
    "    print(f'Number of sequences: {len(sequences)}')\n",
    "\n",
    "    data = [(name, seq) for name, seq in zip(names, sequences)]\n",
    "\n",
    "    # TODO: Load ESM-2 model (esm2_t33_650M_UR50D) and batch converter\n",
    "    #####\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    #####\n",
    "    model.to(device)\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    batch_size = 2 # Reduce if you are running out of cuda memory\n",
    "    num_batches = int(np.ceil(len(data) / batch_size))\n",
    "\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        batch = data[i * batch_size:(i + 1) * batch_size]\n",
    "        names_batch, seqs_batch = zip(*batch)\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(batch)\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        # Extract per-residue representations (on CPU)\n",
    "        with torch.no_grad():\n",
    "            # TODO: inference\n",
    "            #####\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "            #####\n",
    "        # TODO: get per-residue representations\n",
    "        #####\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        #####\n",
    "        # Generate per-sequence representations via averaging\n",
    "        for k, tokens_len in enumerate(batch_lens):\n",
    "            seq_name = names_batch[k]\n",
    "            seq_tokens = token_representations[k, :tokens_len]\n",
    "            seq_mean = seq_tokens.mean(0)\n",
    "            save = {'mean_representations': {33: seq_mean}}\n",
    "            torch.save(save, os.path.join(out_dir, f'{seq_name}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta_list = []\n",
    "for i,seq in enumerate(df_train[\"sequence\"]):\n",
    "    train_fasta_list.append(f\">seq_{i}\\n\")\n",
    "    train_fasta_list.append(seq + \"\\n\")\n",
    "train_fasta_string = \"\".join(train_fasta_list)\n",
    "with open(\"train_fasta.fa\", 'w') as file:\n",
    "    file.write(train_fasta_string)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasta_list = []\n",
    "for row in df_test.itertuples():\n",
    "    test_fasta_list.append(f\">{row.mutant}\\n\")\n",
    "    test_fasta_list.append(row.sequence + \"\\n\")\n",
    "test_fasta_string = \"\".join(test_fasta_list)\n",
    "with open(\"test_fasta.fa\", 'w') as file:\n",
    "    file.write(test_fasta_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 670/670 [01:20<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_emb('train_fasta.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 11324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5662/5662 [08:59<00:00, 10.50it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_emb('test_fasta.fa', out_dir = \"esm_embeddings_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Dataset class for new encoding type\n",
    "class ProteinESMDataset(Dataset):\n",
    "    def __init__(self, sequences, seq2name, emb_dir, labels, fitness2idx):\n",
    "        super().__init__()\n",
    "        self.labels = [fitness2idx.get(fitness, -1) for fitness in labels]\n",
    "        self.embeddings = []\n",
    "        for seq in tqdm(sequences, desc='Loading esm embeddings'):\n",
    "            name = seq2name[seq]\n",
    "            emb_file = os.path.join(emb_dir, f'{name}.pt')\n",
    "            emb = torch.load(emb_file)['mean_representations'][33]\n",
    "            self.embeddings.append(emb)\n",
    "        self.ranks = list(rankdata(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        emb = self.embeddings[index]\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        rank = torch.tensor(self.ranks[index], dtype=torch.float32)\n",
    "        return emb, label, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = df_train['sequence']\n",
    "fitness_list = df_train['DMS_score'].tolist()\n",
    "\n",
    "seq_train, seq_val, fitness_train, fitness_val = train_test_split(sequences, fitness_list, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq2name = train_seq2name = {seq: f'seq_{i}' for i, seq in enumerate(sequences)}\n",
    "fitness2idx = {fitness: fitness for idx, fitness in enumerate(fitness_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 1072/1072 [00:13<00:00, 78.37it/s] \n",
      "Loading esm embeddings: 100%|██████████| 268/268 [00:01<00:00, 214.72it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dir = 'esm_embeddings_test'\n",
    "train_dataset = ProteinESMDataset(seq_train, train_seq2name, emb_dir, fitness_train, fitness2idx)\n",
    "val_dataset = ProteinESMDataset(seq_val, train_seq2name, emb_dir, fitness_val, fitness2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_esm(model, data):\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for row in data:\n",
    "        esm,fitness, ranks = row\n",
    "        esm = esm.to(device)\n",
    "        fitness = fitness.to(device)\n",
    "        ranks = ranks.to(device)\n",
    "        preds.append(model(esm.unsqueeze(0)))\n",
    "        truths.append(fitness)\n",
    "    mse_list = [(x - y)**2 for x,y in zip(truths, preds)]\n",
    "    mse = sum(mse_list)/len(mse_list)\n",
    "    return (mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_esm(model, data):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for row in data:\n",
    "        esm,fitness, ranks = row\n",
    "        esm = esm.to(device)\n",
    "        fitness = fitness.to(device)\n",
    "        ranks = ranks.to(device)\n",
    "        preds.append(model(esm.unsqueeze(0)))\n",
    "        truths.append(fitness)\n",
    "    preds = [float(x) for x in preds]\n",
    "    truths = [float(x) for x in truths]\n",
    "    cor = spearmanr(preds, truths)\n",
    "    return (cor.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=25600):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        # TODO: linear layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.fc3 = nn.Linear(hidden_dim//2, hidden_dim//4)\n",
    "        self.fc4 = nn.Linear(hidden_dim//4, hidden_dim//8)\n",
    "        self.fc5 = nn.Linear(hidden_dim//8, hidden_dim//16)\n",
    "        self.fc6 = nn.Linear(hidden_dim//16, hidden_dim//32)\n",
    "        self.fc7 = nn.Linear(hidden_dim//32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward function\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc7(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=128):\n",
    "        super(RNNRegressor, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, 4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward function\n",
    "        #h0 = torch.zeros(1, x.size(0), 12800).to(x.device)\n",
    "        x,_ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, embed_dim=1280, num_heads=4, num_layers=4, ff_dim=128):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        #####\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(32)  # Global average pooling\n",
    "        #self.linear = nn.Linear(embed_dim,1)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As written this code is broken(won't train due to dimensonality issues related to batch size)\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, embed_dim=1280, num_filters=128, kernel_size=2, num_layers=2):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if (i > 0):\n",
    "                self.conv_layers.append(nn.Conv1d(num_filters, num_filters, kernel_size))\n",
    "            else:\n",
    "                self.conv_layers.append(nn.Conv1d(embed_dim, num_filters, kernel_size))\n",
    "        self.fc = nn.Linear(num_filters, 1)\n",
    "    def forward(self, x):\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MultiHeadAttentionRegressor(nn.Module):\n",
    "    def __init__(self, embed_dim=1280, num_heads=8, num_layers=4, ff_dim=128):\n",
    "        super(MultiHeadAttentionRegressor, self).__init__()\n",
    "        self.attention_layers = []\n",
    "        self.attention_layers = nn.ModuleList([nn.MultiheadAttention(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.linear_1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.dropout_1 = nn.Dropout(0.1)\n",
    "        self.linear_2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout_2 = nn.Dropout(0.1)\n",
    "        self.norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm_2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        #self.pooling = nn.AdaptiveAvgPool1d(32)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.attention_layers:\n",
    "            x2,_ = layer(x,x,x)\n",
    "            x = x + self.dropout_1(x2)\n",
    "            x = self.norm_1(x)\n",
    "            x2 = self.linear_2(self.dropout_1(F.relu(self.linear_1(x))))\n",
    "            x = x + self.dropout_2(x2)\n",
    "            x = self.norm_2(x)\n",
    "        #x = self.pooling(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains models using ESM embeddings\n",
    "def train_model_ESM(model, train_dataset, val_dataset, epochs=100, batch_size=256, lr=1e-3, patience=10, device='cuda:0'):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.MarginRankingLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_mse = 100000000000\n",
    "    patience_counter = 0\n",
    "    best_ckpt = None\n",
    "    total_loss = 0\n",
    "    best_loss = 100000000000\n",
    "    best_cor = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for sequences, fitness, ranks in train_loader:\n",
    "            #print(sequences.shape, fitness.shape, ranks.shape)\n",
    "            model = model.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            fitness = fitness.to(device)\n",
    "            ranks = ranks.to(device)\n",
    "            outputs = model(sequences)\n",
    "            outputs = outputs.to(device)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            y = []\n",
    "            for a,b in zip(outputs, fitness):\n",
    "                if (b>a):\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(-1)\n",
    "            y_tens = torch.tensor(y, dtype=torch.float32)\n",
    "            y_tens = y_tens.to(device)\n",
    "            loss = criterion(outputs, fitness, y_tens)\n",
    "            #loss = criterion(outputs, fitness)\n",
    "            #zero's grad for backprogation\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #backpropagates\n",
    "            loss.backward()\n",
    "\n",
    "            #steps forward\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "#         truth, preds = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for sequences, fitness in val_loader:\n",
    "#                 # TODO: model inference\n",
    "#                 #####\n",
    "#                 model = model.to(device)\n",
    "#                 sequences = sequences.to(device)\n",
    "#                 fitness = fitness.to(device)\n",
    "#                 outputs = model(sequences)\n",
    "#                 outputs = outputs.to(device)\n",
    "#                 truth.append(fitness)\n",
    "#                 preds.append(outputs)\n",
    "#                 #####\n",
    "\n",
    "        model = model.to(device)\n",
    "        #print(val_dataset.__getitem__(0)[0].shape)\n",
    "        #val_mse = mse_esm(model, val_dataset)\n",
    "        val_cor = cor_esm(model, val_dataset)\n",
    "        val_cor = float(val_cor)\n",
    "        #val_mse = float(val_mse)\n",
    "        end_epoch = time.time()\n",
    "        #print(total_loss, train_corr.statistic, val_corr.statistic)\n",
    "        print(f'Epoch [{epoch+1} / {epochs}]: Train Loss={total_loss:.4f}, Val Cor={val_cor:.4f}, Time={end_epoch - start_epoch:.4f} sec')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_cor > best_cor:\n",
    "            best_cor = val_cor\n",
    "            #print(\"model_updated\")\n",
    "            best_ckpt = deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model, best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiHeadAttentionRegressor().to(device)\n",
    "model, best_ckpt = train_model_ESM(model, train_dataset, val_dataset, epochs=500, batch_size=32, lr=1e-4, patience=50, device=device)\n",
    "model.load_state_dict(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 1240/1240 [00:02<00:00, 561.43it/s] \n"
     ]
    }
   ],
   "source": [
    "sequences = df_train['sequence']\n",
    "fitness_list = df_train['DMS_score'].tolist()\n",
    "\n",
    "train_seq2name = train_seq2name = {seq: f'seq_{i}' for i, seq in enumerate(sequences)}\n",
    "fitness2idx = {fitness: fitness for idx, fitness in enumerate(fitness_list)}\n",
    "\n",
    "emb_dir = 'esm_embeddings_test'\n",
    "total_dataset = ProteinESMDataset(sequences, train_seq2name, emb_dir, fitness_list, fitness2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 992/992 [00:00<00:00, 2391.54it/s]\n",
      "Loading esm embeddings: 100%|██████████| 248/248 [00:00<00:00, 2404.26it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences = df_train['sequence']\n",
    "fitness_list = df_train['DMS_score'].tolist()\n",
    "\n",
    "seq_train, seq_val, fitness_train, fitness_val = train_test_split(sequences, fitness_list, test_size=0.2, random_state=0)\n",
    "\n",
    "train_seq2name = train_seq2name = {seq: f'seq_{i}' for i, seq in enumerate(sequences)}\n",
    "fitness2idx = {fitness: fitness for idx, fitness in enumerate(fitness_list)}\n",
    "\n",
    "emb_dir = 'esm_embeddings_test'\n",
    "train_dataset = ProteinESMDataset(seq_train, train_seq2name, emb_dir, fitness_train, fitness2idx)\n",
    "val_dataset = ProteinESMDataset(seq_val, train_seq2name, emb_dir, fitness_val, fitness2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 2000]: Train Loss=0.2709, Val Cor=-0.0676, Time=0.2361 sec\n",
      "Epoch [2 / 2000]: Train Loss=1.4495, Val Cor=0.0809, Time=0.2329 sec\n",
      "model_updated\n",
      "Epoch [3 / 2000]: Train Loss=0.3209, Val Cor=-0.0876, Time=0.2327 sec\n",
      "Epoch [4 / 2000]: Train Loss=0.2355, Val Cor=-0.0855, Time=0.2323 sec\n",
      "Epoch [5 / 2000]: Train Loss=0.2134, Val Cor=-0.0885, Time=0.3291 sec\n",
      "Epoch [6 / 2000]: Train Loss=0.1921, Val Cor=-0.0891, Time=0.2331 sec\n",
      "Epoch [7 / 2000]: Train Loss=0.2038, Val Cor=-0.0891, Time=0.2326 sec\n",
      "Epoch [8 / 2000]: Train Loss=0.1941, Val Cor=-0.0884, Time=0.2327 sec\n",
      "Epoch [9 / 2000]: Train Loss=0.2003, Val Cor=-0.0866, Time=0.2328 sec\n",
      "Epoch [10 / 2000]: Train Loss=0.1968, Val Cor=-0.0858, Time=0.2326 sec\n",
      "Epoch [11 / 2000]: Train Loss=0.1920, Val Cor=-0.0862, Time=0.2329 sec\n",
      "Epoch [12 / 2000]: Train Loss=0.1957, Val Cor=-0.0850, Time=0.2326 sec\n",
      "Epoch [13 / 2000]: Train Loss=0.1924, Val Cor=-0.0856, Time=0.2326 sec\n",
      "Epoch [14 / 2000]: Train Loss=0.1926, Val Cor=-0.0840, Time=0.2326 sec\n",
      "Epoch [15 / 2000]: Train Loss=0.1937, Val Cor=-0.0833, Time=0.2326 sec\n",
      "Epoch [16 / 2000]: Train Loss=0.1927, Val Cor=-0.0820, Time=0.2323 sec\n",
      "Epoch [17 / 2000]: Train Loss=0.1920, Val Cor=-0.0809, Time=0.2326 sec\n",
      "Epoch [18 / 2000]: Train Loss=0.1930, Val Cor=-0.0812, Time=0.2327 sec\n",
      "Epoch [19 / 2000]: Train Loss=0.1922, Val Cor=-0.0805, Time=0.2325 sec\n",
      "Epoch [20 / 2000]: Train Loss=0.1921, Val Cor=-0.0796, Time=0.2327 sec\n",
      "Epoch [21 / 2000]: Train Loss=0.1926, Val Cor=-0.0772, Time=0.2328 sec\n",
      "Epoch [22 / 2000]: Train Loss=0.1921, Val Cor=-0.0744, Time=0.2325 sec\n",
      "Epoch [23 / 2000]: Train Loss=0.1920, Val Cor=-0.0719, Time=0.2324 sec\n",
      "Epoch [24 / 2000]: Train Loss=0.1923, Val Cor=-0.0700, Time=0.2324 sec\n",
      "Epoch [25 / 2000]: Train Loss=0.1920, Val Cor=-0.0683, Time=0.2327 sec\n",
      "Epoch [26 / 2000]: Train Loss=0.1920, Val Cor=-0.0643, Time=0.2325 sec\n",
      "Epoch [27 / 2000]: Train Loss=0.1921, Val Cor=-0.0594, Time=0.2325 sec\n",
      "Epoch [28 / 2000]: Train Loss=0.1919, Val Cor=-0.0532, Time=0.2325 sec\n",
      "Epoch [29 / 2000]: Train Loss=0.1920, Val Cor=-0.0501, Time=0.2327 sec\n",
      "Epoch [30 / 2000]: Train Loss=0.1921, Val Cor=-0.0463, Time=0.2334 sec\n",
      "Epoch [31 / 2000]: Train Loss=0.1919, Val Cor=-0.0432, Time=0.2330 sec\n",
      "Epoch [32 / 2000]: Train Loss=0.1920, Val Cor=-0.0390, Time=0.2324 sec\n",
      "Epoch [33 / 2000]: Train Loss=0.1920, Val Cor=-0.0348, Time=0.2327 sec\n",
      "Epoch [34 / 2000]: Train Loss=0.1918, Val Cor=-0.0312, Time=0.2326 sec\n",
      "Epoch [35 / 2000]: Train Loss=0.1919, Val Cor=-0.0257, Time=0.2326 sec\n",
      "Epoch [36 / 2000]: Train Loss=0.1918, Val Cor=-0.0218, Time=0.2331 sec\n",
      "Epoch [37 / 2000]: Train Loss=0.1918, Val Cor=-0.0187, Time=0.2331 sec\n",
      "Epoch [38 / 2000]: Train Loss=0.1918, Val Cor=-0.0125, Time=0.2325 sec\n",
      "Epoch [39 / 2000]: Train Loss=0.1918, Val Cor=-0.0060, Time=0.2327 sec\n",
      "Epoch [40 / 2000]: Train Loss=0.1918, Val Cor=-0.0005, Time=0.2328 sec\n",
      "Epoch [41 / 2000]: Train Loss=0.1918, Val Cor=0.0029, Time=0.2328 sec\n",
      "Epoch [42 / 2000]: Train Loss=0.1917, Val Cor=0.0064, Time=0.2326 sec\n",
      "Epoch [43 / 2000]: Train Loss=0.1917, Val Cor=0.0111, Time=0.2324 sec\n",
      "Epoch [44 / 2000]: Train Loss=0.1917, Val Cor=0.0181, Time=0.2322 sec\n",
      "Epoch [45 / 2000]: Train Loss=0.1916, Val Cor=0.0247, Time=0.2327 sec\n",
      "Epoch [46 / 2000]: Train Loss=0.1917, Val Cor=0.0295, Time=0.2325 sec\n",
      "Epoch [47 / 2000]: Train Loss=0.1916, Val Cor=0.0349, Time=0.2323 sec\n",
      "Epoch [48 / 2000]: Train Loss=0.1916, Val Cor=0.0407, Time=0.2325 sec\n",
      "Epoch [49 / 2000]: Train Loss=0.1915, Val Cor=0.0473, Time=0.2328 sec\n",
      "Epoch [50 / 2000]: Train Loss=0.1916, Val Cor=0.0527, Time=0.2325 sec\n",
      "Epoch [51 / 2000]: Train Loss=0.1915, Val Cor=0.0559, Time=0.2322 sec\n",
      "Epoch [52 / 2000]: Train Loss=0.1915, Val Cor=0.0602, Time=0.2325 sec\n",
      "Epoch [53 / 2000]: Train Loss=0.1915, Val Cor=0.0676, Time=0.2328 sec\n",
      "Epoch [54 / 2000]: Train Loss=0.1914, Val Cor=0.0729, Time=0.2326 sec\n",
      "Epoch [55 / 2000]: Train Loss=0.1914, Val Cor=0.0750, Time=0.2329 sec\n",
      "Epoch [56 / 2000]: Train Loss=0.1913, Val Cor=0.0787, Time=0.2324 sec\n",
      "Epoch [57 / 2000]: Train Loss=0.1913, Val Cor=0.0850, Time=0.2331 sec\n",
      "model_updated\n",
      "Epoch [58 / 2000]: Train Loss=0.1912, Val Cor=0.0892, Time=0.2326 sec\n",
      "model_updated\n",
      "Epoch [59 / 2000]: Train Loss=0.1911, Val Cor=0.0958, Time=0.2321 sec\n",
      "model_updated\n",
      "Epoch [60 / 2000]: Train Loss=0.1911, Val Cor=0.1011, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [61 / 2000]: Train Loss=0.1910, Val Cor=0.1090, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [62 / 2000]: Train Loss=0.1909, Val Cor=0.1158, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [63 / 2000]: Train Loss=0.1908, Val Cor=0.1227, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [64 / 2000]: Train Loss=0.1907, Val Cor=0.1298, Time=0.2326 sec\n",
      "model_updated\n",
      "Epoch [65 / 2000]: Train Loss=0.1906, Val Cor=0.1368, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [66 / 2000]: Train Loss=0.1904, Val Cor=0.1435, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [67 / 2000]: Train Loss=0.1902, Val Cor=0.1499, Time=0.2323 sec\n",
      "model_updated\n",
      "Epoch [68 / 2000]: Train Loss=0.1901, Val Cor=0.1594, Time=0.2328 sec\n",
      "model_updated\n",
      "Epoch [69 / 2000]: Train Loss=0.1922, Val Cor=0.2039, Time=0.2323 sec\n",
      "model_updated\n",
      "Epoch [70 / 2000]: Train Loss=0.2023, Val Cor=0.2257, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [71 / 2000]: Train Loss=0.1915, Val Cor=0.2480, Time=0.2321 sec\n",
      "model_updated\n",
      "Epoch [72 / 2000]: Train Loss=0.1972, Val Cor=0.2524, Time=0.2323 sec\n",
      "model_updated\n",
      "Epoch [73 / 2000]: Train Loss=0.1928, Val Cor=0.2549, Time=0.2323 sec\n",
      "model_updated\n",
      "Epoch [74 / 2000]: Train Loss=0.1919, Val Cor=0.2556, Time=0.2323 sec\n",
      "model_updated\n",
      "Epoch [75 / 2000]: Train Loss=0.1939, Val Cor=0.2655, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [76 / 2000]: Train Loss=0.1940, Val Cor=0.2699, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [77 / 2000]: Train Loss=0.1924, Val Cor=0.2727, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [78 / 2000]: Train Loss=0.1916, Val Cor=0.2768, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [79 / 2000]: Train Loss=0.1927, Val Cor=0.2794, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [80 / 2000]: Train Loss=0.1925, Val Cor=0.2791, Time=0.2324 sec\n",
      "Epoch [81 / 2000]: Train Loss=0.1916, Val Cor=0.2787, Time=0.2326 sec\n",
      "Epoch [82 / 2000]: Train Loss=0.1919, Val Cor=0.2786, Time=0.2325 sec\n",
      "Epoch [83 / 2000]: Train Loss=0.1923, Val Cor=0.2792, Time=0.2324 sec\n",
      "Epoch [84 / 2000]: Train Loss=0.1921, Val Cor=0.2809, Time=0.2323 sec\n",
      "model_updated\n",
      "Epoch [85 / 2000]: Train Loss=0.1916, Val Cor=0.2824, Time=0.2326 sec\n",
      "model_updated\n",
      "Epoch [86 / 2000]: Train Loss=0.1917, Val Cor=0.2827, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [87 / 2000]: Train Loss=0.1921, Val Cor=0.2834, Time=0.2322 sec\n",
      "model_updated\n",
      "Epoch [88 / 2000]: Train Loss=0.1919, Val Cor=0.2835, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [89 / 2000]: Train Loss=0.1917, Val Cor=0.2835, Time=0.3290 sec\n",
      "model_updated\n",
      "Epoch [90 / 2000]: Train Loss=0.1916, Val Cor=0.2844, Time=0.2326 sec\n",
      "model_updated\n",
      "Epoch [91 / 2000]: Train Loss=0.1918, Val Cor=0.2840, Time=0.2327 sec\n",
      "Epoch [92 / 2000]: Train Loss=0.1918, Val Cor=0.2832, Time=0.2326 sec\n",
      "Epoch [93 / 2000]: Train Loss=0.1916, Val Cor=0.2824, Time=0.2327 sec\n",
      "Epoch [94 / 2000]: Train Loss=0.1916, Val Cor=0.2822, Time=0.2326 sec\n",
      "Epoch [95 / 2000]: Train Loss=0.1917, Val Cor=0.2823, Time=0.2325 sec\n",
      "Epoch [96 / 2000]: Train Loss=0.1917, Val Cor=0.2826, Time=0.2325 sec\n",
      "Epoch [97 / 2000]: Train Loss=0.1916, Val Cor=0.2826, Time=0.2330 sec\n",
      "Epoch [98 / 2000]: Train Loss=0.1916, Val Cor=0.2828, Time=0.2326 sec\n",
      "Epoch [99 / 2000]: Train Loss=0.1916, Val Cor=0.2828, Time=0.2325 sec\n",
      "Epoch [100 / 2000]: Train Loss=0.1917, Val Cor=0.2826, Time=0.2329 sec\n",
      "Epoch [101 / 2000]: Train Loss=0.1916, Val Cor=0.2825, Time=0.2328 sec\n",
      "Epoch [102 / 2000]: Train Loss=0.1916, Val Cor=0.2826, Time=0.2324 sec\n",
      "Epoch [103 / 2000]: Train Loss=0.1916, Val Cor=0.2827, Time=0.2326 sec\n",
      "Epoch [104 / 2000]: Train Loss=0.1916, Val Cor=0.2827, Time=0.2323 sec\n",
      "Epoch [105 / 2000]: Train Loss=0.1916, Val Cor=0.2831, Time=0.2329 sec\n",
      "Epoch [106 / 2000]: Train Loss=0.1915, Val Cor=0.2837, Time=0.2322 sec\n",
      "Epoch [107 / 2000]: Train Loss=0.1916, Val Cor=0.2837, Time=0.2325 sec\n",
      "Epoch [108 / 2000]: Train Loss=0.1916, Val Cor=0.2840, Time=0.2324 sec\n",
      "Epoch [109 / 2000]: Train Loss=0.1915, Val Cor=0.2842, Time=0.2326 sec\n",
      "Epoch [110 / 2000]: Train Loss=0.1915, Val Cor=0.2840, Time=0.2324 sec\n",
      "Epoch [111 / 2000]: Train Loss=0.1915, Val Cor=0.2839, Time=0.2324 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [112 / 2000]: Train Loss=0.1915, Val Cor=0.2835, Time=0.2322 sec\n",
      "Epoch [113 / 2000]: Train Loss=0.1915, Val Cor=0.2834, Time=0.2330 sec\n",
      "Epoch [114 / 2000]: Train Loss=0.1915, Val Cor=0.2828, Time=0.2327 sec\n",
      "Epoch [115 / 2000]: Train Loss=0.1915, Val Cor=0.2826, Time=0.2327 sec\n",
      "Epoch [116 / 2000]: Train Loss=0.1915, Val Cor=0.2832, Time=0.2326 sec\n",
      "Epoch [117 / 2000]: Train Loss=0.1915, Val Cor=0.2828, Time=0.2324 sec\n",
      "Epoch [118 / 2000]: Train Loss=0.1915, Val Cor=0.2831, Time=0.2325 sec\n",
      "Epoch [119 / 2000]: Train Loss=0.1915, Val Cor=0.2829, Time=0.2326 sec\n",
      "Epoch [120 / 2000]: Train Loss=0.1914, Val Cor=0.2825, Time=0.2326 sec\n",
      "Epoch [121 / 2000]: Train Loss=0.1914, Val Cor=0.2829, Time=0.2329 sec\n",
      "Epoch [122 / 2000]: Train Loss=0.1914, Val Cor=0.2820, Time=0.2327 sec\n",
      "Epoch [123 / 2000]: Train Loss=0.1914, Val Cor=0.2817, Time=0.2329 sec\n",
      "Epoch [124 / 2000]: Train Loss=0.1914, Val Cor=0.2815, Time=0.2326 sec\n",
      "Epoch [125 / 2000]: Train Loss=0.1914, Val Cor=0.2820, Time=0.2326 sec\n",
      "Epoch [126 / 2000]: Train Loss=0.1914, Val Cor=0.2815, Time=0.2322 sec\n",
      "Epoch [127 / 2000]: Train Loss=0.1914, Val Cor=0.2812, Time=0.2324 sec\n",
      "Epoch [128 / 2000]: Train Loss=0.1913, Val Cor=0.2806, Time=0.2325 sec\n",
      "Epoch [129 / 2000]: Train Loss=0.1913, Val Cor=0.2801, Time=0.2329 sec\n",
      "Epoch [130 / 2000]: Train Loss=0.1913, Val Cor=0.2793, Time=0.2325 sec\n",
      "Epoch [131 / 2000]: Train Loss=0.1913, Val Cor=0.2788, Time=0.2325 sec\n",
      "Epoch [132 / 2000]: Train Loss=0.1913, Val Cor=0.2790, Time=0.2328 sec\n",
      "Epoch [133 / 2000]: Train Loss=0.1912, Val Cor=0.2796, Time=0.2327 sec\n",
      "Epoch [134 / 2000]: Train Loss=0.1912, Val Cor=0.2799, Time=0.2326 sec\n",
      "Epoch [135 / 2000]: Train Loss=0.1912, Val Cor=0.2797, Time=0.2328 sec\n",
      "Epoch [136 / 2000]: Train Loss=0.1911, Val Cor=0.2794, Time=0.2326 sec\n",
      "Epoch [137 / 2000]: Train Loss=0.1911, Val Cor=0.2803, Time=0.2329 sec\n",
      "Epoch [138 / 2000]: Train Loss=0.1910, Val Cor=0.2799, Time=0.2326 sec\n",
      "Epoch [139 / 2000]: Train Loss=0.1910, Val Cor=0.2801, Time=0.2322 sec\n",
      "Epoch [140 / 2000]: Train Loss=0.1909, Val Cor=0.2802, Time=0.2326 sec\n",
      "Epoch [141 / 2000]: Train Loss=0.1908, Val Cor=0.2802, Time=0.2330 sec\n",
      "Epoch [142 / 2000]: Train Loss=0.1908, Val Cor=0.2803, Time=0.2326 sec\n",
      "Epoch [143 / 2000]: Train Loss=0.1907, Val Cor=0.2801, Time=0.2325 sec\n",
      "Epoch [144 / 2000]: Train Loss=0.1906, Val Cor=0.2807, Time=0.2328 sec\n",
      "Epoch [145 / 2000]: Train Loss=0.1905, Val Cor=0.2805, Time=0.2334 sec\n",
      "Epoch [146 / 2000]: Train Loss=0.1904, Val Cor=0.2817, Time=0.2327 sec\n",
      "Epoch [147 / 2000]: Train Loss=0.1903, Val Cor=0.2814, Time=0.2327 sec\n",
      "Epoch [148 / 2000]: Train Loss=0.1902, Val Cor=0.2808, Time=0.2326 sec\n",
      "Epoch [149 / 2000]: Train Loss=0.1901, Val Cor=0.2807, Time=0.2327 sec\n",
      "Epoch [150 / 2000]: Train Loss=0.1900, Val Cor=0.2800, Time=0.2328 sec\n",
      "Epoch [151 / 2000]: Train Loss=0.1901, Val Cor=0.2831, Time=0.2330 sec\n",
      "Epoch [152 / 2000]: Train Loss=0.1924, Val Cor=0.2822, Time=0.2329 sec\n",
      "Epoch [153 / 2000]: Train Loss=0.1913, Val Cor=0.2837, Time=0.2326 sec\n",
      "Epoch [154 / 2000]: Train Loss=0.1903, Val Cor=0.2835, Time=0.2323 sec\n",
      "Epoch [155 / 2000]: Train Loss=0.1910, Val Cor=0.2837, Time=0.2325 sec\n",
      "Epoch [156 / 2000]: Train Loss=0.1905, Val Cor=0.2831, Time=0.2324 sec\n",
      "Epoch [157 / 2000]: Train Loss=0.1907, Val Cor=0.2833, Time=0.2328 sec\n",
      "Epoch [158 / 2000]: Train Loss=0.1906, Val Cor=0.2844, Time=0.2325 sec\n",
      "Epoch [159 / 2000]: Train Loss=0.1905, Val Cor=0.2848, Time=0.2326 sec\n",
      "model_updated\n",
      "Epoch [160 / 2000]: Train Loss=0.1907, Val Cor=0.2851, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [161 / 2000]: Train Loss=0.1904, Val Cor=0.2855, Time=0.2328 sec\n",
      "model_updated\n",
      "Epoch [162 / 2000]: Train Loss=0.1908, Val Cor=0.2849, Time=0.2324 sec\n",
      "Epoch [163 / 2000]: Train Loss=0.1903, Val Cor=0.2859, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [164 / 2000]: Train Loss=0.1904, Val Cor=0.2858, Time=0.2325 sec\n",
      "Epoch [165 / 2000]: Train Loss=0.1902, Val Cor=0.2853, Time=0.2328 sec\n",
      "Epoch [166 / 2000]: Train Loss=0.1901, Val Cor=0.2860, Time=0.2327 sec\n",
      "model_updated\n",
      "Epoch [167 / 2000]: Train Loss=0.1900, Val Cor=0.2855, Time=0.2324 sec\n",
      "Epoch [168 / 2000]: Train Loss=0.1899, Val Cor=0.2859, Time=0.2325 sec\n",
      "Epoch [169 / 2000]: Train Loss=0.1897, Val Cor=0.2864, Time=0.2329 sec\n",
      "model_updated\n",
      "Epoch [170 / 2000]: Train Loss=0.1895, Val Cor=0.2864, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [171 / 2000]: Train Loss=0.1892, Val Cor=0.2864, Time=0.2324 sec\n",
      "Epoch [172 / 2000]: Train Loss=0.1890, Val Cor=0.2870, Time=0.2324 sec\n",
      "model_updated\n",
      "Epoch [173 / 2000]: Train Loss=0.1887, Val Cor=0.2871, Time=0.3305 sec\n",
      "model_updated\n",
      "Epoch [174 / 2000]: Train Loss=0.1882, Val Cor=0.2869, Time=0.2330 sec\n",
      "Epoch [175 / 2000]: Train Loss=0.1877, Val Cor=0.2872, Time=0.2329 sec\n",
      "model_updated\n",
      "Epoch [176 / 2000]: Train Loss=0.1873, Val Cor=0.2863, Time=0.2325 sec\n",
      "Epoch [177 / 2000]: Train Loss=0.2070, Val Cor=0.1941, Time=0.2329 sec\n",
      "Epoch [178 / 2000]: Train Loss=0.2201, Val Cor=0.2096, Time=0.2325 sec\n",
      "Epoch [179 / 2000]: Train Loss=0.2112, Val Cor=0.1590, Time=0.2326 sec\n",
      "Epoch [180 / 2000]: Train Loss=0.1971, Val Cor=0.2587, Time=0.2324 sec\n",
      "Epoch [181 / 2000]: Train Loss=0.1937, Val Cor=0.2574, Time=0.2331 sec\n",
      "Epoch [182 / 2000]: Train Loss=0.1969, Val Cor=0.2596, Time=0.2325 sec\n",
      "Epoch [183 / 2000]: Train Loss=0.1920, Val Cor=0.2568, Time=0.2326 sec\n",
      "Epoch [184 / 2000]: Train Loss=0.1934, Val Cor=0.2538, Time=0.2337 sec\n",
      "Epoch [185 / 2000]: Train Loss=0.1948, Val Cor=0.2557, Time=0.2339 sec\n",
      "Epoch [186 / 2000]: Train Loss=0.1939, Val Cor=0.2594, Time=0.2329 sec\n",
      "Epoch [187 / 2000]: Train Loss=0.1922, Val Cor=0.2600, Time=0.2326 sec\n",
      "Epoch [188 / 2000]: Train Loss=0.1922, Val Cor=0.2611, Time=0.2330 sec\n",
      "Epoch [189 / 2000]: Train Loss=0.1936, Val Cor=0.2624, Time=0.2326 sec\n",
      "Epoch [190 / 2000]: Train Loss=0.1929, Val Cor=0.2641, Time=0.2325 sec\n",
      "Epoch [191 / 2000]: Train Loss=0.1919, Val Cor=0.2664, Time=0.2325 sec\n",
      "Epoch [192 / 2000]: Train Loss=0.1922, Val Cor=0.2664, Time=0.2325 sec\n",
      "Epoch [193 / 2000]: Train Loss=0.1929, Val Cor=0.2647, Time=0.2329 sec\n",
      "Epoch [194 / 2000]: Train Loss=0.1929, Val Cor=0.2656, Time=0.2326 sec\n",
      "Epoch [195 / 2000]: Train Loss=0.1923, Val Cor=0.2672, Time=0.2325 sec\n",
      "Epoch [196 / 2000]: Train Loss=0.1919, Val Cor=0.2682, Time=0.2331 sec\n",
      "Epoch [197 / 2000]: Train Loss=0.1922, Val Cor=0.2694, Time=0.2331 sec\n",
      "Epoch [198 / 2000]: Train Loss=0.1926, Val Cor=0.2714, Time=0.2329 sec\n",
      "Epoch [199 / 2000]: Train Loss=0.1923, Val Cor=0.2736, Time=0.2326 sec\n",
      "Epoch [200 / 2000]: Train Loss=0.1920, Val Cor=0.2721, Time=0.2325 sec\n",
      "Epoch [201 / 2000]: Train Loss=0.1919, Val Cor=0.2741, Time=0.2330 sec\n",
      "Epoch [202 / 2000]: Train Loss=0.1922, Val Cor=0.2748, Time=0.2325 sec\n",
      "Epoch [203 / 2000]: Train Loss=0.1922, Val Cor=0.2745, Time=0.2327 sec\n",
      "Epoch [204 / 2000]: Train Loss=0.1921, Val Cor=0.2754, Time=0.2323 sec\n",
      "Epoch [205 / 2000]: Train Loss=0.1919, Val Cor=0.2748, Time=0.2330 sec\n",
      "Epoch [206 / 2000]: Train Loss=0.1919, Val Cor=0.2756, Time=0.2325 sec\n",
      "Epoch [207 / 2000]: Train Loss=0.1921, Val Cor=0.2766, Time=0.2327 sec\n",
      "Epoch [208 / 2000]: Train Loss=0.1921, Val Cor=0.2763, Time=0.2326 sec\n",
      "Epoch [209 / 2000]: Train Loss=0.1920, Val Cor=0.2741, Time=0.2326 sec\n",
      "Epoch [210 / 2000]: Train Loss=0.1919, Val Cor=0.2740, Time=0.2326 sec\n",
      "Epoch [211 / 2000]: Train Loss=0.1919, Val Cor=0.2736, Time=0.2327 sec\n",
      "Epoch [212 / 2000]: Train Loss=0.1920, Val Cor=0.2739, Time=0.2324 sec\n",
      "Epoch [213 / 2000]: Train Loss=0.1920, Val Cor=0.2740, Time=0.2328 sec\n",
      "Epoch [214 / 2000]: Train Loss=0.1919, Val Cor=0.2738, Time=0.2326 sec\n",
      "Epoch [215 / 2000]: Train Loss=0.1919, Val Cor=0.2745, Time=0.2326 sec\n",
      "Epoch [216 / 2000]: Train Loss=0.1919, Val Cor=0.2746, Time=0.2327 sec\n",
      "Epoch [217 / 2000]: Train Loss=0.1919, Val Cor=0.2742, Time=0.2328 sec\n",
      "Epoch [218 / 2000]: Train Loss=0.1919, Val Cor=0.2747, Time=0.2326 sec\n",
      "Epoch [219 / 2000]: Train Loss=0.1919, Val Cor=0.2738, Time=0.2326 sec\n",
      "Epoch [220 / 2000]: Train Loss=0.1919, Val Cor=0.2741, Time=0.2326 sec\n",
      "Epoch [221 / 2000]: Train Loss=0.1919, Val Cor=0.2742, Time=0.2329 sec\n",
      "Epoch [222 / 2000]: Train Loss=0.1919, Val Cor=0.2743, Time=0.2326 sec\n",
      "Epoch [223 / 2000]: Train Loss=0.1919, Val Cor=0.2734, Time=0.2326 sec\n",
      "Epoch [224 / 2000]: Train Loss=0.1919, Val Cor=0.2738, Time=0.2326 sec\n",
      "Epoch [225 / 2000]: Train Loss=0.1919, Val Cor=0.2740, Time=0.2331 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226 / 2000]: Train Loss=0.1919, Val Cor=0.2746, Time=0.2327 sec\n",
      "Epoch [227 / 2000]: Train Loss=0.1919, Val Cor=0.2750, Time=0.2330 sec\n",
      "Epoch [228 / 2000]: Train Loss=0.1918, Val Cor=0.2750, Time=0.2330 sec\n",
      "Epoch [229 / 2000]: Train Loss=0.1918, Val Cor=0.2752, Time=0.2330 sec\n",
      "Epoch [230 / 2000]: Train Loss=0.1918, Val Cor=0.2751, Time=0.2325 sec\n",
      "Epoch [231 / 2000]: Train Loss=0.1918, Val Cor=0.2758, Time=0.2327 sec\n",
      "Epoch [232 / 2000]: Train Loss=0.1918, Val Cor=0.2758, Time=0.2322 sec\n",
      "Epoch [233 / 2000]: Train Loss=0.1918, Val Cor=0.2761, Time=0.2329 sec\n",
      "Epoch [234 / 2000]: Train Loss=0.1918, Val Cor=0.2757, Time=0.2326 sec\n",
      "Epoch [235 / 2000]: Train Loss=0.1918, Val Cor=0.2757, Time=0.2325 sec\n",
      "Epoch [236 / 2000]: Train Loss=0.1918, Val Cor=0.2759, Time=0.2335 sec\n",
      "Epoch [237 / 2000]: Train Loss=0.1918, Val Cor=0.2769, Time=0.2338 sec\n",
      "Epoch [238 / 2000]: Train Loss=0.1918, Val Cor=0.2779, Time=0.2329 sec\n",
      "Epoch [239 / 2000]: Train Loss=0.1917, Val Cor=0.2785, Time=0.2327 sec\n",
      "Epoch [240 / 2000]: Train Loss=0.1917, Val Cor=0.2791, Time=0.2327 sec\n",
      "Epoch [241 / 2000]: Train Loss=0.1917, Val Cor=0.2790, Time=0.2329 sec\n",
      "Epoch [242 / 2000]: Train Loss=0.1917, Val Cor=0.2803, Time=0.2327 sec\n",
      "Epoch [243 / 2000]: Train Loss=0.1916, Val Cor=0.2802, Time=0.2330 sec\n",
      "Epoch [244 / 2000]: Train Loss=0.1916, Val Cor=0.2807, Time=0.2327 sec\n",
      "Epoch [245 / 2000]: Train Loss=0.1915, Val Cor=0.2808, Time=0.2329 sec\n",
      "Epoch [246 / 2000]: Train Loss=0.1915, Val Cor=0.2863, Time=0.2327 sec\n",
      "Epoch [247 / 2000]: Train Loss=0.1919, Val Cor=0.2852, Time=0.2326 sec\n",
      "Epoch [248 / 2000]: Train Loss=0.1915, Val Cor=0.2850, Time=0.2324 sec\n",
      "Epoch [249 / 2000]: Train Loss=0.1915, Val Cor=0.2858, Time=0.2328 sec\n",
      "Epoch [250 / 2000]: Train Loss=0.1916, Val Cor=0.2856, Time=0.2327 sec\n",
      "Epoch [251 / 2000]: Train Loss=0.1915, Val Cor=0.2859, Time=0.2328 sec\n",
      "Epoch [252 / 2000]: Train Loss=0.1913, Val Cor=0.2866, Time=0.2323 sec\n",
      "Epoch [253 / 2000]: Train Loss=0.1915, Val Cor=0.2864, Time=0.2330 sec\n",
      "Epoch [254 / 2000]: Train Loss=0.1916, Val Cor=0.2870, Time=0.2328 sec\n",
      "Epoch [255 / 2000]: Train Loss=0.1911, Val Cor=0.2878, Time=0.2325 sec\n",
      "model_updated\n",
      "Epoch [256 / 2000]: Train Loss=0.1915, Val Cor=0.2854, Time=0.2325 sec\n",
      "Epoch [257 / 2000]: Train Loss=0.1913, Val Cor=0.2862, Time=0.3324 sec\n",
      "Epoch [258 / 2000]: Train Loss=0.1912, Val Cor=0.2881, Time=0.2332 sec\n",
      "model_updated\n",
      "Epoch [259 / 2000]: Train Loss=0.1912, Val Cor=0.2870, Time=0.2326 sec\n",
      "Epoch [260 / 2000]: Train Loss=0.1908, Val Cor=0.2874, Time=0.2326 sec\n",
      "Epoch [261 / 2000]: Train Loss=0.1909, Val Cor=0.2868, Time=0.2330 sec\n",
      "Epoch [262 / 2000]: Train Loss=0.1906, Val Cor=0.2872, Time=0.2329 sec\n",
      "Epoch [263 / 2000]: Train Loss=0.1905, Val Cor=0.2873, Time=0.2327 sec\n",
      "Epoch [264 / 2000]: Train Loss=0.1905, Val Cor=0.2856, Time=0.2327 sec\n",
      "Epoch [265 / 2000]: Train Loss=0.1903, Val Cor=0.2880, Time=0.2326 sec\n",
      "Epoch [266 / 2000]: Train Loss=0.1902, Val Cor=0.2873, Time=0.2327 sec\n",
      "Epoch [267 / 2000]: Train Loss=0.1921, Val Cor=0.2860, Time=0.2325 sec\n",
      "Epoch [268 / 2000]: Train Loss=0.1945, Val Cor=0.2867, Time=0.2325 sec\n",
      "Epoch [269 / 2000]: Train Loss=0.1920, Val Cor=0.2868, Time=0.2329 sec\n",
      "Epoch [270 / 2000]: Train Loss=0.1924, Val Cor=0.2877, Time=0.2326 sec\n",
      "Epoch [271 / 2000]: Train Loss=0.1906, Val Cor=0.2861, Time=0.2331 sec\n",
      "Epoch [272 / 2000]: Train Loss=0.1934, Val Cor=0.2871, Time=0.2328 sec\n",
      "Epoch [273 / 2000]: Train Loss=0.1908, Val Cor=0.2880, Time=0.2329 sec\n",
      "Epoch [274 / 2000]: Train Loss=0.1912, Val Cor=0.2881, Time=0.2327 sec\n",
      "model_updated\n",
      "Epoch [275 / 2000]: Train Loss=0.1922, Val Cor=0.2877, Time=0.2323 sec\n",
      "Epoch [276 / 2000]: Train Loss=0.1911, Val Cor=0.2870, Time=0.2324 sec\n",
      "Epoch [277 / 2000]: Train Loss=0.1908, Val Cor=0.2859, Time=0.2326 sec\n",
      "Epoch [278 / 2000]: Train Loss=0.1919, Val Cor=0.2858, Time=0.2326 sec\n",
      "Epoch [279 / 2000]: Train Loss=0.1912, Val Cor=0.2880, Time=0.2327 sec\n",
      "Epoch [280 / 2000]: Train Loss=0.1907, Val Cor=0.2873, Time=0.2330 sec\n",
      "Epoch [281 / 2000]: Train Loss=0.1914, Val Cor=0.2873, Time=0.2329 sec\n",
      "Epoch [282 / 2000]: Train Loss=0.1913, Val Cor=0.2879, Time=0.2325 sec\n",
      "Epoch [283 / 2000]: Train Loss=0.1906, Val Cor=0.2858, Time=0.2326 sec\n",
      "Epoch [284 / 2000]: Train Loss=0.1908, Val Cor=0.2859, Time=0.2325 sec\n",
      "Epoch [285 / 2000]: Train Loss=0.1912, Val Cor=0.2863, Time=0.2329 sec\n",
      "Epoch [286 / 2000]: Train Loss=0.1906, Val Cor=0.2875, Time=0.2329 sec\n",
      "Epoch [287 / 2000]: Train Loss=0.1905, Val Cor=0.2870, Time=0.2332 sec\n",
      "Epoch [288 / 2000]: Train Loss=0.1908, Val Cor=0.2872, Time=0.2327 sec\n",
      "Epoch [289 / 2000]: Train Loss=0.1904, Val Cor=0.2870, Time=0.2330 sec\n",
      "Epoch [290 / 2000]: Train Loss=0.1903, Val Cor=0.2865, Time=0.2329 sec\n",
      "Epoch [291 / 2000]: Train Loss=0.1904, Val Cor=0.2868, Time=0.2327 sec\n",
      "Epoch [292 / 2000]: Train Loss=0.1900, Val Cor=0.2867, Time=0.2327 sec\n",
      "Epoch [293 / 2000]: Train Loss=0.1899, Val Cor=0.2870, Time=0.2329 sec\n",
      "Epoch [294 / 2000]: Train Loss=0.1898, Val Cor=0.2872, Time=0.2326 sec\n",
      "Epoch [295 / 2000]: Train Loss=0.1895, Val Cor=0.2867, Time=0.2326 sec\n",
      "Epoch [296 / 2000]: Train Loss=0.1897, Val Cor=0.2871, Time=0.2328 sec\n",
      "Epoch [297 / 2000]: Train Loss=0.1900, Val Cor=0.2867, Time=0.2325 sec\n",
      "Epoch [298 / 2000]: Train Loss=0.1888, Val Cor=0.2870, Time=0.2327 sec\n",
      "Epoch [299 / 2000]: Train Loss=0.1891, Val Cor=0.2874, Time=0.2329 sec\n",
      "Epoch [300 / 2000]: Train Loss=0.1921, Val Cor=0.2866, Time=0.2326 sec\n",
      "Epoch [301 / 2000]: Train Loss=0.1902, Val Cor=0.2871, Time=0.2326 sec\n",
      "Epoch [302 / 2000]: Train Loss=0.1886, Val Cor=0.2868, Time=0.2326 sec\n",
      "Epoch [303 / 2000]: Train Loss=0.1879, Val Cor=0.2866, Time=0.2325 sec\n",
      "Epoch [304 / 2000]: Train Loss=0.1894, Val Cor=0.2868, Time=0.2326 sec\n",
      "Epoch [305 / 2000]: Train Loss=0.1931, Val Cor=0.2865, Time=0.2330 sec\n",
      "Epoch [306 / 2000]: Train Loss=0.1887, Val Cor=0.2869, Time=0.2326 sec\n",
      "Epoch [307 / 2000]: Train Loss=0.1872, Val Cor=0.2871, Time=0.2322 sec\n",
      "Epoch [308 / 2000]: Train Loss=0.1877, Val Cor=0.2865, Time=0.2327 sec\n",
      "Epoch [309 / 2000]: Train Loss=0.1894, Val Cor=0.2868, Time=0.2325 sec\n",
      "Epoch [310 / 2000]: Train Loss=0.1938, Val Cor=0.2868, Time=0.2325 sec\n",
      "Epoch [311 / 2000]: Train Loss=0.1867, Val Cor=0.2863, Time=0.2325 sec\n",
      "Epoch [312 / 2000]: Train Loss=0.1977, Val Cor=0.2876, Time=0.2326 sec\n",
      "Epoch [313 / 2000]: Train Loss=0.1968, Val Cor=0.2871, Time=0.2327 sec\n",
      "Epoch [314 / 2000]: Train Loss=0.1993, Val Cor=0.2910, Time=0.2332 sec\n",
      "model_updated\n",
      "Epoch [315 / 2000]: Train Loss=0.1984, Val Cor=0.2795, Time=0.2326 sec\n",
      "Epoch [316 / 2000]: Train Loss=0.1964, Val Cor=0.2789, Time=0.2326 sec\n",
      "Epoch [317 / 2000]: Train Loss=0.1941, Val Cor=0.2796, Time=0.2328 sec\n",
      "Epoch [318 / 2000]: Train Loss=0.1923, Val Cor=0.2801, Time=0.2328 sec\n",
      "Epoch [319 / 2000]: Train Loss=0.1919, Val Cor=0.2803, Time=0.2326 sec\n",
      "Epoch [320 / 2000]: Train Loss=0.1928, Val Cor=0.2806, Time=0.2326 sec\n",
      "Epoch [321 / 2000]: Train Loss=0.1940, Val Cor=0.2805, Time=0.2326 sec\n",
      "Epoch [322 / 2000]: Train Loss=0.1938, Val Cor=0.2808, Time=0.2326 sec\n",
      "Epoch [323 / 2000]: Train Loss=0.1929, Val Cor=0.2811, Time=0.2329 sec\n",
      "Epoch [324 / 2000]: Train Loss=0.1922, Val Cor=0.2815, Time=0.2330 sec\n",
      "Epoch [325 / 2000]: Train Loss=0.1919, Val Cor=0.2811, Time=0.2329 sec\n",
      "Epoch [326 / 2000]: Train Loss=0.1919, Val Cor=0.2809, Time=0.2328 sec\n",
      "Epoch [327 / 2000]: Train Loss=0.1921, Val Cor=0.2811, Time=0.2330 sec\n",
      "Epoch [328 / 2000]: Train Loss=0.1922, Val Cor=0.2816, Time=0.2325 sec\n",
      "Epoch [329 / 2000]: Train Loss=0.1923, Val Cor=0.2817, Time=0.2329 sec\n",
      "Epoch [330 / 2000]: Train Loss=0.1922, Val Cor=0.2817, Time=0.2328 sec\n",
      "Epoch [331 / 2000]: Train Loss=0.1920, Val Cor=0.2818, Time=0.2325 sec\n",
      "Epoch [332 / 2000]: Train Loss=0.1919, Val Cor=0.2819, Time=0.2325 sec\n",
      "Epoch [333 / 2000]: Train Loss=0.1919, Val Cor=0.2819, Time=0.2330 sec\n",
      "Epoch [334 / 2000]: Train Loss=0.1919, Val Cor=0.2819, Time=0.2325 sec\n",
      "Epoch [335 / 2000]: Train Loss=0.1920, Val Cor=0.2821, Time=0.2328 sec\n",
      "Epoch [336 / 2000]: Train Loss=0.1920, Val Cor=0.2824, Time=0.2329 sec\n",
      "Epoch [337 / 2000]: Train Loss=0.1920, Val Cor=0.2823, Time=0.2330 sec\n",
      "Epoch [338 / 2000]: Train Loss=0.1919, Val Cor=0.2820, Time=0.2325 sec\n",
      "Epoch [339 / 2000]: Train Loss=0.1919, Val Cor=0.2818, Time=0.2326 sec\n",
      "Epoch [340 / 2000]: Train Loss=0.1919, Val Cor=0.2818, Time=0.2326 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [341 / 2000]: Train Loss=0.1919, Val Cor=0.2817, Time=0.3290 sec\n",
      "Epoch [342 / 2000]: Train Loss=0.1919, Val Cor=0.2819, Time=0.2331 sec\n",
      "Epoch [343 / 2000]: Train Loss=0.1919, Val Cor=0.2821, Time=0.2326 sec\n",
      "Epoch [344 / 2000]: Train Loss=0.1919, Val Cor=0.2820, Time=0.2329 sec\n",
      "Epoch [345 / 2000]: Train Loss=0.1919, Val Cor=0.2819, Time=0.2328 sec\n",
      "Epoch [346 / 2000]: Train Loss=0.1919, Val Cor=0.2820, Time=0.2326 sec\n",
      "Epoch [347 / 2000]: Train Loss=0.1919, Val Cor=0.2820, Time=0.2325 sec\n",
      "Epoch [348 / 2000]: Train Loss=0.1919, Val Cor=0.2821, Time=0.2327 sec\n",
      "Epoch [349 / 2000]: Train Loss=0.1919, Val Cor=0.2821, Time=0.2327 sec\n",
      "Epoch [350 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2326 sec\n",
      "Epoch [351 / 2000]: Train Loss=0.1919, Val Cor=0.2822, Time=0.2325 sec\n",
      "Epoch [352 / 2000]: Train Loss=0.1919, Val Cor=0.2824, Time=0.2326 sec\n",
      "Epoch [353 / 2000]: Train Loss=0.1919, Val Cor=0.2824, Time=0.2329 sec\n",
      "Epoch [354 / 2000]: Train Loss=0.1919, Val Cor=0.2822, Time=0.2327 sec\n",
      "Epoch [355 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2327 sec\n",
      "Epoch [356 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2330 sec\n",
      "Epoch [357 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2330 sec\n",
      "Epoch [358 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2327 sec\n",
      "Epoch [359 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2328 sec\n",
      "Epoch [360 / 2000]: Train Loss=0.1919, Val Cor=0.2823, Time=0.2326 sec\n",
      "Epoch [361 / 2000]: Train Loss=0.1919, Val Cor=0.2822, Time=0.2329 sec\n",
      "Epoch [362 / 2000]: Train Loss=0.1918, Val Cor=0.2823, Time=0.2324 sec\n",
      "Epoch [363 / 2000]: Train Loss=0.1918, Val Cor=0.2823, Time=0.2326 sec\n",
      "Epoch [364 / 2000]: Train Loss=0.1918, Val Cor=0.2822, Time=0.2327 sec\n",
      "Epoch [365 / 2000]: Train Loss=0.1918, Val Cor=0.2823, Time=0.2329 sec\n",
      "Epoch [366 / 2000]: Train Loss=0.1918, Val Cor=0.2823, Time=0.2325 sec\n",
      "Epoch [367 / 2000]: Train Loss=0.1918, Val Cor=0.2823, Time=0.2322 sec\n",
      "Epoch [368 / 2000]: Train Loss=0.1918, Val Cor=0.2822, Time=0.2327 sec\n",
      "Epoch [369 / 2000]: Train Loss=0.1918, Val Cor=0.2822, Time=0.2328 sec\n",
      "Epoch [370 / 2000]: Train Loss=0.1918, Val Cor=0.2821, Time=0.2327 sec\n",
      "Epoch [371 / 2000]: Train Loss=0.1918, Val Cor=0.2821, Time=0.2326 sec\n",
      "Epoch [372 / 2000]: Train Loss=0.1918, Val Cor=0.2821, Time=0.2323 sec\n",
      "Epoch [373 / 2000]: Train Loss=0.1918, Val Cor=0.2819, Time=0.2330 sec\n",
      "Epoch [374 / 2000]: Train Loss=0.1918, Val Cor=0.2819, Time=0.2324 sec\n",
      "Epoch [375 / 2000]: Train Loss=0.1918, Val Cor=0.2818, Time=0.2327 sec\n",
      "Epoch [376 / 2000]: Train Loss=0.1917, Val Cor=0.2818, Time=0.2326 sec\n",
      "Epoch [377 / 2000]: Train Loss=0.1917, Val Cor=0.2818, Time=0.2325 sec\n",
      "Epoch [378 / 2000]: Train Loss=0.1917, Val Cor=0.2818, Time=0.2327 sec\n",
      "Epoch [379 / 2000]: Train Loss=0.1917, Val Cor=0.2820, Time=0.2327 sec\n",
      "Epoch [380 / 2000]: Train Loss=0.1917, Val Cor=0.2819, Time=0.2325 sec\n",
      "Epoch [381 / 2000]: Train Loss=0.1916, Val Cor=0.2820, Time=0.2328 sec\n",
      "Epoch [382 / 2000]: Train Loss=0.1916, Val Cor=0.2818, Time=0.2326 sec\n",
      "Epoch [383 / 2000]: Train Loss=0.1915, Val Cor=0.2818, Time=0.2326 sec\n",
      "Epoch [384 / 2000]: Train Loss=0.1915, Val Cor=0.2816, Time=0.2326 sec\n",
      "Epoch [385 / 2000]: Train Loss=0.1914, Val Cor=0.2817, Time=0.2327 sec\n",
      "Epoch [386 / 2000]: Train Loss=0.1914, Val Cor=0.2817, Time=0.2326 sec\n",
      "Epoch [387 / 2000]: Train Loss=0.1913, Val Cor=0.2818, Time=0.2324 sec\n",
      "Epoch [388 / 2000]: Train Loss=0.1912, Val Cor=0.2816, Time=0.2324 sec\n",
      "Epoch [389 / 2000]: Train Loss=0.1911, Val Cor=0.2815, Time=0.2323 sec\n",
      "Epoch [390 / 2000]: Train Loss=0.1910, Val Cor=0.2813, Time=0.2326 sec\n",
      "Epoch [391 / 2000]: Train Loss=0.1910, Val Cor=0.2814, Time=0.2325 sec\n",
      "Epoch [392 / 2000]: Train Loss=0.1914, Val Cor=0.2809, Time=0.2326 sec\n",
      "Epoch [393 / 2000]: Train Loss=0.1910, Val Cor=0.2807, Time=0.2328 sec\n",
      "Epoch [394 / 2000]: Train Loss=0.1905, Val Cor=0.2819, Time=0.2326 sec\n",
      "Epoch [395 / 2000]: Train Loss=0.1908, Val Cor=0.2809, Time=0.2326 sec\n",
      "Epoch [396 / 2000]: Train Loss=0.1904, Val Cor=0.2808, Time=0.2324 sec\n",
      "Epoch [397 / 2000]: Train Loss=0.1905, Val Cor=0.2820, Time=0.2326 sec\n",
      "Epoch [398 / 2000]: Train Loss=0.1914, Val Cor=0.2813, Time=0.2325 sec\n",
      "Epoch [399 / 2000]: Train Loss=0.1902, Val Cor=0.2810, Time=0.2325 sec\n",
      "Epoch [400 / 2000]: Train Loss=0.1899, Val Cor=0.2818, Time=0.2329 sec\n",
      "Epoch [401 / 2000]: Train Loss=0.1903, Val Cor=0.2820, Time=0.2333 sec\n",
      "Epoch [402 / 2000]: Train Loss=0.1895, Val Cor=0.2822, Time=0.2325 sec\n",
      "Epoch [403 / 2000]: Train Loss=0.1898, Val Cor=0.2825, Time=0.2328 sec\n",
      "Epoch [404 / 2000]: Train Loss=0.1941, Val Cor=0.2809, Time=0.2325 sec\n",
      "Epoch [405 / 2000]: Train Loss=0.1976, Val Cor=0.2803, Time=0.2327 sec\n",
      "Epoch [406 / 2000]: Train Loss=0.1959, Val Cor=0.2821, Time=0.2326 sec\n",
      "Epoch [407 / 2000]: Train Loss=0.1895, Val Cor=0.2825, Time=0.2326 sec\n",
      "Epoch [408 / 2000]: Train Loss=0.1957, Val Cor=0.2815, Time=0.2325 sec\n",
      "Epoch [409 / 2000]: Train Loss=0.1903, Val Cor=0.2802, Time=0.2328 sec\n",
      "Epoch [410 / 2000]: Train Loss=0.1913, Val Cor=0.2807, Time=0.2328 sec\n",
      "Epoch [411 / 2000]: Train Loss=0.1918, Val Cor=0.2801, Time=0.2324 sec\n",
      "Epoch [412 / 2000]: Train Loss=0.1918, Val Cor=0.2801, Time=0.2326 sec\n",
      "Epoch [413 / 2000]: Train Loss=0.1916, Val Cor=0.2805, Time=0.2327 sec\n",
      "Epoch [414 / 2000]: Train Loss=0.1915, Val Cor=0.2800, Time=0.2326 sec\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLPRegressor().to(device)\n",
    "model, best_model_ckpt = train_model_ESM(model, train_dataset, val_dataset, epochs=2000, batch_size=992, lr=5e-4, patience=100, device=device)\n",
    "model.load_state_dict(best_model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2909922784894928"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_esm(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 992/992 [00:04<00:00, 229.12it/s]\n",
      "Loading esm embeddings: 100%|██████████| 248/248 [00:01<00:00, 186.58it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences = df_train['sequence']\n",
    "fitness_list = df_train['DMS_score'].tolist()\n",
    "\n",
    "seq_train, seq_val, fitness_train, fitness_val = train_test_split(sequences, fitness_list, test_size=0.2, random_state=7)\n",
    "\n",
    "train_seq2name = train_seq2name = {seq: f'seq_{i}' for i, seq in enumerate(sequences)}\n",
    "fitness2idx = {fitness: fitness for idx, fitness in enumerate(fitness_list)}\n",
    "\n",
    "emb_dir = 'esm_embeddings_test'\n",
    "train_dataset = ProteinESMDataset(seq_train, train_seq2name, emb_dir, fitness_train, fitness2idx)\n",
    "val_dataset = ProteinESMDataset(seq_val, train_seq2name, emb_dir, fitness_val, fitness2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 3000]: Train Loss=6.2552, Val Cor=0.0558, Time=0.2694 sec\n",
      "model_updated\n",
      "Epoch [2 / 3000]: Train Loss=5.8030, Val Cor=0.1489, Time=0.1093 sec\n",
      "model_updated\n",
      "Epoch [3 / 3000]: Train Loss=5.8472, Val Cor=0.2020, Time=0.1083 sec\n",
      "model_updated\n",
      "Epoch [4 / 3000]: Train Loss=5.8279, Val Cor=0.2261, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [5 / 3000]: Train Loss=5.8601, Val Cor=0.2230, Time=0.1083 sec\n",
      "Epoch [6 / 3000]: Train Loss=5.8681, Val Cor=0.2686, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [7 / 3000]: Train Loss=5.8730, Val Cor=0.2436, Time=0.1079 sec\n",
      "Epoch [8 / 3000]: Train Loss=5.8383, Val Cor=0.2308, Time=0.1082 sec\n",
      "Epoch [9 / 3000]: Train Loss=5.8579, Val Cor=0.2353, Time=0.1079 sec\n",
      "Epoch [10 / 3000]: Train Loss=5.8012, Val Cor=0.2307, Time=0.1079 sec\n",
      "Epoch [11 / 3000]: Train Loss=5.7931, Val Cor=0.2187, Time=0.1079 sec\n",
      "Epoch [12 / 3000]: Train Loss=5.8069, Val Cor=0.2245, Time=0.1078 sec\n",
      "Epoch [13 / 3000]: Train Loss=5.7862, Val Cor=0.2220, Time=0.1079 sec\n",
      "Epoch [14 / 3000]: Train Loss=5.7708, Val Cor=0.2222, Time=0.1083 sec\n",
      "Epoch [15 / 3000]: Train Loss=5.7819, Val Cor=0.2285, Time=0.1079 sec\n",
      "Epoch [16 / 3000]: Train Loss=5.7874, Val Cor=0.2198, Time=0.1079 sec\n",
      "Epoch [17 / 3000]: Train Loss=5.7775, Val Cor=0.2196, Time=0.1078 sec\n",
      "Epoch [18 / 3000]: Train Loss=5.7980, Val Cor=0.2456, Time=0.1079 sec\n",
      "Epoch [19 / 3000]: Train Loss=5.7657, Val Cor=0.2313, Time=0.1078 sec\n",
      "Epoch [20 / 3000]: Train Loss=5.7575, Val Cor=0.2304, Time=0.1080 sec\n",
      "Epoch [21 / 3000]: Train Loss=5.7896, Val Cor=0.2437, Time=0.1078 sec\n",
      "Epoch [22 / 3000]: Train Loss=5.7602, Val Cor=0.2333, Time=0.1078 sec\n",
      "Epoch [23 / 3000]: Train Loss=5.7480, Val Cor=0.2342, Time=0.1077 sec\n",
      "Epoch [24 / 3000]: Train Loss=5.8059, Val Cor=0.2453, Time=0.1079 sec\n",
      "Epoch [25 / 3000]: Train Loss=5.8073, Val Cor=0.2485, Time=0.1077 sec\n",
      "Epoch [26 / 3000]: Train Loss=5.7709, Val Cor=0.2541, Time=0.1078 sec\n",
      "Epoch [27 / 3000]: Train Loss=5.7225, Val Cor=0.2568, Time=0.1078 sec\n",
      "Epoch [28 / 3000]: Train Loss=5.7135, Val Cor=0.2612, Time=0.1079 sec\n",
      "Epoch [29 / 3000]: Train Loss=5.6708, Val Cor=0.2618, Time=0.1077 sec\n",
      "Epoch [30 / 3000]: Train Loss=5.6597, Val Cor=0.2653, Time=0.1079 sec\n",
      "Epoch [31 / 3000]: Train Loss=5.6437, Val Cor=0.2775, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [32 / 3000]: Train Loss=5.6526, Val Cor=0.2808, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [33 / 3000]: Train Loss=5.6053, Val Cor=0.2740, Time=0.1076 sec\n",
      "Epoch [34 / 3000]: Train Loss=5.5675, Val Cor=0.2867, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [35 / 3000]: Train Loss=5.5205, Val Cor=0.2987, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [36 / 3000]: Train Loss=5.5855, Val Cor=0.3100, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [37 / 3000]: Train Loss=5.7787, Val Cor=0.3118, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [38 / 3000]: Train Loss=5.5634, Val Cor=0.3199, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [39 / 3000]: Train Loss=5.5650, Val Cor=0.3237, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [40 / 3000]: Train Loss=5.6478, Val Cor=0.3317, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [41 / 3000]: Train Loss=5.5938, Val Cor=0.3331, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [42 / 3000]: Train Loss=5.5178, Val Cor=0.3367, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [43 / 3000]: Train Loss=5.5136, Val Cor=0.3448, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [44 / 3000]: Train Loss=5.5221, Val Cor=0.3539, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [45 / 3000]: Train Loss=5.5564, Val Cor=0.3462, Time=0.1077 sec\n",
      "Epoch [46 / 3000]: Train Loss=5.4708, Val Cor=0.3512, Time=0.1077 sec\n",
      "Epoch [47 / 3000]: Train Loss=5.5221, Val Cor=0.3524, Time=0.1077 sec\n",
      "Epoch [48 / 3000]: Train Loss=5.5111, Val Cor=0.3589, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [49 / 3000]: Train Loss=5.4715, Val Cor=0.3573, Time=0.1075 sec\n",
      "Epoch [50 / 3000]: Train Loss=5.4384, Val Cor=0.3627, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [51 / 3000]: Train Loss=5.5321, Val Cor=0.3588, Time=0.1075 sec\n",
      "Epoch [52 / 3000]: Train Loss=5.4127, Val Cor=0.3604, Time=0.1077 sec\n",
      "Epoch [53 / 3000]: Train Loss=5.4273, Val Cor=0.3652, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [54 / 3000]: Train Loss=5.4126, Val Cor=0.3673, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [55 / 3000]: Train Loss=5.4453, Val Cor=0.3669, Time=0.1075 sec\n",
      "Epoch [56 / 3000]: Train Loss=5.4739, Val Cor=0.3678, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [57 / 3000]: Train Loss=5.3655, Val Cor=0.3767, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [58 / 3000]: Train Loss=5.3665, Val Cor=0.3866, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [59 / 3000]: Train Loss=5.3549, Val Cor=0.3888, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [60 / 3000]: Train Loss=5.3468, Val Cor=0.3932, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [61 / 3000]: Train Loss=5.3477, Val Cor=0.3928, Time=0.1077 sec\n",
      "Epoch [62 / 3000]: Train Loss=5.2786, Val Cor=0.3961, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [63 / 3000]: Train Loss=5.4601, Val Cor=0.3960, Time=0.1077 sec\n",
      "Epoch [64 / 3000]: Train Loss=5.2890, Val Cor=0.3982, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [65 / 3000]: Train Loss=5.4276, Val Cor=0.3996, Time=0.1074 sec\n",
      "model_updated\n",
      "Epoch [66 / 3000]: Train Loss=5.3240, Val Cor=0.3984, Time=0.1076 sec\n",
      "Epoch [67 / 3000]: Train Loss=5.2924, Val Cor=0.4003, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [68 / 3000]: Train Loss=5.4154, Val Cor=0.4040, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [69 / 3000]: Train Loss=5.2793, Val Cor=0.4052, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [70 / 3000]: Train Loss=5.4582, Val Cor=0.4070, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [71 / 3000]: Train Loss=5.4513, Val Cor=0.4016, Time=0.1076 sec\n",
      "Epoch [72 / 3000]: Train Loss=5.3328, Val Cor=0.4052, Time=0.1078 sec\n",
      "Epoch [73 / 3000]: Train Loss=5.4293, Val Cor=0.4031, Time=0.1077 sec\n",
      "Epoch [74 / 3000]: Train Loss=5.4329, Val Cor=0.4042, Time=0.1077 sec\n",
      "Epoch [75 / 3000]: Train Loss=5.3869, Val Cor=0.4065, Time=0.1076 sec\n",
      "Epoch [76 / 3000]: Train Loss=5.3842, Val Cor=0.4102, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [77 / 3000]: Train Loss=5.3691, Val Cor=0.4083, Time=0.1077 sec\n",
      "Epoch [78 / 3000]: Train Loss=5.4009, Val Cor=0.4130, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [79 / 3000]: Train Loss=5.4556, Val Cor=0.4153, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [80 / 3000]: Train Loss=5.2532, Val Cor=0.4148, Time=0.1080 sec\n",
      "Epoch [81 / 3000]: Train Loss=5.3655, Val Cor=0.4174, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [82 / 3000]: Train Loss=5.3115, Val Cor=0.4187, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [83 / 3000]: Train Loss=5.3303, Val Cor=0.4181, Time=0.1076 sec\n",
      "Epoch [84 / 3000]: Train Loss=5.2162, Val Cor=0.4205, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [85 / 3000]: Train Loss=5.2819, Val Cor=0.4209, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [86 / 3000]: Train Loss=5.1883, Val Cor=0.4217, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [87 / 3000]: Train Loss=5.3357, Val Cor=0.4226, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [88 / 3000]: Train Loss=5.3600, Val Cor=0.4246, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [89 / 3000]: Train Loss=5.3761, Val Cor=0.4238, Time=0.1075 sec\n",
      "Epoch [90 / 3000]: Train Loss=5.4329, Val Cor=0.4229, Time=0.1086 sec\n",
      "Epoch [91 / 3000]: Train Loss=5.3453, Val Cor=0.4273, Time=0.1089 sec\n",
      "model_updated\n",
      "Epoch [92 / 3000]: Train Loss=5.1842, Val Cor=0.4276, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [93 / 3000]: Train Loss=5.1226, Val Cor=0.4315, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [94 / 3000]: Train Loss=5.1819, Val Cor=0.4330, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [95 / 3000]: Train Loss=5.2359, Val Cor=0.4329, Time=0.1075 sec\n",
      "Epoch [96 / 3000]: Train Loss=5.3105, Val Cor=0.4338, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [97 / 3000]: Train Loss=5.5233, Val Cor=0.4351, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [98 / 3000]: Train Loss=5.3188, Val Cor=0.4325, Time=0.1079 sec\n",
      "Epoch [99 / 3000]: Train Loss=5.2456, Val Cor=0.4308, Time=0.1085 sec\n",
      "Epoch [100 / 3000]: Train Loss=5.2228, Val Cor=0.4307, Time=0.1084 sec\n",
      "Epoch [101 / 3000]: Train Loss=5.4484, Val Cor=0.4275, Time=0.1077 sec\n",
      "Epoch [102 / 3000]: Train Loss=5.3202, Val Cor=0.4300, Time=0.1079 sec\n",
      "Epoch [103 / 3000]: Train Loss=5.2987, Val Cor=0.4330, Time=0.1076 sec\n",
      "Epoch [104 / 3000]: Train Loss=5.3195, Val Cor=0.4304, Time=0.1079 sec\n",
      "Epoch [105 / 3000]: Train Loss=5.3110, Val Cor=0.4296, Time=0.1079 sec\n",
      "Epoch [106 / 3000]: Train Loss=5.1871, Val Cor=0.4331, Time=0.1081 sec\n",
      "Epoch [107 / 3000]: Train Loss=5.1454, Val Cor=0.4344, Time=0.1078 sec\n",
      "Epoch [108 / 3000]: Train Loss=5.1371, Val Cor=0.4363, Time=0.1087 sec\n",
      "model_updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [109 / 3000]: Train Loss=5.0959, Val Cor=0.4371, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [110 / 3000]: Train Loss=5.2392, Val Cor=0.4384, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [111 / 3000]: Train Loss=5.1112, Val Cor=0.4484, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [112 / 3000]: Train Loss=5.2505, Val Cor=0.4475, Time=0.1078 sec\n",
      "Epoch [113 / 3000]: Train Loss=5.4193, Val Cor=0.4372, Time=0.1077 sec\n",
      "Epoch [114 / 3000]: Train Loss=5.2468, Val Cor=0.4388, Time=0.1079 sec\n",
      "Epoch [115 / 3000]: Train Loss=5.1568, Val Cor=0.4362, Time=0.1077 sec\n",
      "Epoch [116 / 3000]: Train Loss=5.1336, Val Cor=0.4398, Time=0.1079 sec\n",
      "Epoch [117 / 3000]: Train Loss=5.2261, Val Cor=0.4399, Time=0.1076 sec\n",
      "Epoch [118 / 3000]: Train Loss=5.1420, Val Cor=0.4417, Time=0.1081 sec\n",
      "Epoch [119 / 3000]: Train Loss=5.1320, Val Cor=0.4476, Time=0.1082 sec\n",
      "Epoch [120 / 3000]: Train Loss=5.1369, Val Cor=0.4426, Time=0.1077 sec\n",
      "Epoch [121 / 3000]: Train Loss=5.1625, Val Cor=0.4443, Time=0.1076 sec\n",
      "Epoch [122 / 3000]: Train Loss=5.1584, Val Cor=0.4460, Time=0.1078 sec\n",
      "Epoch [123 / 3000]: Train Loss=5.1308, Val Cor=0.4425, Time=0.1075 sec\n",
      "Epoch [124 / 3000]: Train Loss=5.0634, Val Cor=0.4495, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [125 / 3000]: Train Loss=5.1178, Val Cor=0.4522, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [126 / 3000]: Train Loss=5.0637, Val Cor=0.4521, Time=0.1077 sec\n",
      "Epoch [127 / 3000]: Train Loss=5.0445, Val Cor=0.4522, Time=0.1077 sec\n",
      "Epoch [128 / 3000]: Train Loss=5.0983, Val Cor=0.4490, Time=0.1078 sec\n",
      "Epoch [129 / 3000]: Train Loss=5.3565, Val Cor=0.4501, Time=0.1078 sec\n",
      "Epoch [130 / 3000]: Train Loss=5.2026, Val Cor=0.4505, Time=0.1078 sec\n",
      "Epoch [131 / 3000]: Train Loss=5.1437, Val Cor=0.4533, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [132 / 3000]: Train Loss=5.2391, Val Cor=0.4518, Time=0.1077 sec\n",
      "Epoch [133 / 3000]: Train Loss=5.2524, Val Cor=0.4472, Time=0.1075 sec\n",
      "Epoch [134 / 3000]: Train Loss=5.2339, Val Cor=0.4490, Time=0.1078 sec\n",
      "Epoch [135 / 3000]: Train Loss=5.0985, Val Cor=0.4533, Time=0.1079 sec\n",
      "Epoch [136 / 3000]: Train Loss=5.0482, Val Cor=0.4545, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [137 / 3000]: Train Loss=4.9404, Val Cor=0.4561, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [138 / 3000]: Train Loss=5.1752, Val Cor=0.4563, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [139 / 3000]: Train Loss=5.0740, Val Cor=0.4538, Time=0.1076 sec\n",
      "Epoch [140 / 3000]: Train Loss=5.0510, Val Cor=0.4544, Time=0.1078 sec\n",
      "Epoch [141 / 3000]: Train Loss=5.2268, Val Cor=0.4572, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [142 / 3000]: Train Loss=4.9922, Val Cor=0.4553, Time=0.1076 sec\n",
      "Epoch [143 / 3000]: Train Loss=5.1169, Val Cor=0.4564, Time=0.1077 sec\n",
      "Epoch [144 / 3000]: Train Loss=5.1083, Val Cor=0.4560, Time=0.1078 sec\n",
      "Epoch [145 / 3000]: Train Loss=5.0295, Val Cor=0.4553, Time=0.1075 sec\n",
      "Epoch [146 / 3000]: Train Loss=4.9997, Val Cor=0.4547, Time=0.1079 sec\n",
      "Epoch [147 / 3000]: Train Loss=4.9398, Val Cor=0.4575, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [148 / 3000]: Train Loss=4.9855, Val Cor=0.4573, Time=0.1076 sec\n",
      "Epoch [149 / 3000]: Train Loss=5.0502, Val Cor=0.4543, Time=0.1076 sec\n",
      "Epoch [150 / 3000]: Train Loss=5.0934, Val Cor=0.4547, Time=0.1077 sec\n",
      "Epoch [151 / 3000]: Train Loss=5.2376, Val Cor=0.4591, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [152 / 3000]: Train Loss=5.2566, Val Cor=0.4508, Time=0.1080 sec\n",
      "Epoch [153 / 3000]: Train Loss=5.1591, Val Cor=0.4519, Time=0.1077 sec\n",
      "Epoch [154 / 3000]: Train Loss=5.0056, Val Cor=0.4526, Time=0.1079 sec\n",
      "Epoch [155 / 3000]: Train Loss=5.0344, Val Cor=0.4512, Time=0.1091 sec\n",
      "Epoch [156 / 3000]: Train Loss=4.9656, Val Cor=0.4548, Time=0.1087 sec\n",
      "Epoch [157 / 3000]: Train Loss=5.2084, Val Cor=0.4583, Time=0.1085 sec\n",
      "Epoch [158 / 3000]: Train Loss=5.1209, Val Cor=0.4576, Time=0.1087 sec\n",
      "Epoch [159 / 3000]: Train Loss=5.0395, Val Cor=0.4556, Time=0.1084 sec\n",
      "Epoch [160 / 3000]: Train Loss=4.9746, Val Cor=0.4537, Time=0.1085 sec\n",
      "Epoch [161 / 3000]: Train Loss=4.9946, Val Cor=0.4547, Time=0.1083 sec\n",
      "Epoch [162 / 3000]: Train Loss=5.0577, Val Cor=0.4543, Time=0.1086 sec\n",
      "Epoch [163 / 3000]: Train Loss=5.2879, Val Cor=0.4548, Time=0.1083 sec\n",
      "Epoch [164 / 3000]: Train Loss=5.1188, Val Cor=0.4533, Time=0.1093 sec\n",
      "Epoch [165 / 3000]: Train Loss=5.1466, Val Cor=0.4532, Time=0.1087 sec\n",
      "Epoch [166 / 3000]: Train Loss=5.0464, Val Cor=0.4564, Time=0.1089 sec\n",
      "Epoch [167 / 3000]: Train Loss=4.9885, Val Cor=0.4584, Time=0.1084 sec\n",
      "Epoch [168 / 3000]: Train Loss=5.0232, Val Cor=0.4558, Time=0.1083 sec\n",
      "Epoch [169 / 3000]: Train Loss=4.9616, Val Cor=0.4574, Time=0.1084 sec\n",
      "Epoch [170 / 3000]: Train Loss=4.9648, Val Cor=0.4578, Time=0.1086 sec\n",
      "Epoch [171 / 3000]: Train Loss=5.2262, Val Cor=0.4596, Time=0.1098 sec\n",
      "model_updated\n",
      "Epoch [172 / 3000]: Train Loss=5.2160, Val Cor=0.4565, Time=0.1083 sec\n",
      "Epoch [173 / 3000]: Train Loss=4.9237, Val Cor=0.4551, Time=0.1085 sec\n",
      "Epoch [174 / 3000]: Train Loss=5.0617, Val Cor=0.4534, Time=0.1084 sec\n",
      "Epoch [175 / 3000]: Train Loss=5.1158, Val Cor=0.4550, Time=0.1083 sec\n",
      "Epoch [176 / 3000]: Train Loss=5.0167, Val Cor=0.4565, Time=0.1082 sec\n",
      "Epoch [177 / 3000]: Train Loss=4.9846, Val Cor=0.4586, Time=0.1079 sec\n",
      "Epoch [178 / 3000]: Train Loss=5.1708, Val Cor=0.4592, Time=0.1079 sec\n",
      "Epoch [179 / 3000]: Train Loss=5.2317, Val Cor=0.4581, Time=0.1080 sec\n",
      "Epoch [180 / 3000]: Train Loss=5.2255, Val Cor=0.4575, Time=0.1080 sec\n",
      "Epoch [181 / 3000]: Train Loss=5.0924, Val Cor=0.4561, Time=0.1078 sec\n",
      "Epoch [182 / 3000]: Train Loss=5.1944, Val Cor=0.4552, Time=0.1079 sec\n",
      "Epoch [183 / 3000]: Train Loss=4.9885, Val Cor=0.4542, Time=0.1083 sec\n",
      "Epoch [184 / 3000]: Train Loss=4.9597, Val Cor=0.4567, Time=0.1079 sec\n",
      "Epoch [185 / 3000]: Train Loss=4.9289, Val Cor=0.4578, Time=0.1079 sec\n",
      "Epoch [186 / 3000]: Train Loss=4.8691, Val Cor=0.4592, Time=0.1076 sec\n",
      "Epoch [187 / 3000]: Train Loss=4.9982, Val Cor=0.4585, Time=0.1078 sec\n",
      "Epoch [188 / 3000]: Train Loss=5.0060, Val Cor=0.4603, Time=0.1076 sec\n",
      "model_updated\n",
      "Epoch [189 / 3000]: Train Loss=4.9538, Val Cor=0.4581, Time=0.1077 sec\n",
      "Epoch [190 / 3000]: Train Loss=4.9937, Val Cor=0.4587, Time=0.1075 sec\n",
      "Epoch [191 / 3000]: Train Loss=4.9774, Val Cor=0.4573, Time=0.1077 sec\n",
      "Epoch [192 / 3000]: Train Loss=4.9379, Val Cor=0.4605, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [193 / 3000]: Train Loss=4.9758, Val Cor=0.4594, Time=0.1076 sec\n",
      "Epoch [194 / 3000]: Train Loss=4.9688, Val Cor=0.4603, Time=0.1078 sec\n",
      "Epoch [195 / 3000]: Train Loss=5.0268, Val Cor=0.4596, Time=0.1075 sec\n",
      "Epoch [196 / 3000]: Train Loss=5.0186, Val Cor=0.4610, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [197 / 3000]: Train Loss=4.8533, Val Cor=0.4616, Time=0.1075 sec\n",
      "model_updated\n",
      "Epoch [198 / 3000]: Train Loss=5.0194, Val Cor=0.4615, Time=0.1074 sec\n",
      "Epoch [199 / 3000]: Train Loss=4.8875, Val Cor=0.4594, Time=0.1077 sec\n",
      "Epoch [200 / 3000]: Train Loss=4.8788, Val Cor=0.4615, Time=0.1077 sec\n",
      "Epoch [201 / 3000]: Train Loss=4.9741, Val Cor=0.4601, Time=0.2399 sec\n",
      "Epoch [202 / 3000]: Train Loss=4.7720, Val Cor=0.4625, Time=0.1093 sec\n",
      "model_updated\n",
      "Epoch [203 / 3000]: Train Loss=4.9615, Val Cor=0.4621, Time=0.1082 sec\n",
      "Epoch [204 / 3000]: Train Loss=5.0842, Val Cor=0.4629, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [205 / 3000]: Train Loss=4.8509, Val Cor=0.4623, Time=0.1081 sec\n",
      "Epoch [206 / 3000]: Train Loss=5.1377, Val Cor=0.4604, Time=0.1081 sec\n",
      "Epoch [207 / 3000]: Train Loss=5.0668, Val Cor=0.4617, Time=0.1081 sec\n",
      "Epoch [208 / 3000]: Train Loss=4.8600, Val Cor=0.4624, Time=0.1079 sec\n",
      "Epoch [209 / 3000]: Train Loss=4.8586, Val Cor=0.4621, Time=0.1080 sec\n",
      "Epoch [210 / 3000]: Train Loss=4.9474, Val Cor=0.4625, Time=0.1080 sec\n",
      "Epoch [211 / 3000]: Train Loss=4.8362, Val Cor=0.4633, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [212 / 3000]: Train Loss=4.8309, Val Cor=0.4617, Time=0.1080 sec\n",
      "Epoch [213 / 3000]: Train Loss=5.0998, Val Cor=0.4638, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [214 / 3000]: Train Loss=4.9511, Val Cor=0.4630, Time=0.1077 sec\n",
      "Epoch [215 / 3000]: Train Loss=4.9349, Val Cor=0.4618, Time=0.1079 sec\n",
      "Epoch [216 / 3000]: Train Loss=5.1786, Val Cor=0.4629, Time=0.1078 sec\n",
      "Epoch [217 / 3000]: Train Loss=4.9829, Val Cor=0.4628, Time=0.1080 sec\n",
      "Epoch [218 / 3000]: Train Loss=4.8907, Val Cor=0.4622, Time=0.1079 sec\n",
      "Epoch [219 / 3000]: Train Loss=4.8517, Val Cor=0.4617, Time=0.1080 sec\n",
      "Epoch [220 / 3000]: Train Loss=4.8725, Val Cor=0.4613, Time=0.1079 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [221 / 3000]: Train Loss=4.9161, Val Cor=0.4597, Time=0.1080 sec\n",
      "Epoch [222 / 3000]: Train Loss=4.9145, Val Cor=0.4590, Time=0.1078 sec\n",
      "Epoch [223 / 3000]: Train Loss=4.9003, Val Cor=0.4600, Time=0.1079 sec\n",
      "Epoch [224 / 3000]: Train Loss=4.8852, Val Cor=0.4609, Time=0.1079 sec\n",
      "Epoch [225 / 3000]: Train Loss=4.8140, Val Cor=0.4601, Time=0.1080 sec\n",
      "Epoch [226 / 3000]: Train Loss=4.8726, Val Cor=0.4627, Time=0.1079 sec\n",
      "Epoch [227 / 3000]: Train Loss=4.9194, Val Cor=0.4617, Time=0.1080 sec\n",
      "Epoch [228 / 3000]: Train Loss=4.9948, Val Cor=0.4638, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [229 / 3000]: Train Loss=4.8954, Val Cor=0.4639, Time=0.1082 sec\n",
      "model_updated\n",
      "Epoch [230 / 3000]: Train Loss=5.0697, Val Cor=0.4641, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [231 / 3000]: Train Loss=4.9290, Val Cor=0.4635, Time=0.1080 sec\n",
      "Epoch [232 / 3000]: Train Loss=4.8747, Val Cor=0.4632, Time=0.1080 sec\n",
      "Epoch [233 / 3000]: Train Loss=4.8378, Val Cor=0.4632, Time=0.1078 sec\n",
      "Epoch [234 / 3000]: Train Loss=4.8845, Val Cor=0.4630, Time=0.1081 sec\n",
      "Epoch [235 / 3000]: Train Loss=4.9195, Val Cor=0.4608, Time=0.1080 sec\n",
      "Epoch [236 / 3000]: Train Loss=4.7972, Val Cor=0.4619, Time=0.1077 sec\n",
      "Epoch [237 / 3000]: Train Loss=4.8792, Val Cor=0.4641, Time=0.1081 sec\n",
      "Epoch [238 / 3000]: Train Loss=4.7964, Val Cor=0.4625, Time=0.1084 sec\n",
      "Epoch [239 / 3000]: Train Loss=4.8464, Val Cor=0.4639, Time=0.1080 sec\n",
      "Epoch [240 / 3000]: Train Loss=5.1139, Val Cor=0.4645, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [241 / 3000]: Train Loss=4.9958, Val Cor=0.4641, Time=0.1082 sec\n",
      "Epoch [242 / 3000]: Train Loss=4.7958, Val Cor=0.4652, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [243 / 3000]: Train Loss=4.7898, Val Cor=0.4644, Time=0.1080 sec\n",
      "Epoch [244 / 3000]: Train Loss=4.8723, Val Cor=0.4621, Time=0.1077 sec\n",
      "Epoch [245 / 3000]: Train Loss=4.9317, Val Cor=0.4634, Time=0.1078 sec\n",
      "Epoch [246 / 3000]: Train Loss=4.8455, Val Cor=0.4655, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [247 / 3000]: Train Loss=5.1106, Val Cor=0.4632, Time=0.1081 sec\n",
      "Epoch [248 / 3000]: Train Loss=4.9395, Val Cor=0.4643, Time=0.1081 sec\n",
      "Epoch [249 / 3000]: Train Loss=4.9317, Val Cor=0.4624, Time=0.1078 sec\n",
      "Epoch [250 / 3000]: Train Loss=4.9569, Val Cor=0.4643, Time=0.1078 sec\n",
      "Epoch [251 / 3000]: Train Loss=5.0308, Val Cor=0.4642, Time=0.1078 sec\n",
      "Epoch [252 / 3000]: Train Loss=4.9428, Val Cor=0.4651, Time=0.1078 sec\n",
      "Epoch [253 / 3000]: Train Loss=5.1463, Val Cor=0.4681, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [254 / 3000]: Train Loss=4.8082, Val Cor=0.4678, Time=0.1078 sec\n",
      "Epoch [255 / 3000]: Train Loss=4.9491, Val Cor=0.4681, Time=0.1076 sec\n",
      "Epoch [256 / 3000]: Train Loss=4.9228, Val Cor=0.4657, Time=0.1078 sec\n",
      "Epoch [257 / 3000]: Train Loss=4.8436, Val Cor=0.4659, Time=0.1079 sec\n",
      "Epoch [258 / 3000]: Train Loss=4.7962, Val Cor=0.4670, Time=0.1077 sec\n",
      "Epoch [259 / 3000]: Train Loss=4.8493, Val Cor=0.4677, Time=0.1077 sec\n",
      "Epoch [260 / 3000]: Train Loss=4.7584, Val Cor=0.4638, Time=0.1077 sec\n",
      "Epoch [261 / 3000]: Train Loss=4.9819, Val Cor=0.4624, Time=0.1077 sec\n",
      "Epoch [262 / 3000]: Train Loss=4.8424, Val Cor=0.4626, Time=0.1076 sec\n",
      "Epoch [263 / 3000]: Train Loss=4.8792, Val Cor=0.4640, Time=0.1077 sec\n",
      "Epoch [264 / 3000]: Train Loss=4.8353, Val Cor=0.4614, Time=0.1083 sec\n",
      "Epoch [265 / 3000]: Train Loss=5.0371, Val Cor=0.4636, Time=0.1085 sec\n",
      "Epoch [266 / 3000]: Train Loss=5.0966, Val Cor=0.4617, Time=0.1085 sec\n",
      "Epoch [267 / 3000]: Train Loss=4.8173, Val Cor=0.4637, Time=0.1085 sec\n",
      "Epoch [268 / 3000]: Train Loss=5.0065, Val Cor=0.4650, Time=0.1079 sec\n",
      "Epoch [269 / 3000]: Train Loss=4.8898, Val Cor=0.4686, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [270 / 3000]: Train Loss=4.9311, Val Cor=0.4653, Time=0.1078 sec\n",
      "Epoch [271 / 3000]: Train Loss=4.7359, Val Cor=0.4654, Time=0.1079 sec\n",
      "Epoch [272 / 3000]: Train Loss=4.7280, Val Cor=0.4676, Time=0.1078 sec\n",
      "Epoch [273 / 3000]: Train Loss=5.0298, Val Cor=0.4666, Time=0.1078 sec\n",
      "Epoch [274 / 3000]: Train Loss=4.8758, Val Cor=0.4672, Time=0.1077 sec\n",
      "Epoch [275 / 3000]: Train Loss=4.8349, Val Cor=0.4682, Time=0.1078 sec\n",
      "Epoch [276 / 3000]: Train Loss=4.7795, Val Cor=0.4670, Time=0.1077 sec\n",
      "Epoch [277 / 3000]: Train Loss=4.8648, Val Cor=0.4675, Time=0.1076 sec\n",
      "Epoch [278 / 3000]: Train Loss=4.8956, Val Cor=0.4676, Time=0.1076 sec\n",
      "Epoch [279 / 3000]: Train Loss=4.8724, Val Cor=0.4659, Time=0.1078 sec\n",
      "Epoch [280 / 3000]: Train Loss=4.7208, Val Cor=0.4671, Time=0.1077 sec\n",
      "Epoch [281 / 3000]: Train Loss=4.9056, Val Cor=0.4674, Time=0.1076 sec\n",
      "Epoch [282 / 3000]: Train Loss=4.8058, Val Cor=0.4658, Time=0.1074 sec\n",
      "Epoch [283 / 3000]: Train Loss=4.7701, Val Cor=0.4668, Time=0.1077 sec\n",
      "Epoch [284 / 3000]: Train Loss=4.7067, Val Cor=0.4671, Time=0.1076 sec\n",
      "Epoch [285 / 3000]: Train Loss=4.9611, Val Cor=0.4675, Time=0.1075 sec\n",
      "Epoch [286 / 3000]: Train Loss=4.7445, Val Cor=0.4667, Time=0.1076 sec\n",
      "Epoch [287 / 3000]: Train Loss=4.8367, Val Cor=0.4682, Time=0.1083 sec\n",
      "Epoch [288 / 3000]: Train Loss=4.6805, Val Cor=0.4695, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [289 / 3000]: Train Loss=4.8608, Val Cor=0.4701, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [290 / 3000]: Train Loss=4.8353, Val Cor=0.4668, Time=0.1080 sec\n",
      "Epoch [291 / 3000]: Train Loss=4.7847, Val Cor=0.4668, Time=0.1082 sec\n",
      "Epoch [292 / 3000]: Train Loss=4.8594, Val Cor=0.4667, Time=0.1080 sec\n",
      "Epoch [293 / 3000]: Train Loss=4.8101, Val Cor=0.4647, Time=0.1082 sec\n",
      "Epoch [294 / 3000]: Train Loss=4.7329, Val Cor=0.4663, Time=0.1081 sec\n",
      "Epoch [295 / 3000]: Train Loss=4.6788, Val Cor=0.4660, Time=0.1081 sec\n",
      "Epoch [296 / 3000]: Train Loss=4.8918, Val Cor=0.4662, Time=0.1080 sec\n",
      "Epoch [297 / 3000]: Train Loss=4.8609, Val Cor=0.4654, Time=0.1081 sec\n",
      "Epoch [298 / 3000]: Train Loss=4.7314, Val Cor=0.4633, Time=0.1081 sec\n",
      "Epoch [299 / 3000]: Train Loss=4.7053, Val Cor=0.4634, Time=0.1080 sec\n",
      "Epoch [300 / 3000]: Train Loss=4.8640, Val Cor=0.4657, Time=0.1081 sec\n",
      "Epoch [301 / 3000]: Train Loss=4.7258, Val Cor=0.4639, Time=0.1082 sec\n",
      "Epoch [302 / 3000]: Train Loss=4.7128, Val Cor=0.4632, Time=0.1081 sec\n",
      "Epoch [303 / 3000]: Train Loss=4.7452, Val Cor=0.4625, Time=0.1082 sec\n",
      "Epoch [304 / 3000]: Train Loss=4.7071, Val Cor=0.4633, Time=0.1081 sec\n",
      "Epoch [305 / 3000]: Train Loss=4.6332, Val Cor=0.4632, Time=0.1082 sec\n",
      "Epoch [306 / 3000]: Train Loss=4.6902, Val Cor=0.4627, Time=0.1080 sec\n",
      "Epoch [307 / 3000]: Train Loss=4.7502, Val Cor=0.4642, Time=0.1082 sec\n",
      "Epoch [308 / 3000]: Train Loss=5.0408, Val Cor=0.4663, Time=0.1079 sec\n",
      "Epoch [309 / 3000]: Train Loss=4.7859, Val Cor=0.4671, Time=0.1081 sec\n",
      "Epoch [310 / 3000]: Train Loss=5.0541, Val Cor=0.4686, Time=0.1080 sec\n",
      "Epoch [311 / 3000]: Train Loss=4.7325, Val Cor=0.4705, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [312 / 3000]: Train Loss=4.7107, Val Cor=0.4673, Time=0.1082 sec\n",
      "Epoch [313 / 3000]: Train Loss=4.7737, Val Cor=0.4679, Time=0.1082 sec\n",
      "Epoch [314 / 3000]: Train Loss=4.9204, Val Cor=0.4674, Time=0.1080 sec\n",
      "Epoch [315 / 3000]: Train Loss=4.7634, Val Cor=0.4681, Time=0.1082 sec\n",
      "Epoch [316 / 3000]: Train Loss=4.6602, Val Cor=0.4666, Time=0.1082 sec\n",
      "Epoch [317 / 3000]: Train Loss=4.8004, Val Cor=0.4648, Time=0.1084 sec\n",
      "Epoch [318 / 3000]: Train Loss=4.8478, Val Cor=0.4649, Time=0.1082 sec\n",
      "Epoch [319 / 3000]: Train Loss=4.8697, Val Cor=0.4672, Time=0.1082 sec\n",
      "Epoch [320 / 3000]: Train Loss=4.8860, Val Cor=0.4680, Time=0.1081 sec\n",
      "Epoch [321 / 3000]: Train Loss=4.8853, Val Cor=0.4666, Time=0.1083 sec\n",
      "Epoch [322 / 3000]: Train Loss=4.7172, Val Cor=0.4669, Time=0.1081 sec\n",
      "Epoch [323 / 3000]: Train Loss=4.7079, Val Cor=0.4663, Time=0.1082 sec\n",
      "Epoch [324 / 3000]: Train Loss=4.7332, Val Cor=0.4658, Time=0.1082 sec\n",
      "Epoch [325 / 3000]: Train Loss=5.1464, Val Cor=0.4654, Time=0.1082 sec\n",
      "Epoch [326 / 3000]: Train Loss=4.9020, Val Cor=0.4675, Time=0.1082 sec\n",
      "Epoch [327 / 3000]: Train Loss=4.7247, Val Cor=0.4669, Time=0.1081 sec\n",
      "Epoch [328 / 3000]: Train Loss=4.6926, Val Cor=0.4670, Time=0.1082 sec\n",
      "Epoch [329 / 3000]: Train Loss=5.0125, Val Cor=0.4654, Time=0.1081 sec\n",
      "Epoch [330 / 3000]: Train Loss=4.9247, Val Cor=0.4654, Time=0.1082 sec\n",
      "Epoch [331 / 3000]: Train Loss=4.8198, Val Cor=0.4670, Time=0.1082 sec\n",
      "Epoch [332 / 3000]: Train Loss=4.9013, Val Cor=0.4674, Time=0.1082 sec\n",
      "Epoch [333 / 3000]: Train Loss=4.6448, Val Cor=0.4660, Time=0.1082 sec\n",
      "Epoch [334 / 3000]: Train Loss=4.9044, Val Cor=0.4683, Time=0.1084 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [335 / 3000]: Train Loss=4.7858, Val Cor=0.4654, Time=0.1082 sec\n",
      "Epoch [336 / 3000]: Train Loss=4.9274, Val Cor=0.4705, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [337 / 3000]: Train Loss=4.7801, Val Cor=0.4679, Time=0.1082 sec\n",
      "Epoch [338 / 3000]: Train Loss=4.6855, Val Cor=0.4680, Time=0.1081 sec\n",
      "Epoch [339 / 3000]: Train Loss=4.9331, Val Cor=0.4671, Time=0.1083 sec\n",
      "Epoch [340 / 3000]: Train Loss=4.8754, Val Cor=0.4658, Time=0.1082 sec\n",
      "Epoch [341 / 3000]: Train Loss=4.9138, Val Cor=0.4684, Time=0.1083 sec\n",
      "Epoch [342 / 3000]: Train Loss=4.8502, Val Cor=0.4717, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [343 / 3000]: Train Loss=4.7303, Val Cor=0.4711, Time=0.1082 sec\n",
      "Epoch [344 / 3000]: Train Loss=4.7200, Val Cor=0.4691, Time=0.1081 sec\n",
      "Epoch [345 / 3000]: Train Loss=4.9700, Val Cor=0.4688, Time=0.1082 sec\n",
      "Epoch [346 / 3000]: Train Loss=4.7752, Val Cor=0.4672, Time=0.1080 sec\n",
      "Epoch [347 / 3000]: Train Loss=4.7719, Val Cor=0.4694, Time=0.1080 sec\n",
      "Epoch [348 / 3000]: Train Loss=4.9107, Val Cor=0.4711, Time=0.1080 sec\n",
      "Epoch [349 / 3000]: Train Loss=4.8145, Val Cor=0.4719, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [350 / 3000]: Train Loss=4.6989, Val Cor=0.4710, Time=0.1081 sec\n",
      "Epoch [351 / 3000]: Train Loss=4.7307, Val Cor=0.4704, Time=0.1082 sec\n",
      "Epoch [352 / 3000]: Train Loss=4.6555, Val Cor=0.4674, Time=0.1081 sec\n",
      "Epoch [353 / 3000]: Train Loss=4.6352, Val Cor=0.4661, Time=0.1086 sec\n",
      "Epoch [354 / 3000]: Train Loss=4.5874, Val Cor=0.4671, Time=0.1082 sec\n",
      "Epoch [355 / 3000]: Train Loss=4.7542, Val Cor=0.4648, Time=0.1082 sec\n",
      "Epoch [356 / 3000]: Train Loss=4.6938, Val Cor=0.4683, Time=0.1080 sec\n",
      "Epoch [357 / 3000]: Train Loss=4.7911, Val Cor=0.4685, Time=0.1083 sec\n",
      "Epoch [358 / 3000]: Train Loss=5.1385, Val Cor=0.4696, Time=0.1079 sec\n",
      "Epoch [359 / 3000]: Train Loss=4.8315, Val Cor=0.4694, Time=0.1082 sec\n",
      "Epoch [360 / 3000]: Train Loss=4.6857, Val Cor=0.4694, Time=0.1080 sec\n",
      "Epoch [361 / 3000]: Train Loss=4.8496, Val Cor=0.4703, Time=0.1083 sec\n",
      "Epoch [362 / 3000]: Train Loss=4.6844, Val Cor=0.4683, Time=0.1079 sec\n",
      "Epoch [363 / 3000]: Train Loss=4.7957, Val Cor=0.4703, Time=0.1083 sec\n",
      "Epoch [364 / 3000]: Train Loss=5.0036, Val Cor=0.4725, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [365 / 3000]: Train Loss=4.9995, Val Cor=0.4727, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [366 / 3000]: Train Loss=4.7340, Val Cor=0.4731, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [367 / 3000]: Train Loss=4.8214, Val Cor=0.4733, Time=0.1082 sec\n",
      "model_updated\n",
      "Epoch [368 / 3000]: Train Loss=4.6774, Val Cor=0.4721, Time=0.1081 sec\n",
      "Epoch [369 / 3000]: Train Loss=4.5663, Val Cor=0.4692, Time=0.1084 sec\n",
      "Epoch [370 / 3000]: Train Loss=4.7314, Val Cor=0.4683, Time=0.1081 sec\n",
      "Epoch [371 / 3000]: Train Loss=4.6187, Val Cor=0.4688, Time=0.1081 sec\n",
      "Epoch [372 / 3000]: Train Loss=4.5872, Val Cor=0.4688, Time=0.1081 sec\n",
      "Epoch [373 / 3000]: Train Loss=4.7330, Val Cor=0.4686, Time=0.1082 sec\n",
      "Epoch [374 / 3000]: Train Loss=4.8626, Val Cor=0.4706, Time=0.1081 sec\n",
      "Epoch [375 / 3000]: Train Loss=4.7597, Val Cor=0.4695, Time=0.1082 sec\n",
      "Epoch [376 / 3000]: Train Loss=4.7333, Val Cor=0.4696, Time=0.1080 sec\n",
      "Epoch [377 / 3000]: Train Loss=4.7012, Val Cor=0.4712, Time=0.1080 sec\n",
      "Epoch [378 / 3000]: Train Loss=4.5967, Val Cor=0.4712, Time=0.1082 sec\n",
      "Epoch [379 / 3000]: Train Loss=4.6530, Val Cor=0.4706, Time=0.1081 sec\n",
      "Epoch [380 / 3000]: Train Loss=4.9209, Val Cor=0.4683, Time=0.1081 sec\n",
      "Epoch [381 / 3000]: Train Loss=4.7801, Val Cor=0.4679, Time=0.1081 sec\n",
      "Epoch [382 / 3000]: Train Loss=4.8893, Val Cor=0.4691, Time=0.1082 sec\n",
      "Epoch [383 / 3000]: Train Loss=4.6574, Val Cor=0.4743, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [384 / 3000]: Train Loss=4.6966, Val Cor=0.4760, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [385 / 3000]: Train Loss=4.6272, Val Cor=0.4755, Time=0.1081 sec\n",
      "Epoch [386 / 3000]: Train Loss=4.6831, Val Cor=0.4725, Time=0.1082 sec\n",
      "Epoch [387 / 3000]: Train Loss=4.6829, Val Cor=0.4694, Time=0.1082 sec\n",
      "Epoch [388 / 3000]: Train Loss=4.6425, Val Cor=0.4706, Time=0.1081 sec\n",
      "Epoch [389 / 3000]: Train Loss=4.6601, Val Cor=0.4720, Time=0.1082 sec\n",
      "Epoch [390 / 3000]: Train Loss=4.5533, Val Cor=0.4718, Time=0.1081 sec\n",
      "Epoch [391 / 3000]: Train Loss=4.8178, Val Cor=0.4709, Time=0.1082 sec\n",
      "Epoch [392 / 3000]: Train Loss=4.6305, Val Cor=0.4707, Time=0.1079 sec\n",
      "Epoch [393 / 3000]: Train Loss=4.7138, Val Cor=0.4719, Time=0.1081 sec\n",
      "Epoch [394 / 3000]: Train Loss=4.8675, Val Cor=0.4724, Time=0.1079 sec\n",
      "Epoch [395 / 3000]: Train Loss=4.5957, Val Cor=0.4731, Time=0.1079 sec\n",
      "Epoch [396 / 3000]: Train Loss=4.6400, Val Cor=0.4722, Time=0.1080 sec\n",
      "Epoch [397 / 3000]: Train Loss=4.6811, Val Cor=0.4720, Time=0.1080 sec\n",
      "Epoch [398 / 3000]: Train Loss=4.6705, Val Cor=0.4722, Time=0.1081 sec\n",
      "Epoch [399 / 3000]: Train Loss=4.6348, Val Cor=0.4717, Time=0.1079 sec\n",
      "Epoch [400 / 3000]: Train Loss=4.7451, Val Cor=0.4746, Time=0.1081 sec\n",
      "Epoch [401 / 3000]: Train Loss=4.8047, Val Cor=0.4742, Time=0.1082 sec\n",
      "Epoch [402 / 3000]: Train Loss=4.5738, Val Cor=0.4722, Time=0.1081 sec\n",
      "Epoch [403 / 3000]: Train Loss=4.8517, Val Cor=0.4729, Time=0.1083 sec\n",
      "Epoch [404 / 3000]: Train Loss=4.7141, Val Cor=0.4744, Time=0.1079 sec\n",
      "Epoch [405 / 3000]: Train Loss=4.6816, Val Cor=0.4752, Time=0.1082 sec\n",
      "Epoch [406 / 3000]: Train Loss=4.6832, Val Cor=0.4749, Time=0.1080 sec\n",
      "Epoch [407 / 3000]: Train Loss=4.6086, Val Cor=0.4756, Time=0.1082 sec\n",
      "Epoch [408 / 3000]: Train Loss=4.7297, Val Cor=0.4745, Time=0.1079 sec\n",
      "Epoch [409 / 3000]: Train Loss=4.5695, Val Cor=0.4751, Time=0.1083 sec\n",
      "Epoch [410 / 3000]: Train Loss=4.5605, Val Cor=0.4730, Time=0.1081 sec\n",
      "Epoch [411 / 3000]: Train Loss=4.5358, Val Cor=0.4709, Time=0.1082 sec\n",
      "Epoch [412 / 3000]: Train Loss=5.0528, Val Cor=0.4741, Time=0.1081 sec\n",
      "Epoch [413 / 3000]: Train Loss=4.8692, Val Cor=0.4750, Time=0.1083 sec\n",
      "Epoch [414 / 3000]: Train Loss=4.6848, Val Cor=0.4748, Time=0.1081 sec\n",
      "Epoch [415 / 3000]: Train Loss=4.8385, Val Cor=0.4759, Time=0.1082 sec\n",
      "Epoch [416 / 3000]: Train Loss=4.7190, Val Cor=0.4764, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [417 / 3000]: Train Loss=4.7944, Val Cor=0.4762, Time=0.1082 sec\n",
      "Epoch [418 / 3000]: Train Loss=4.8944, Val Cor=0.4777, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [419 / 3000]: Train Loss=4.7091, Val Cor=0.4760, Time=0.1081 sec\n",
      "Epoch [420 / 3000]: Train Loss=4.6030, Val Cor=0.4780, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [421 / 3000]: Train Loss=4.6711, Val Cor=0.4765, Time=0.1081 sec\n",
      "Epoch [422 / 3000]: Train Loss=4.8203, Val Cor=0.4760, Time=0.1081 sec\n",
      "Epoch [423 / 3000]: Train Loss=4.6960, Val Cor=0.4760, Time=0.1082 sec\n",
      "Epoch [424 / 3000]: Train Loss=4.6047, Val Cor=0.4773, Time=0.1080 sec\n",
      "Epoch [425 / 3000]: Train Loss=4.7451, Val Cor=0.4754, Time=0.1080 sec\n",
      "Epoch [426 / 3000]: Train Loss=4.6082, Val Cor=0.4752, Time=0.1081 sec\n",
      "Epoch [427 / 3000]: Train Loss=5.2753, Val Cor=0.4765, Time=0.1083 sec\n",
      "Epoch [428 / 3000]: Train Loss=4.9335, Val Cor=0.4772, Time=0.1082 sec\n",
      "Epoch [429 / 3000]: Train Loss=4.7886, Val Cor=0.4775, Time=0.1081 sec\n",
      "Epoch [430 / 3000]: Train Loss=4.6882, Val Cor=0.4767, Time=0.1081 sec\n",
      "Epoch [431 / 3000]: Train Loss=4.5837, Val Cor=0.4771, Time=0.1082 sec\n",
      "Epoch [432 / 3000]: Train Loss=4.5836, Val Cor=0.4767, Time=0.1082 sec\n",
      "Epoch [433 / 3000]: Train Loss=4.6443, Val Cor=0.4771, Time=0.1081 sec\n",
      "Epoch [434 / 3000]: Train Loss=4.7887, Val Cor=0.4764, Time=0.1082 sec\n",
      "Epoch [435 / 3000]: Train Loss=4.6514, Val Cor=0.4755, Time=0.1079 sec\n",
      "Epoch [436 / 3000]: Train Loss=4.6251, Val Cor=0.4735, Time=0.1079 sec\n",
      "Epoch [437 / 3000]: Train Loss=4.4346, Val Cor=0.4756, Time=0.1080 sec\n",
      "Epoch [438 / 3000]: Train Loss=4.7251, Val Cor=0.4775, Time=0.1084 sec\n",
      "Epoch [439 / 3000]: Train Loss=4.6717, Val Cor=0.4773, Time=0.1081 sec\n",
      "Epoch [440 / 3000]: Train Loss=4.6938, Val Cor=0.4778, Time=0.1079 sec\n",
      "Epoch [441 / 3000]: Train Loss=4.8578, Val Cor=0.4791, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [442 / 3000]: Train Loss=4.7385, Val Cor=0.4782, Time=0.1081 sec\n",
      "Epoch [443 / 3000]: Train Loss=4.7553, Val Cor=0.4770, Time=0.1080 sec\n",
      "Epoch [444 / 3000]: Train Loss=4.6709, Val Cor=0.4777, Time=0.1080 sec\n",
      "Epoch [445 / 3000]: Train Loss=4.4756, Val Cor=0.4758, Time=0.1081 sec\n",
      "Epoch [446 / 3000]: Train Loss=4.9348, Val Cor=0.4768, Time=0.1082 sec\n",
      "Epoch [447 / 3000]: Train Loss=4.5319, Val Cor=0.4787, Time=0.1082 sec\n",
      "Epoch [448 / 3000]: Train Loss=4.6374, Val Cor=0.4746, Time=0.1081 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [449 / 3000]: Train Loss=4.8037, Val Cor=0.4784, Time=0.1081 sec\n",
      "Epoch [450 / 3000]: Train Loss=4.8290, Val Cor=0.4764, Time=0.1081 sec\n",
      "Epoch [451 / 3000]: Train Loss=4.6147, Val Cor=0.4760, Time=0.1083 sec\n",
      "Epoch [452 / 3000]: Train Loss=4.7000, Val Cor=0.4775, Time=0.1080 sec\n",
      "Epoch [453 / 3000]: Train Loss=4.6943, Val Cor=0.4765, Time=0.1079 sec\n",
      "Epoch [454 / 3000]: Train Loss=4.6690, Val Cor=0.4783, Time=0.1079 sec\n",
      "Epoch [455 / 3000]: Train Loss=4.6459, Val Cor=0.4790, Time=0.1081 sec\n",
      "Epoch [456 / 3000]: Train Loss=5.0259, Val Cor=0.4771, Time=0.1081 sec\n",
      "Epoch [457 / 3000]: Train Loss=4.7722, Val Cor=0.4755, Time=0.1082 sec\n",
      "Epoch [458 / 3000]: Train Loss=4.8014, Val Cor=0.4781, Time=0.1080 sec\n",
      "Epoch [459 / 3000]: Train Loss=5.0603, Val Cor=0.4792, Time=0.1086 sec\n",
      "model_updated\n",
      "Epoch [460 / 3000]: Train Loss=4.7118, Val Cor=0.4796, Time=0.1088 sec\n",
      "model_updated\n",
      "Epoch [461 / 3000]: Train Loss=4.5261, Val Cor=0.4772, Time=0.1085 sec\n",
      "Epoch [462 / 3000]: Train Loss=4.5588, Val Cor=0.4758, Time=0.1081 sec\n",
      "Epoch [463 / 3000]: Train Loss=4.6148, Val Cor=0.4747, Time=0.1082 sec\n",
      "Epoch [464 / 3000]: Train Loss=4.8523, Val Cor=0.4785, Time=0.1081 sec\n",
      "Epoch [465 / 3000]: Train Loss=4.9076, Val Cor=0.4770, Time=0.1081 sec\n",
      "Epoch [466 / 3000]: Train Loss=4.6412, Val Cor=0.4770, Time=0.1083 sec\n",
      "Epoch [467 / 3000]: Train Loss=4.5553, Val Cor=0.4778, Time=0.1081 sec\n",
      "Epoch [468 / 3000]: Train Loss=4.5009, Val Cor=0.4764, Time=0.1081 sec\n",
      "Epoch [469 / 3000]: Train Loss=4.4896, Val Cor=0.4757, Time=0.1080 sec\n",
      "Epoch [470 / 3000]: Train Loss=4.5092, Val Cor=0.4744, Time=0.1080 sec\n",
      "Epoch [471 / 3000]: Train Loss=4.6033, Val Cor=0.4767, Time=0.1083 sec\n",
      "Epoch [472 / 3000]: Train Loss=4.6759, Val Cor=0.4782, Time=0.1081 sec\n",
      "Epoch [473 / 3000]: Train Loss=4.7731, Val Cor=0.4788, Time=0.1081 sec\n",
      "Epoch [474 / 3000]: Train Loss=4.8291, Val Cor=0.4770, Time=0.1081 sec\n",
      "Epoch [475 / 3000]: Train Loss=4.8449, Val Cor=0.4780, Time=0.1081 sec\n",
      "Epoch [476 / 3000]: Train Loss=4.7646, Val Cor=0.4785, Time=0.1081 sec\n",
      "Epoch [477 / 3000]: Train Loss=4.7108, Val Cor=0.4785, Time=0.1082 sec\n",
      "Epoch [478 / 3000]: Train Loss=4.7910, Val Cor=0.4768, Time=0.1082 sec\n",
      "Epoch [479 / 3000]: Train Loss=4.7114, Val Cor=0.4793, Time=0.1083 sec\n",
      "Epoch [480 / 3000]: Train Loss=4.6745, Val Cor=0.4776, Time=0.1081 sec\n",
      "Epoch [481 / 3000]: Train Loss=4.4969, Val Cor=0.4776, Time=0.1082 sec\n",
      "Epoch [482 / 3000]: Train Loss=4.4545, Val Cor=0.4761, Time=0.1080 sec\n",
      "Epoch [483 / 3000]: Train Loss=4.8088, Val Cor=0.4750, Time=0.1082 sec\n",
      "Epoch [484 / 3000]: Train Loss=4.9461, Val Cor=0.4762, Time=0.1081 sec\n",
      "Epoch [485 / 3000]: Train Loss=4.7848, Val Cor=0.4790, Time=0.1082 sec\n",
      "Epoch [486 / 3000]: Train Loss=4.7603, Val Cor=0.4792, Time=0.1082 sec\n",
      "Epoch [487 / 3000]: Train Loss=4.6779, Val Cor=0.4776, Time=0.1083 sec\n",
      "Epoch [488 / 3000]: Train Loss=4.7393, Val Cor=0.4788, Time=0.1082 sec\n",
      "Epoch [489 / 3000]: Train Loss=4.6980, Val Cor=0.4779, Time=0.1081 sec\n",
      "Epoch [490 / 3000]: Train Loss=4.5461, Val Cor=0.4801, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [491 / 3000]: Train Loss=4.6044, Val Cor=0.4798, Time=0.1079 sec\n",
      "Epoch [492 / 3000]: Train Loss=4.5928, Val Cor=0.4800, Time=0.1080 sec\n",
      "Epoch [493 / 3000]: Train Loss=4.7126, Val Cor=0.4791, Time=0.1081 sec\n",
      "Epoch [494 / 3000]: Train Loss=4.3573, Val Cor=0.4797, Time=0.1080 sec\n",
      "Epoch [495 / 3000]: Train Loss=4.4600, Val Cor=0.4780, Time=0.1081 sec\n",
      "Epoch [496 / 3000]: Train Loss=4.9217, Val Cor=0.4786, Time=0.1081 sec\n",
      "Epoch [497 / 3000]: Train Loss=4.6241, Val Cor=0.4814, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [498 / 3000]: Train Loss=4.7701, Val Cor=0.4787, Time=0.1080 sec\n",
      "Epoch [499 / 3000]: Train Loss=4.6594, Val Cor=0.4809, Time=0.1081 sec\n",
      "Epoch [500 / 3000]: Train Loss=4.5754, Val Cor=0.4796, Time=0.1080 sec\n",
      "Epoch [501 / 3000]: Train Loss=4.5913, Val Cor=0.4799, Time=0.1080 sec\n",
      "Epoch [502 / 3000]: Train Loss=4.7366, Val Cor=0.4874, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [503 / 3000]: Train Loss=4.6841, Val Cor=0.4784, Time=0.1079 sec\n",
      "Epoch [504 / 3000]: Train Loss=4.5597, Val Cor=0.4790, Time=0.1082 sec\n",
      "Epoch [505 / 3000]: Train Loss=4.4141, Val Cor=0.4789, Time=0.1082 sec\n",
      "Epoch [506 / 3000]: Train Loss=4.9953, Val Cor=0.4818, Time=0.1083 sec\n",
      "Epoch [507 / 3000]: Train Loss=4.6420, Val Cor=0.4864, Time=0.1082 sec\n",
      "Epoch [508 / 3000]: Train Loss=4.5466, Val Cor=0.4799, Time=0.1081 sec\n",
      "Epoch [509 / 3000]: Train Loss=4.5853, Val Cor=0.4810, Time=0.1081 sec\n",
      "Epoch [510 / 3000]: Train Loss=4.4650, Val Cor=0.4837, Time=0.1080 sec\n",
      "Epoch [511 / 3000]: Train Loss=4.5746, Val Cor=0.4842, Time=0.1082 sec\n",
      "Epoch [512 / 3000]: Train Loss=4.6199, Val Cor=0.4755, Time=0.1080 sec\n",
      "Epoch [513 / 3000]: Train Loss=4.6375, Val Cor=0.4779, Time=0.1080 sec\n",
      "Epoch [514 / 3000]: Train Loss=4.6893, Val Cor=0.4768, Time=0.1081 sec\n",
      "Epoch [515 / 3000]: Train Loss=4.6466, Val Cor=0.4867, Time=0.1081 sec\n",
      "Epoch [516 / 3000]: Train Loss=4.7553, Val Cor=0.4743, Time=0.1081 sec\n",
      "Epoch [517 / 3000]: Train Loss=4.6177, Val Cor=0.4817, Time=0.1081 sec\n",
      "Epoch [518 / 3000]: Train Loss=4.6979, Val Cor=0.4865, Time=0.1080 sec\n",
      "Epoch [519 / 3000]: Train Loss=4.6623, Val Cor=0.4732, Time=0.1081 sec\n",
      "Epoch [520 / 3000]: Train Loss=4.5432, Val Cor=0.4836, Time=0.1082 sec\n",
      "Epoch [521 / 3000]: Train Loss=4.4913, Val Cor=0.4782, Time=0.1080 sec\n",
      "Epoch [522 / 3000]: Train Loss=4.7172, Val Cor=0.4833, Time=0.1080 sec\n",
      "Epoch [523 / 3000]: Train Loss=4.7346, Val Cor=0.4669, Time=0.1089 sec\n",
      "Epoch [524 / 3000]: Train Loss=4.6566, Val Cor=0.1881, Time=0.1086 sec\n",
      "Epoch [525 / 3000]: Train Loss=4.4364, Val Cor=0.4836, Time=0.1084 sec\n",
      "Epoch [526 / 3000]: Train Loss=4.3557, Val Cor=0.4760, Time=0.1083 sec\n",
      "Epoch [527 / 3000]: Train Loss=4.6154, Val Cor=0.4730, Time=0.1082 sec\n",
      "Epoch [528 / 3000]: Train Loss=4.6816, Val Cor=0.4836, Time=0.1087 sec\n",
      "Epoch [529 / 3000]: Train Loss=4.5869, Val Cor=0.4688, Time=0.1084 sec\n",
      "Epoch [530 / 3000]: Train Loss=4.5255, Val Cor=0.4724, Time=0.1085 sec\n",
      "Epoch [531 / 3000]: Train Loss=4.6304, Val Cor=0.4661, Time=0.1084 sec\n",
      "Epoch [532 / 3000]: Train Loss=4.5012, Val Cor=0.4864, Time=0.1086 sec\n",
      "Epoch [533 / 3000]: Train Loss=4.2654, Val Cor=0.4830, Time=0.1082 sec\n",
      "Epoch [534 / 3000]: Train Loss=4.5171, Val Cor=0.4788, Time=0.1087 sec\n",
      "Epoch [535 / 3000]: Train Loss=4.4465, Val Cor=0.4842, Time=0.1084 sec\n",
      "Epoch [536 / 3000]: Train Loss=4.3676, Val Cor=0.4830, Time=0.1085 sec\n",
      "Epoch [537 / 3000]: Train Loss=4.4086, Val Cor=0.4813, Time=0.1085 sec\n",
      "Epoch [538 / 3000]: Train Loss=4.4924, Val Cor=0.4363, Time=0.1083 sec\n",
      "Epoch [539 / 3000]: Train Loss=4.4246, Val Cor=0.4856, Time=0.1083 sec\n",
      "Epoch [540 / 3000]: Train Loss=4.8435, Val Cor=0.4823, Time=0.1084 sec\n",
      "Epoch [541 / 3000]: Train Loss=4.6683, Val Cor=0.4861, Time=0.1084 sec\n",
      "Epoch [542 / 3000]: Train Loss=4.3953, Val Cor=0.4771, Time=0.1085 sec\n",
      "Epoch [543 / 3000]: Train Loss=4.6806, Val Cor=0.4820, Time=0.1083 sec\n",
      "Epoch [544 / 3000]: Train Loss=4.4792, Val Cor=0.4466, Time=0.1084 sec\n",
      "Epoch [545 / 3000]: Train Loss=4.3520, Val Cor=0.4785, Time=0.1082 sec\n",
      "Epoch [546 / 3000]: Train Loss=4.5399, Val Cor=0.4780, Time=0.1084 sec\n",
      "Epoch [547 / 3000]: Train Loss=4.9086, Val Cor=0.4859, Time=0.1084 sec\n",
      "Epoch [548 / 3000]: Train Loss=4.5342, Val Cor=0.4845, Time=0.1086 sec\n",
      "Epoch [549 / 3000]: Train Loss=4.5176, Val Cor=0.4493, Time=0.1083 sec\n",
      "Epoch [550 / 3000]: Train Loss=4.5595, Val Cor=0.4778, Time=0.1084 sec\n",
      "Epoch [551 / 3000]: Train Loss=4.4358, Val Cor=0.4856, Time=0.1084 sec\n",
      "Epoch [552 / 3000]: Train Loss=4.4255, Val Cor=0.4645, Time=0.1085 sec\n",
      "Epoch [553 / 3000]: Train Loss=4.3866, Val Cor=0.4814, Time=0.1082 sec\n",
      "Epoch [554 / 3000]: Train Loss=4.5931, Val Cor=0.4841, Time=0.1084 sec\n",
      "Epoch [555 / 3000]: Train Loss=4.4915, Val Cor=0.4532, Time=0.1083 sec\n",
      "Epoch [556 / 3000]: Train Loss=4.5842, Val Cor=0.4824, Time=0.1082 sec\n",
      "Epoch [557 / 3000]: Train Loss=4.4833, Val Cor=0.4825, Time=0.1084 sec\n",
      "Epoch [558 / 3000]: Train Loss=4.4544, Val Cor=0.4864, Time=0.1084 sec\n",
      "Epoch [559 / 3000]: Train Loss=4.5621, Val Cor=0.4855, Time=0.1083 sec\n",
      "Epoch [560 / 3000]: Train Loss=4.4976, Val Cor=0.4697, Time=0.1084 sec\n",
      "Epoch [561 / 3000]: Train Loss=4.6667, Val Cor=-0.0708, Time=0.1084 sec\n",
      "Epoch [562 / 3000]: Train Loss=4.7530, Val Cor=0.3415, Time=0.1085 sec\n",
      "Epoch [563 / 3000]: Train Loss=4.8957, Val Cor=-0.3911, Time=0.1083 sec\n",
      "Epoch [564 / 3000]: Train Loss=4.7796, Val Cor=-0.1940, Time=0.1082 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [565 / 3000]: Train Loss=4.7868, Val Cor=0.3481, Time=0.1082 sec\n",
      "Epoch [566 / 3000]: Train Loss=4.7399, Val Cor=-0.4448, Time=0.1085 sec\n",
      "Epoch [567 / 3000]: Train Loss=4.6720, Val Cor=-0.4148, Time=0.1083 sec\n",
      "Epoch [568 / 3000]: Train Loss=4.8568, Val Cor=-0.0388, Time=0.1085 sec\n",
      "Epoch [569 / 3000]: Train Loss=4.4656, Val Cor=-0.2740, Time=0.1084 sec\n",
      "Epoch [570 / 3000]: Train Loss=4.6167, Val Cor=0.4825, Time=0.1084 sec\n",
      "Epoch [571 / 3000]: Train Loss=4.3528, Val Cor=0.1747, Time=0.1084 sec\n",
      "Epoch [572 / 3000]: Train Loss=4.5272, Val Cor=0.4814, Time=0.1085 sec\n",
      "Epoch [573 / 3000]: Train Loss=4.6887, Val Cor=-0.2987, Time=0.1082 sec\n",
      "Epoch [574 / 3000]: Train Loss=4.4980, Val Cor=0.1723, Time=0.1084 sec\n",
      "Epoch [575 / 3000]: Train Loss=4.8083, Val Cor=-0.1475, Time=0.1082 sec\n",
      "Epoch [576 / 3000]: Train Loss=4.7127, Val Cor=0.4819, Time=0.1084 sec\n",
      "Epoch [577 / 3000]: Train Loss=4.6770, Val Cor=0.2439, Time=0.1081 sec\n",
      "Epoch [578 / 3000]: Train Loss=4.5060, Val Cor=0.3800, Time=0.1083 sec\n",
      "Epoch [579 / 3000]: Train Loss=4.3560, Val Cor=0.4810, Time=0.1082 sec\n",
      "Epoch [580 / 3000]: Train Loss=4.6555, Val Cor=0.0497, Time=0.1086 sec\n",
      "Epoch [581 / 3000]: Train Loss=4.5460, Val Cor=-0.0494, Time=0.1082 sec\n",
      "Epoch [582 / 3000]: Train Loss=4.5665, Val Cor=0.4861, Time=0.1084 sec\n",
      "Epoch [583 / 3000]: Train Loss=4.6768, Val Cor=0.4768, Time=0.1082 sec\n",
      "Epoch [584 / 3000]: Train Loss=4.6938, Val Cor=0.4844, Time=0.1085 sec\n",
      "Epoch [585 / 3000]: Train Loss=4.7177, Val Cor=0.4843, Time=0.1082 sec\n",
      "Epoch [586 / 3000]: Train Loss=4.6802, Val Cor=0.4831, Time=0.1084 sec\n",
      "Epoch [587 / 3000]: Train Loss=4.7133, Val Cor=0.2066, Time=0.1081 sec\n",
      "Epoch [588 / 3000]: Train Loss=4.4291, Val Cor=0.3192, Time=0.1084 sec\n",
      "Epoch [589 / 3000]: Train Loss=4.3410, Val Cor=0.4716, Time=0.1082 sec\n",
      "Epoch [590 / 3000]: Train Loss=4.2739, Val Cor=0.4706, Time=0.1085 sec\n",
      "Epoch [591 / 3000]: Train Loss=4.4663, Val Cor=0.4478, Time=0.1082 sec\n",
      "Epoch [592 / 3000]: Train Loss=4.5318, Val Cor=0.4101, Time=0.1085 sec\n",
      "Epoch [593 / 3000]: Train Loss=4.5396, Val Cor=0.3385, Time=0.1083 sec\n",
      "Epoch [594 / 3000]: Train Loss=4.6913, Val Cor=-0.2347, Time=0.1083 sec\n",
      "Epoch [595 / 3000]: Train Loss=4.6552, Val Cor=-0.1555, Time=0.1082 sec\n",
      "Epoch [596 / 3000]: Train Loss=4.7893, Val Cor=-0.1703, Time=0.1083 sec\n",
      "Epoch [597 / 3000]: Train Loss=4.6435, Val Cor=0.1685, Time=0.1083 sec\n",
      "Epoch [598 / 3000]: Train Loss=4.6274, Val Cor=0.4925, Time=0.1083 sec\n",
      "model_updated\n",
      "Epoch [599 / 3000]: Train Loss=4.6591, Val Cor=0.1207, Time=0.1084 sec\n",
      "Epoch [600 / 3000]: Train Loss=4.6133, Val Cor=0.4753, Time=0.1082 sec\n",
      "Epoch [601 / 3000]: Train Loss=4.5229, Val Cor=0.4342, Time=0.1083 sec\n",
      "Epoch [602 / 3000]: Train Loss=4.5032, Val Cor=0.2765, Time=0.1085 sec\n",
      "Epoch [603 / 3000]: Train Loss=4.6133, Val Cor=0.4132, Time=0.1082 sec\n",
      "Epoch [604 / 3000]: Train Loss=4.5682, Val Cor=0.3808, Time=0.1083 sec\n",
      "Epoch [605 / 3000]: Train Loss=4.4676, Val Cor=0.4593, Time=0.1082 sec\n",
      "Epoch [606 / 3000]: Train Loss=4.4389, Val Cor=0.4647, Time=0.1083 sec\n",
      "Epoch [607 / 3000]: Train Loss=4.4740, Val Cor=0.3889, Time=0.1083 sec\n",
      "Epoch [608 / 3000]: Train Loss=4.3667, Val Cor=0.4863, Time=0.1081 sec\n",
      "Epoch [609 / 3000]: Train Loss=4.4851, Val Cor=0.3892, Time=0.1085 sec\n",
      "Epoch [610 / 3000]: Train Loss=4.4929, Val Cor=0.4066, Time=0.1082 sec\n",
      "Epoch [611 / 3000]: Train Loss=4.4973, Val Cor=0.4596, Time=0.1083 sec\n",
      "Epoch [612 / 3000]: Train Loss=4.4893, Val Cor=0.4454, Time=0.1083 sec\n",
      "Epoch [613 / 3000]: Train Loss=4.4862, Val Cor=0.4419, Time=0.1083 sec\n",
      "Epoch [614 / 3000]: Train Loss=4.2522, Val Cor=0.4287, Time=0.1083 sec\n",
      "Epoch [615 / 3000]: Train Loss=4.4379, Val Cor=0.4541, Time=0.1080 sec\n",
      "Epoch [616 / 3000]: Train Loss=4.3099, Val Cor=0.3486, Time=0.1082 sec\n",
      "Epoch [617 / 3000]: Train Loss=4.5813, Val Cor=0.3725, Time=0.1080 sec\n",
      "Epoch [618 / 3000]: Train Loss=4.5886, Val Cor=0.2176, Time=0.1080 sec\n",
      "Epoch [619 / 3000]: Train Loss=4.6688, Val Cor=0.3492, Time=0.1081 sec\n",
      "Epoch [620 / 3000]: Train Loss=4.5780, Val Cor=0.1119, Time=0.1082 sec\n",
      "Epoch [621 / 3000]: Train Loss=4.5489, Val Cor=0.4539, Time=0.1079 sec\n",
      "Epoch [622 / 3000]: Train Loss=4.4730, Val Cor=0.4772, Time=0.1080 sec\n",
      "Epoch [623 / 3000]: Train Loss=4.6385, Val Cor=0.4521, Time=0.1077 sec\n",
      "Epoch [624 / 3000]: Train Loss=4.3510, Val Cor=0.4663, Time=0.1079 sec\n",
      "Epoch [625 / 3000]: Train Loss=4.4219, Val Cor=0.4693, Time=0.1079 sec\n",
      "Epoch [626 / 3000]: Train Loss=4.3606, Val Cor=0.4656, Time=0.1081 sec\n",
      "Epoch [627 / 3000]: Train Loss=4.5086, Val Cor=0.1880, Time=0.1078 sec\n",
      "Epoch [628 / 3000]: Train Loss=4.7024, Val Cor=0.4365, Time=0.1081 sec\n",
      "Epoch [629 / 3000]: Train Loss=4.4726, Val Cor=0.2862, Time=0.1078 sec\n",
      "Epoch [630 / 3000]: Train Loss=4.4991, Val Cor=0.3769, Time=0.1080 sec\n",
      "Epoch [631 / 3000]: Train Loss=4.4954, Val Cor=0.4288, Time=0.1079 sec\n",
      "Epoch [632 / 3000]: Train Loss=4.4303, Val Cor=0.4337, Time=0.1081 sec\n",
      "Epoch [633 / 3000]: Train Loss=4.3050, Val Cor=0.3842, Time=0.1079 sec\n",
      "Epoch [634 / 3000]: Train Loss=4.3518, Val Cor=0.4301, Time=0.1081 sec\n",
      "Epoch [635 / 3000]: Train Loss=4.4304, Val Cor=0.3902, Time=0.1078 sec\n",
      "Epoch [636 / 3000]: Train Loss=4.3899, Val Cor=0.1428, Time=0.1082 sec\n",
      "Epoch [637 / 3000]: Train Loss=4.7109, Val Cor=0.4830, Time=0.1078 sec\n",
      "Epoch [638 / 3000]: Train Loss=4.6500, Val Cor=0.4347, Time=0.1081 sec\n",
      "Epoch [639 / 3000]: Train Loss=4.4981, Val Cor=0.4766, Time=0.1077 sec\n",
      "Epoch [640 / 3000]: Train Loss=4.5324, Val Cor=0.4830, Time=0.1081 sec\n",
      "Epoch [641 / 3000]: Train Loss=4.4626, Val Cor=0.4715, Time=0.1082 sec\n",
      "Epoch [642 / 3000]: Train Loss=4.4787, Val Cor=0.4802, Time=0.1085 sec\n",
      "Epoch [643 / 3000]: Train Loss=4.6734, Val Cor=0.4377, Time=0.1084 sec\n",
      "Epoch [644 / 3000]: Train Loss=4.5358, Val Cor=0.4723, Time=0.1080 sec\n",
      "Epoch [645 / 3000]: Train Loss=4.5427, Val Cor=0.4781, Time=0.1081 sec\n",
      "Epoch [646 / 3000]: Train Loss=4.3767, Val Cor=0.4121, Time=0.1081 sec\n",
      "Epoch [647 / 3000]: Train Loss=4.3829, Val Cor=0.4796, Time=0.1078 sec\n",
      "Epoch [648 / 3000]: Train Loss=4.3874, Val Cor=0.4382, Time=0.1078 sec\n",
      "Epoch [649 / 3000]: Train Loss=4.4544, Val Cor=0.4873, Time=0.1080 sec\n",
      "Epoch [650 / 3000]: Train Loss=4.2764, Val Cor=0.4783, Time=0.1082 sec\n",
      "Epoch [651 / 3000]: Train Loss=4.3595, Val Cor=0.4280, Time=0.1080 sec\n",
      "Epoch [652 / 3000]: Train Loss=4.4472, Val Cor=0.4597, Time=0.1080 sec\n",
      "Epoch [653 / 3000]: Train Loss=4.3421, Val Cor=0.4393, Time=0.1079 sec\n",
      "Epoch [654 / 3000]: Train Loss=4.3249, Val Cor=0.4356, Time=0.1081 sec\n",
      "Epoch [655 / 3000]: Train Loss=4.3115, Val Cor=0.4464, Time=0.1082 sec\n",
      "Epoch [656 / 3000]: Train Loss=4.2012, Val Cor=0.4883, Time=0.1079 sec\n",
      "Epoch [657 / 3000]: Train Loss=4.3614, Val Cor=0.4842, Time=0.1081 sec\n",
      "Epoch [658 / 3000]: Train Loss=4.2545, Val Cor=0.4770, Time=0.1080 sec\n",
      "Epoch [659 / 3000]: Train Loss=4.8979, Val Cor=0.4603, Time=0.1078 sec\n",
      "Epoch [660 / 3000]: Train Loss=4.4441, Val Cor=0.4835, Time=0.1079 sec\n",
      "Epoch [661 / 3000]: Train Loss=4.3621, Val Cor=0.3740, Time=0.1079 sec\n",
      "Epoch [662 / 3000]: Train Loss=4.5212, Val Cor=0.2855, Time=0.1079 sec\n",
      "Epoch [663 / 3000]: Train Loss=4.4784, Val Cor=0.3520, Time=0.1079 sec\n",
      "Epoch [664 / 3000]: Train Loss=4.3561, Val Cor=0.3750, Time=0.1079 sec\n",
      "Epoch [665 / 3000]: Train Loss=4.5295, Val Cor=0.3414, Time=0.1080 sec\n",
      "Epoch [666 / 3000]: Train Loss=4.3679, Val Cor=0.4415, Time=0.1078 sec\n",
      "Epoch [667 / 3000]: Train Loss=4.2597, Val Cor=0.4807, Time=0.1078 sec\n",
      "Epoch [668 / 3000]: Train Loss=4.3654, Val Cor=0.4630, Time=0.1080 sec\n",
      "Epoch [669 / 3000]: Train Loss=4.3461, Val Cor=0.4753, Time=0.1080 sec\n",
      "Epoch [670 / 3000]: Train Loss=4.6725, Val Cor=0.4730, Time=0.1080 sec\n",
      "Epoch [671 / 3000]: Train Loss=4.3862, Val Cor=0.4831, Time=0.1079 sec\n",
      "Epoch [672 / 3000]: Train Loss=4.5056, Val Cor=0.4645, Time=0.1082 sec\n",
      "Epoch [673 / 3000]: Train Loss=4.3420, Val Cor=0.4714, Time=0.1078 sec\n",
      "Epoch [674 / 3000]: Train Loss=4.5370, Val Cor=0.4737, Time=0.1082 sec\n",
      "Epoch [675 / 3000]: Train Loss=4.5050, Val Cor=0.4136, Time=0.1079 sec\n",
      "Epoch [676 / 3000]: Train Loss=4.4744, Val Cor=0.4862, Time=0.1079 sec\n",
      "Epoch [677 / 3000]: Train Loss=4.2981, Val Cor=0.4658, Time=0.1079 sec\n",
      "Epoch [678 / 3000]: Train Loss=4.3019, Val Cor=0.3301, Time=0.1079 sec\n",
      "Epoch [679 / 3000]: Train Loss=4.4622, Val Cor=0.4609, Time=0.1079 sec\n",
      "Epoch [680 / 3000]: Train Loss=4.3941, Val Cor=0.3972, Time=0.1081 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [681 / 3000]: Train Loss=4.3768, Val Cor=0.4341, Time=0.1078 sec\n",
      "Epoch [682 / 3000]: Train Loss=4.4184, Val Cor=0.4523, Time=0.1080 sec\n",
      "Epoch [683 / 3000]: Train Loss=4.4197, Val Cor=0.4876, Time=0.1080 sec\n",
      "Epoch [684 / 3000]: Train Loss=4.1867, Val Cor=0.4849, Time=0.1079 sec\n",
      "Epoch [685 / 3000]: Train Loss=4.3063, Val Cor=0.4827, Time=0.1080 sec\n",
      "Epoch [686 / 3000]: Train Loss=4.2999, Val Cor=0.4236, Time=0.1080 sec\n",
      "Epoch [687 / 3000]: Train Loss=4.4503, Val Cor=0.4349, Time=0.1077 sec\n",
      "Epoch [688 / 3000]: Train Loss=4.4013, Val Cor=0.3351, Time=0.1082 sec\n",
      "Epoch [689 / 3000]: Train Loss=4.4073, Val Cor=0.4829, Time=0.1079 sec\n",
      "Epoch [690 / 3000]: Train Loss=4.4046, Val Cor=0.3761, Time=0.1080 sec\n",
      "Epoch [691 / 3000]: Train Loss=4.4501, Val Cor=0.4912, Time=0.1079 sec\n",
      "Epoch [692 / 3000]: Train Loss=4.3129, Val Cor=0.4814, Time=0.1081 sec\n",
      "Epoch [693 / 3000]: Train Loss=4.4184, Val Cor=0.4921, Time=0.1078 sec\n",
      "Epoch [694 / 3000]: Train Loss=4.4972, Val Cor=0.4818, Time=0.1081 sec\n",
      "Epoch [695 / 3000]: Train Loss=4.5794, Val Cor=0.4826, Time=0.1080 sec\n",
      "Epoch [696 / 3000]: Train Loss=4.4505, Val Cor=0.4775, Time=0.1079 sec\n",
      "Epoch [697 / 3000]: Train Loss=4.3619, Val Cor=0.4582, Time=0.1080 sec\n",
      "Epoch [698 / 3000]: Train Loss=4.4124, Val Cor=0.4841, Time=0.1080 sec\n",
      "Epoch [699 / 3000]: Train Loss=4.3789, Val Cor=0.4747, Time=0.1080 sec\n",
      "Epoch [700 / 3000]: Train Loss=4.3192, Val Cor=0.4785, Time=0.1081 sec\n",
      "Epoch [701 / 3000]: Train Loss=4.3076, Val Cor=0.4300, Time=0.1079 sec\n",
      "Epoch [702 / 3000]: Train Loss=4.3349, Val Cor=0.4878, Time=0.1079 sec\n",
      "Epoch [703 / 3000]: Train Loss=4.1574, Val Cor=0.4811, Time=0.1077 sec\n",
      "Epoch [704 / 3000]: Train Loss=4.1933, Val Cor=0.4860, Time=0.1079 sec\n",
      "Epoch [705 / 3000]: Train Loss=4.5257, Val Cor=0.4577, Time=0.1079 sec\n",
      "Epoch [706 / 3000]: Train Loss=4.3110, Val Cor=0.5008, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [707 / 3000]: Train Loss=4.7101, Val Cor=0.4724, Time=0.1078 sec\n",
      "Epoch [708 / 3000]: Train Loss=4.4928, Val Cor=0.4832, Time=0.1079 sec\n",
      "Epoch [709 / 3000]: Train Loss=4.4231, Val Cor=0.4868, Time=0.1079 sec\n",
      "Epoch [710 / 3000]: Train Loss=4.3931, Val Cor=0.4681, Time=0.1082 sec\n",
      "Epoch [711 / 3000]: Train Loss=4.3607, Val Cor=0.4545, Time=0.1081 sec\n",
      "Epoch [712 / 3000]: Train Loss=4.6696, Val Cor=0.4822, Time=0.1081 sec\n",
      "Epoch [713 / 3000]: Train Loss=4.4685, Val Cor=0.4878, Time=0.1080 sec\n",
      "Epoch [714 / 3000]: Train Loss=4.5276, Val Cor=0.4466, Time=0.1079 sec\n",
      "Epoch [715 / 3000]: Train Loss=4.3250, Val Cor=0.4499, Time=0.1080 sec\n",
      "Epoch [716 / 3000]: Train Loss=4.4239, Val Cor=0.4813, Time=0.1080 sec\n",
      "Epoch [717 / 3000]: Train Loss=4.3818, Val Cor=0.3969, Time=0.1079 sec\n",
      "Epoch [718 / 3000]: Train Loss=4.3823, Val Cor=0.4231, Time=0.1085 sec\n",
      "Epoch [719 / 3000]: Train Loss=4.2143, Val Cor=0.4470, Time=0.1085 sec\n",
      "Epoch [720 / 3000]: Train Loss=4.1859, Val Cor=0.4556, Time=0.1080 sec\n",
      "Epoch [721 / 3000]: Train Loss=4.3232, Val Cor=0.4855, Time=0.1083 sec\n",
      "Epoch [722 / 3000]: Train Loss=5.9857, Val Cor=0.3286, Time=0.1084 sec\n",
      "Epoch [723 / 3000]: Train Loss=5.7190, Val Cor=0.4117, Time=0.1077 sec\n",
      "Epoch [724 / 3000]: Train Loss=5.4817, Val Cor=0.4686, Time=0.1082 sec\n",
      "Epoch [725 / 3000]: Train Loss=5.4161, Val Cor=0.4830, Time=0.1083 sec\n",
      "Epoch [726 / 3000]: Train Loss=4.8373, Val Cor=0.4836, Time=0.1080 sec\n",
      "Epoch [727 / 3000]: Train Loss=4.4561, Val Cor=0.4575, Time=0.1077 sec\n",
      "Epoch [728 / 3000]: Train Loss=4.6908, Val Cor=0.4735, Time=0.1079 sec\n",
      "Epoch [729 / 3000]: Train Loss=4.8245, Val Cor=0.4857, Time=0.1079 sec\n",
      "Epoch [730 / 3000]: Train Loss=4.6832, Val Cor=0.4828, Time=0.1078 sec\n",
      "Epoch [731 / 3000]: Train Loss=4.5545, Val Cor=0.4828, Time=0.1077 sec\n",
      "Epoch [732 / 3000]: Train Loss=4.4297, Val Cor=0.4790, Time=0.1079 sec\n",
      "Epoch [733 / 3000]: Train Loss=4.5314, Val Cor=0.4735, Time=0.1079 sec\n",
      "Epoch [734 / 3000]: Train Loss=4.4199, Val Cor=0.4764, Time=0.1080 sec\n",
      "Epoch [735 / 3000]: Train Loss=4.3035, Val Cor=0.4480, Time=0.1078 sec\n",
      "Epoch [736 / 3000]: Train Loss=4.5742, Val Cor=0.2772, Time=0.1081 sec\n",
      "Epoch [737 / 3000]: Train Loss=4.6513, Val Cor=0.4893, Time=0.1076 sec\n",
      "Epoch [738 / 3000]: Train Loss=4.6528, Val Cor=0.2312, Time=0.1080 sec\n",
      "Epoch [739 / 3000]: Train Loss=4.7260, Val Cor=0.4806, Time=0.1078 sec\n",
      "Epoch [740 / 3000]: Train Loss=4.4524, Val Cor=0.4792, Time=0.1079 sec\n",
      "Epoch [741 / 3000]: Train Loss=4.3414, Val Cor=0.4881, Time=0.1077 sec\n",
      "Epoch [742 / 3000]: Train Loss=4.4219, Val Cor=0.4104, Time=0.1079 sec\n",
      "Epoch [743 / 3000]: Train Loss=4.2957, Val Cor=0.4709, Time=0.1077 sec\n",
      "Epoch [744 / 3000]: Train Loss=4.2325, Val Cor=0.4907, Time=0.1077 sec\n",
      "Epoch [745 / 3000]: Train Loss=4.2867, Val Cor=0.4288, Time=0.1077 sec\n",
      "Epoch [746 / 3000]: Train Loss=4.2654, Val Cor=0.4845, Time=0.1077 sec\n",
      "Epoch [747 / 3000]: Train Loss=4.4210, Val Cor=0.4509, Time=0.1077 sec\n",
      "Epoch [748 / 3000]: Train Loss=4.5314, Val Cor=0.4838, Time=0.1078 sec\n",
      "Epoch [749 / 3000]: Train Loss=4.3729, Val Cor=0.4858, Time=0.1076 sec\n",
      "Epoch [750 / 3000]: Train Loss=4.4286, Val Cor=0.4837, Time=0.1078 sec\n",
      "Epoch [751 / 3000]: Train Loss=4.4912, Val Cor=0.4522, Time=0.1077 sec\n",
      "Epoch [752 / 3000]: Train Loss=4.3545, Val Cor=0.4919, Time=0.1079 sec\n",
      "Epoch [753 / 3000]: Train Loss=4.3717, Val Cor=0.4356, Time=0.1077 sec\n",
      "Epoch [754 / 3000]: Train Loss=4.2595, Val Cor=0.4782, Time=0.1079 sec\n",
      "Epoch [755 / 3000]: Train Loss=4.1677, Val Cor=0.4883, Time=0.1075 sec\n",
      "Epoch [756 / 3000]: Train Loss=4.3496, Val Cor=0.4894, Time=0.1078 sec\n",
      "Epoch [757 / 3000]: Train Loss=4.3306, Val Cor=0.4218, Time=0.1076 sec\n",
      "Epoch [758 / 3000]: Train Loss=4.3932, Val Cor=0.4872, Time=0.1079 sec\n",
      "Epoch [759 / 3000]: Train Loss=4.5212, Val Cor=0.4420, Time=0.1077 sec\n",
      "Epoch [760 / 3000]: Train Loss=4.2596, Val Cor=0.4335, Time=0.1079 sec\n",
      "Epoch [761 / 3000]: Train Loss=4.3406, Val Cor=0.4846, Time=0.1076 sec\n",
      "Epoch [762 / 3000]: Train Loss=4.3585, Val Cor=0.4724, Time=0.1078 sec\n",
      "Epoch [763 / 3000]: Train Loss=4.3312, Val Cor=0.2666, Time=0.1076 sec\n",
      "Epoch [764 / 3000]: Train Loss=4.2255, Val Cor=0.4800, Time=0.1078 sec\n",
      "Epoch [765 / 3000]: Train Loss=4.3918, Val Cor=0.4253, Time=0.1077 sec\n",
      "Epoch [766 / 3000]: Train Loss=4.3361, Val Cor=0.4776, Time=0.1081 sec\n",
      "Epoch [767 / 3000]: Train Loss=4.1769, Val Cor=0.4702, Time=0.1076 sec\n",
      "Epoch [768 / 3000]: Train Loss=4.2460, Val Cor=0.4325, Time=0.1077 sec\n",
      "Epoch [769 / 3000]: Train Loss=4.4400, Val Cor=0.3230, Time=0.1076 sec\n",
      "Epoch [770 / 3000]: Train Loss=4.6191, Val Cor=0.4704, Time=0.1079 sec\n",
      "Epoch [771 / 3000]: Train Loss=4.2730, Val Cor=0.4827, Time=0.1075 sec\n",
      "Epoch [772 / 3000]: Train Loss=4.2457, Val Cor=0.4824, Time=0.1078 sec\n",
      "Epoch [773 / 3000]: Train Loss=4.1070, Val Cor=0.4716, Time=0.1079 sec\n",
      "Epoch [774 / 3000]: Train Loss=4.6042, Val Cor=0.4816, Time=0.1078 sec\n",
      "Epoch [775 / 3000]: Train Loss=4.4289, Val Cor=0.4662, Time=0.1077 sec\n",
      "Epoch [776 / 3000]: Train Loss=4.2259, Val Cor=0.3975, Time=0.1078 sec\n",
      "Epoch [777 / 3000]: Train Loss=4.2528, Val Cor=0.4667, Time=0.1076 sec\n",
      "Epoch [778 / 3000]: Train Loss=4.1728, Val Cor=0.4888, Time=0.1079 sec\n",
      "Epoch [779 / 3000]: Train Loss=4.2870, Val Cor=0.4355, Time=0.1077 sec\n",
      "Epoch [780 / 3000]: Train Loss=4.3382, Val Cor=0.3718, Time=0.1078 sec\n",
      "Epoch [781 / 3000]: Train Loss=4.3058, Val Cor=0.4789, Time=0.1076 sec\n",
      "Epoch [782 / 3000]: Train Loss=4.1702, Val Cor=0.4876, Time=0.1079 sec\n",
      "Epoch [783 / 3000]: Train Loss=4.3278, Val Cor=0.4746, Time=0.1077 sec\n",
      "Epoch [784 / 3000]: Train Loss=4.2664, Val Cor=0.4689, Time=0.1079 sec\n",
      "Epoch [785 / 3000]: Train Loss=4.2535, Val Cor=0.4891, Time=0.1078 sec\n",
      "Epoch [786 / 3000]: Train Loss=4.5056, Val Cor=0.5066, Time=0.1079 sec\n",
      "model_updated\n",
      "Epoch [787 / 3000]: Train Loss=4.5021, Val Cor=0.4164, Time=0.1076 sec\n",
      "Epoch [788 / 3000]: Train Loss=4.3113, Val Cor=0.4615, Time=0.1078 sec\n",
      "Epoch [789 / 3000]: Train Loss=4.1487, Val Cor=0.4159, Time=0.1075 sec\n",
      "Epoch [790 / 3000]: Train Loss=4.2105, Val Cor=0.4784, Time=0.1078 sec\n",
      "Epoch [791 / 3000]: Train Loss=4.1309, Val Cor=0.4793, Time=0.1075 sec\n",
      "Epoch [792 / 3000]: Train Loss=4.3263, Val Cor=0.4815, Time=0.1077 sec\n",
      "Epoch [793 / 3000]: Train Loss=4.2505, Val Cor=0.4768, Time=0.1076 sec\n",
      "Epoch [794 / 3000]: Train Loss=4.4357, Val Cor=0.4889, Time=0.1078 sec\n",
      "Epoch [795 / 3000]: Train Loss=4.4615, Val Cor=0.4471, Time=0.1076 sec\n",
      "Epoch [796 / 3000]: Train Loss=4.1962, Val Cor=0.4402, Time=0.1077 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [797 / 3000]: Train Loss=4.2708, Val Cor=0.4672, Time=0.1076 sec\n",
      "Epoch [798 / 3000]: Train Loss=4.1475, Val Cor=0.4722, Time=0.1077 sec\n",
      "Epoch [799 / 3000]: Train Loss=4.1333, Val Cor=0.4841, Time=0.1076 sec\n",
      "Epoch [800 / 3000]: Train Loss=4.1149, Val Cor=0.4102, Time=0.1077 sec\n",
      "Epoch [801 / 3000]: Train Loss=4.0644, Val Cor=0.4145, Time=0.1076 sec\n",
      "Epoch [802 / 3000]: Train Loss=4.4493, Val Cor=0.4680, Time=0.1077 sec\n",
      "Epoch [803 / 3000]: Train Loss=4.7626, Val Cor=0.3135, Time=0.1077 sec\n",
      "Epoch [804 / 3000]: Train Loss=4.6648, Val Cor=0.0768, Time=0.1077 sec\n",
      "Epoch [805 / 3000]: Train Loss=4.4054, Val Cor=0.4023, Time=0.1076 sec\n",
      "Epoch [806 / 3000]: Train Loss=4.3050, Val Cor=0.4205, Time=0.1080 sec\n",
      "Epoch [807 / 3000]: Train Loss=4.2888, Val Cor=0.4838, Time=0.1077 sec\n",
      "Epoch [808 / 3000]: Train Loss=4.3037, Val Cor=0.4171, Time=0.1078 sec\n",
      "Epoch [809 / 3000]: Train Loss=4.5062, Val Cor=0.2169, Time=0.1078 sec\n",
      "Epoch [810 / 3000]: Train Loss=4.5196, Val Cor=0.4559, Time=0.1078 sec\n",
      "Epoch [811 / 3000]: Train Loss=4.3002, Val Cor=0.2841, Time=0.1077 sec\n",
      "Epoch [812 / 3000]: Train Loss=4.2330, Val Cor=0.1739, Time=0.1079 sec\n",
      "Epoch [813 / 3000]: Train Loss=4.1908, Val Cor=0.3922, Time=0.1078 sec\n",
      "Epoch [814 / 3000]: Train Loss=4.3124, Val Cor=0.3022, Time=0.1077 sec\n",
      "Epoch [815 / 3000]: Train Loss=4.1606, Val Cor=0.4213, Time=0.1076 sec\n",
      "Epoch [816 / 3000]: Train Loss=4.1791, Val Cor=0.2148, Time=0.1078 sec\n",
      "Epoch [817 / 3000]: Train Loss=4.2160, Val Cor=0.4175, Time=0.1077 sec\n",
      "Epoch [818 / 3000]: Train Loss=4.2040, Val Cor=0.1443, Time=0.1078 sec\n",
      "Epoch [819 / 3000]: Train Loss=4.0976, Val Cor=0.2575, Time=0.1074 sec\n",
      "Epoch [820 / 3000]: Train Loss=3.9950, Val Cor=0.4805, Time=0.1078 sec\n",
      "Epoch [821 / 3000]: Train Loss=4.3255, Val Cor=0.4013, Time=0.1075 sec\n",
      "Epoch [822 / 3000]: Train Loss=4.3895, Val Cor=0.3072, Time=0.1079 sec\n",
      "Epoch [823 / 3000]: Train Loss=4.1880, Val Cor=0.3760, Time=0.1076 sec\n",
      "Epoch [824 / 3000]: Train Loss=4.2276, Val Cor=0.4870, Time=0.1078 sec\n",
      "Epoch [825 / 3000]: Train Loss=4.2140, Val Cor=0.4804, Time=0.1077 sec\n",
      "Epoch [826 / 3000]: Train Loss=5.1438, Val Cor=0.4172, Time=0.1078 sec\n",
      "Epoch [827 / 3000]: Train Loss=4.7159, Val Cor=0.4534, Time=0.1077 sec\n",
      "Epoch [828 / 3000]: Train Loss=4.4101, Val Cor=0.3462, Time=0.1078 sec\n",
      "Epoch [829 / 3000]: Train Loss=4.3971, Val Cor=0.4291, Time=0.1078 sec\n",
      "Epoch [830 / 3000]: Train Loss=4.4161, Val Cor=0.4452, Time=0.1081 sec\n",
      "Epoch [831 / 3000]: Train Loss=4.2481, Val Cor=-0.2223, Time=0.1079 sec\n",
      "Epoch [832 / 3000]: Train Loss=4.4206, Val Cor=0.2483, Time=0.1079 sec\n",
      "Epoch [833 / 3000]: Train Loss=4.5157, Val Cor=0.4044, Time=0.1077 sec\n",
      "Epoch [834 / 3000]: Train Loss=4.3221, Val Cor=0.0790, Time=0.1078 sec\n",
      "Epoch [835 / 3000]: Train Loss=4.3436, Val Cor=0.3950, Time=0.1077 sec\n",
      "Epoch [836 / 3000]: Train Loss=4.4110, Val Cor=0.4935, Time=0.1078 sec\n",
      "Epoch [837 / 3000]: Train Loss=4.4282, Val Cor=0.4414, Time=0.1076 sec\n",
      "Epoch [838 / 3000]: Train Loss=4.3917, Val Cor=0.2122, Time=0.1079 sec\n",
      "Epoch [839 / 3000]: Train Loss=4.4378, Val Cor=0.3909, Time=0.1076 sec\n",
      "Epoch [840 / 3000]: Train Loss=4.2389, Val Cor=0.4422, Time=0.1078 sec\n",
      "Epoch [841 / 3000]: Train Loss=4.5705, Val Cor=0.4758, Time=0.1077 sec\n",
      "Epoch [842 / 3000]: Train Loss=4.4657, Val Cor=0.3366, Time=0.1077 sec\n",
      "Epoch [843 / 3000]: Train Loss=4.5038, Val Cor=0.4192, Time=0.1076 sec\n",
      "Epoch [844 / 3000]: Train Loss=4.5216, Val Cor=0.4409, Time=0.1077 sec\n",
      "Epoch [845 / 3000]: Train Loss=4.3449, Val Cor=0.2818, Time=0.1076 sec\n",
      "Epoch [846 / 3000]: Train Loss=4.2828, Val Cor=0.4514, Time=0.1077 sec\n",
      "Epoch [847 / 3000]: Train Loss=4.2204, Val Cor=0.3975, Time=0.1076 sec\n",
      "Epoch [848 / 3000]: Train Loss=4.2805, Val Cor=0.3276, Time=0.1076 sec\n",
      "Epoch [849 / 3000]: Train Loss=4.1851, Val Cor=0.3946, Time=0.1077 sec\n",
      "Epoch [850 / 3000]: Train Loss=4.0302, Val Cor=0.4409, Time=0.1078 sec\n",
      "Epoch [851 / 3000]: Train Loss=4.1431, Val Cor=0.4723, Time=0.1077 sec\n",
      "Epoch [852 / 3000]: Train Loss=4.1804, Val Cor=0.3351, Time=0.1077 sec\n",
      "Epoch [853 / 3000]: Train Loss=4.2239, Val Cor=0.4472, Time=0.1076 sec\n",
      "Epoch [854 / 3000]: Train Loss=4.6647, Val Cor=0.3339, Time=0.1077 sec\n",
      "Epoch [855 / 3000]: Train Loss=4.3166, Val Cor=0.4625, Time=0.1077 sec\n",
      "Epoch [856 / 3000]: Train Loss=4.2577, Val Cor=0.4517, Time=0.1078 sec\n",
      "Epoch [857 / 3000]: Train Loss=4.3193, Val Cor=0.4537, Time=0.1075 sec\n",
      "Epoch [858 / 3000]: Train Loss=4.1467, Val Cor=0.4697, Time=0.1078 sec\n",
      "Epoch [859 / 3000]: Train Loss=4.1121, Val Cor=0.4788, Time=0.1077 sec\n",
      "Epoch [860 / 3000]: Train Loss=4.0034, Val Cor=0.4714, Time=0.1078 sec\n",
      "Epoch [861 / 3000]: Train Loss=4.0382, Val Cor=0.4864, Time=0.1076 sec\n",
      "Epoch [862 / 3000]: Train Loss=4.2831, Val Cor=-0.3099, Time=0.1077 sec\n",
      "Epoch [863 / 3000]: Train Loss=4.4190, Val Cor=0.4576, Time=0.1078 sec\n",
      "Epoch [864 / 3000]: Train Loss=4.2720, Val Cor=0.4762, Time=0.1078 sec\n",
      "Epoch [865 / 3000]: Train Loss=4.1772, Val Cor=0.4715, Time=0.1077 sec\n",
      "Epoch [866 / 3000]: Train Loss=4.2150, Val Cor=0.2006, Time=0.1080 sec\n",
      "Epoch [867 / 3000]: Train Loss=4.1330, Val Cor=0.4769, Time=0.1077 sec\n",
      "Epoch [868 / 3000]: Train Loss=4.1366, Val Cor=0.4405, Time=0.1078 sec\n",
      "Epoch [869 / 3000]: Train Loss=3.9916, Val Cor=0.3988, Time=0.1076 sec\n",
      "Epoch [870 / 3000]: Train Loss=4.0846, Val Cor=0.4683, Time=0.1078 sec\n",
      "Epoch [871 / 3000]: Train Loss=4.0457, Val Cor=0.4836, Time=0.1079 sec\n",
      "Epoch [872 / 3000]: Train Loss=4.1364, Val Cor=0.4777, Time=0.1079 sec\n",
      "Epoch [873 / 3000]: Train Loss=4.3412, Val Cor=0.1172, Time=0.1077 sec\n",
      "Epoch [874 / 3000]: Train Loss=4.2205, Val Cor=0.4519, Time=0.1077 sec\n",
      "Epoch [875 / 3000]: Train Loss=4.4607, Val Cor=0.4932, Time=0.1077 sec\n",
      "Epoch [876 / 3000]: Train Loss=4.4901, Val Cor=0.4814, Time=0.1079 sec\n",
      "Epoch [877 / 3000]: Train Loss=4.3987, Val Cor=0.4679, Time=0.1077 sec\n",
      "Epoch [878 / 3000]: Train Loss=4.4505, Val Cor=0.4640, Time=0.1076 sec\n",
      "Epoch [879 / 3000]: Train Loss=4.5485, Val Cor=0.1566, Time=0.1077 sec\n",
      "Epoch [880 / 3000]: Train Loss=4.4947, Val Cor=0.3389, Time=0.1077 sec\n",
      "Epoch [881 / 3000]: Train Loss=4.3706, Val Cor=0.4917, Time=0.1077 sec\n",
      "Epoch [882 / 3000]: Train Loss=4.3212, Val Cor=0.4782, Time=0.1077 sec\n",
      "Epoch [883 / 3000]: Train Loss=4.2276, Val Cor=0.4874, Time=0.1077 sec\n",
      "Epoch [884 / 3000]: Train Loss=4.2293, Val Cor=0.4376, Time=0.1078 sec\n",
      "Epoch [885 / 3000]: Train Loss=4.1900, Val Cor=0.2793, Time=0.1076 sec\n",
      "Epoch [886 / 3000]: Train Loss=4.2354, Val Cor=0.4769, Time=0.1078 sec\n",
      "Epoch [887 / 3000]: Train Loss=4.1382, Val Cor=0.4346, Time=0.1077 sec\n",
      "Epoch [888 / 3000]: Train Loss=4.0947, Val Cor=0.4660, Time=0.1077 sec\n",
      "Epoch [889 / 3000]: Train Loss=4.0406, Val Cor=0.4829, Time=0.1076 sec\n",
      "Epoch [890 / 3000]: Train Loss=3.9799, Val Cor=0.4389, Time=0.1077 sec\n",
      "Epoch [891 / 3000]: Train Loss=4.1666, Val Cor=0.4852, Time=0.1075 sec\n",
      "Epoch [892 / 3000]: Train Loss=4.0122, Val Cor=0.4900, Time=0.1077 sec\n",
      "Epoch [893 / 3000]: Train Loss=4.0776, Val Cor=0.4841, Time=0.1076 sec\n",
      "Epoch [894 / 3000]: Train Loss=4.2596, Val Cor=0.4741, Time=0.1079 sec\n",
      "Epoch [895 / 3000]: Train Loss=4.2054, Val Cor=0.4902, Time=0.1076 sec\n",
      "Epoch [896 / 3000]: Train Loss=4.1624, Val Cor=0.4754, Time=0.1076 sec\n",
      "Epoch [897 / 3000]: Train Loss=4.4426, Val Cor=0.3503, Time=0.1078 sec\n",
      "Epoch [898 / 3000]: Train Loss=4.6692, Val Cor=0.4825, Time=0.1078 sec\n",
      "Epoch [899 / 3000]: Train Loss=4.2786, Val Cor=0.4456, Time=0.1076 sec\n",
      "Epoch [900 / 3000]: Train Loss=4.2158, Val Cor=0.4557, Time=0.1078 sec\n",
      "Epoch [901 / 3000]: Train Loss=4.1941, Val Cor=0.4480, Time=0.1077 sec\n",
      "Epoch [902 / 3000]: Train Loss=4.1599, Val Cor=0.4421, Time=0.1077 sec\n",
      "Epoch [903 / 3000]: Train Loss=4.2243, Val Cor=0.4541, Time=0.1078 sec\n",
      "Epoch [904 / 3000]: Train Loss=4.3287, Val Cor=0.4874, Time=0.1078 sec\n",
      "Epoch [905 / 3000]: Train Loss=4.0823, Val Cor=0.4739, Time=0.1080 sec\n",
      "Epoch [906 / 3000]: Train Loss=4.2216, Val Cor=0.3669, Time=0.1078 sec\n",
      "Epoch [907 / 3000]: Train Loss=4.0710, Val Cor=0.4358, Time=0.1077 sec\n",
      "Epoch [908 / 3000]: Train Loss=4.1175, Val Cor=0.4646, Time=0.1076 sec\n",
      "Epoch [909 / 3000]: Train Loss=4.1926, Val Cor=0.4757, Time=0.1077 sec\n",
      "Epoch [910 / 3000]: Train Loss=3.9898, Val Cor=0.4626, Time=0.1078 sec\n",
      "Epoch [911 / 3000]: Train Loss=3.9890, Val Cor=0.4845, Time=0.1077 sec\n",
      "Epoch [912 / 3000]: Train Loss=4.3745, Val Cor=0.4988, Time=0.1077 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [913 / 3000]: Train Loss=4.4979, Val Cor=0.4633, Time=0.1076 sec\n",
      "Epoch [914 / 3000]: Train Loss=4.3103, Val Cor=0.0395, Time=0.1079 sec\n",
      "Epoch [915 / 3000]: Train Loss=4.3022, Val Cor=0.4724, Time=0.1076 sec\n",
      "Epoch [916 / 3000]: Train Loss=4.1777, Val Cor=0.3304, Time=0.1079 sec\n",
      "Epoch [917 / 3000]: Train Loss=4.0876, Val Cor=0.3153, Time=0.1076 sec\n",
      "Epoch [918 / 3000]: Train Loss=4.1100, Val Cor=0.4794, Time=0.1078 sec\n",
      "Epoch [919 / 3000]: Train Loss=4.0780, Val Cor=0.3977, Time=0.1077 sec\n",
      "Epoch [920 / 3000]: Train Loss=4.1356, Val Cor=0.4310, Time=0.1078 sec\n",
      "Epoch [921 / 3000]: Train Loss=4.1566, Val Cor=0.4730, Time=0.1077 sec\n",
      "Epoch [922 / 3000]: Train Loss=4.1391, Val Cor=0.2899, Time=0.1080 sec\n",
      "Epoch [923 / 3000]: Train Loss=4.1539, Val Cor=0.4751, Time=0.1083 sec\n",
      "Epoch [924 / 3000]: Train Loss=4.2656, Val Cor=0.3972, Time=0.1076 sec\n",
      "Epoch [925 / 3000]: Train Loss=4.4553, Val Cor=0.3827, Time=0.1078 sec\n",
      "Epoch [926 / 3000]: Train Loss=4.3202, Val Cor=0.4847, Time=0.1076 sec\n",
      "Epoch [927 / 3000]: Train Loss=4.1304, Val Cor=0.1825, Time=0.1076 sec\n",
      "Epoch [928 / 3000]: Train Loss=4.0856, Val Cor=0.3943, Time=0.1077 sec\n",
      "Epoch [929 / 3000]: Train Loss=4.0417, Val Cor=0.4602, Time=0.1075 sec\n",
      "Epoch [930 / 3000]: Train Loss=4.8027, Val Cor=0.4835, Time=0.1075 sec\n",
      "Epoch [931 / 3000]: Train Loss=5.3970, Val Cor=0.4927, Time=0.1076 sec\n",
      "Epoch [932 / 3000]: Train Loss=4.7312, Val Cor=0.4924, Time=0.1076 sec\n",
      "Epoch [933 / 3000]: Train Loss=4.3857, Val Cor=0.4749, Time=0.1075 sec\n",
      "Epoch [934 / 3000]: Train Loss=4.3007, Val Cor=0.4846, Time=0.1075 sec\n",
      "Epoch [935 / 3000]: Train Loss=4.3892, Val Cor=0.5041, Time=0.1077 sec\n",
      "Epoch [936 / 3000]: Train Loss=4.2666, Val Cor=0.4843, Time=0.1074 sec\n",
      "Epoch [937 / 3000]: Train Loss=4.8645, Val Cor=0.4760, Time=0.1076 sec\n",
      "Epoch [938 / 3000]: Train Loss=4.3112, Val Cor=0.4832, Time=0.1076 sec\n",
      "Epoch [939 / 3000]: Train Loss=4.2819, Val Cor=0.4577, Time=0.1076 sec\n",
      "Epoch [940 / 3000]: Train Loss=4.1264, Val Cor=0.4803, Time=0.1076 sec\n",
      "Epoch [941 / 3000]: Train Loss=4.1128, Val Cor=0.4701, Time=0.1076 sec\n",
      "Epoch [942 / 3000]: Train Loss=4.2347, Val Cor=0.4876, Time=0.1074 sec\n",
      "Epoch [943 / 3000]: Train Loss=4.2177, Val Cor=0.4698, Time=0.1076 sec\n",
      "Epoch [944 / 3000]: Train Loss=4.1187, Val Cor=0.4712, Time=0.1074 sec\n",
      "Epoch [945 / 3000]: Train Loss=4.2400, Val Cor=0.4697, Time=0.1075 sec\n",
      "Epoch [946 / 3000]: Train Loss=4.0316, Val Cor=0.4702, Time=0.1074 sec\n",
      "Epoch [947 / 3000]: Train Loss=4.1133, Val Cor=0.4899, Time=0.1076 sec\n",
      "Epoch [948 / 3000]: Train Loss=4.1890, Val Cor=0.4588, Time=0.1074 sec\n",
      "Epoch [949 / 3000]: Train Loss=3.9343, Val Cor=0.4756, Time=0.1077 sec\n",
      "Epoch [950 / 3000]: Train Loss=4.1293, Val Cor=0.4676, Time=0.1076 sec\n",
      "Epoch [951 / 3000]: Train Loss=4.0012, Val Cor=0.3883, Time=0.1076 sec\n",
      "Epoch [952 / 3000]: Train Loss=4.0372, Val Cor=0.4772, Time=0.1076 sec\n",
      "Epoch [953 / 3000]: Train Loss=4.1124, Val Cor=0.4848, Time=0.1074 sec\n",
      "Epoch [954 / 3000]: Train Loss=4.0877, Val Cor=0.4765, Time=0.1078 sec\n",
      "Epoch [955 / 3000]: Train Loss=4.2072, Val Cor=0.4722, Time=0.1075 sec\n",
      "Epoch [956 / 3000]: Train Loss=4.1505, Val Cor=0.4824, Time=0.1078 sec\n",
      "Epoch [957 / 3000]: Train Loss=4.0541, Val Cor=0.4590, Time=0.1076 sec\n",
      "Epoch [958 / 3000]: Train Loss=4.0433, Val Cor=0.4509, Time=0.1077 sec\n",
      "Epoch [959 / 3000]: Train Loss=4.2055, Val Cor=-0.1220, Time=0.1077 sec\n",
      "Epoch [960 / 3000]: Train Loss=4.4516, Val Cor=-0.0487, Time=0.1076 sec\n",
      "Epoch [961 / 3000]: Train Loss=4.3670, Val Cor=-0.0655, Time=0.1074 sec\n",
      "Epoch [962 / 3000]: Train Loss=4.2621, Val Cor=0.3757, Time=0.1077 sec\n",
      "Epoch [963 / 3000]: Train Loss=4.3049, Val Cor=0.4732, Time=0.1077 sec\n",
      "Epoch [964 / 3000]: Train Loss=4.1342, Val Cor=0.2837, Time=0.1076 sec\n",
      "Epoch [965 / 3000]: Train Loss=4.0979, Val Cor=0.4601, Time=0.1076 sec\n",
      "Epoch [966 / 3000]: Train Loss=4.0668, Val Cor=0.4057, Time=0.1077 sec\n",
      "Epoch [967 / 3000]: Train Loss=3.9739, Val Cor=0.4938, Time=0.1077 sec\n",
      "Epoch [968 / 3000]: Train Loss=4.0526, Val Cor=0.4769, Time=0.1077 sec\n",
      "Epoch [969 / 3000]: Train Loss=4.0525, Val Cor=0.4828, Time=0.1078 sec\n",
      "Epoch [970 / 3000]: Train Loss=4.1536, Val Cor=0.2049, Time=0.1077 sec\n",
      "Epoch [971 / 3000]: Train Loss=4.2196, Val Cor=0.3374, Time=0.1076 sec\n",
      "Epoch [972 / 3000]: Train Loss=4.2398, Val Cor=0.4689, Time=0.1078 sec\n",
      "Epoch [973 / 3000]: Train Loss=4.1611, Val Cor=0.4346, Time=0.1076 sec\n",
      "Epoch [974 / 3000]: Train Loss=4.0785, Val Cor=0.4947, Time=0.1077 sec\n",
      "Epoch [975 / 3000]: Train Loss=4.1785, Val Cor=0.4223, Time=0.1077 sec\n",
      "Epoch [976 / 3000]: Train Loss=3.9098, Val Cor=0.4929, Time=0.1076 sec\n",
      "Epoch [977 / 3000]: Train Loss=3.9731, Val Cor=0.4683, Time=0.1076 sec\n",
      "Epoch [978 / 3000]: Train Loss=4.2110, Val Cor=-0.1366, Time=0.1076 sec\n",
      "Epoch [979 / 3000]: Train Loss=4.4634, Val Cor=0.2933, Time=0.1076 sec\n",
      "Epoch [980 / 3000]: Train Loss=4.4907, Val Cor=0.5074, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [981 / 3000]: Train Loss=4.2171, Val Cor=0.4665, Time=0.1077 sec\n",
      "Epoch [982 / 3000]: Train Loss=4.2605, Val Cor=0.4976, Time=0.1078 sec\n",
      "Epoch [983 / 3000]: Train Loss=4.5050, Val Cor=0.4259, Time=0.1077 sec\n",
      "Epoch [984 / 3000]: Train Loss=4.3510, Val Cor=0.4141, Time=0.1078 sec\n",
      "Epoch [985 / 3000]: Train Loss=4.1798, Val Cor=0.4653, Time=0.1077 sec\n",
      "Epoch [986 / 3000]: Train Loss=4.0216, Val Cor=0.4872, Time=0.1078 sec\n",
      "Epoch [987 / 3000]: Train Loss=4.1524, Val Cor=0.4886, Time=0.1078 sec\n",
      "Epoch [988 / 3000]: Train Loss=4.0869, Val Cor=0.3148, Time=0.1079 sec\n",
      "Epoch [989 / 3000]: Train Loss=4.1919, Val Cor=0.4812, Time=0.1076 sec\n",
      "Epoch [990 / 3000]: Train Loss=4.2706, Val Cor=0.4781, Time=0.1078 sec\n",
      "Epoch [991 / 3000]: Train Loss=4.1269, Val Cor=0.4123, Time=0.1078 sec\n",
      "Epoch [992 / 3000]: Train Loss=4.3842, Val Cor=0.4774, Time=0.1080 sec\n",
      "Epoch [993 / 3000]: Train Loss=4.8268, Val Cor=0.5030, Time=0.1076 sec\n",
      "Epoch [994 / 3000]: Train Loss=4.5069, Val Cor=0.2637, Time=0.1078 sec\n",
      "Epoch [995 / 3000]: Train Loss=4.3452, Val Cor=0.4179, Time=0.1076 sec\n",
      "Epoch [996 / 3000]: Train Loss=4.1659, Val Cor=0.3479, Time=0.1076 sec\n",
      "Epoch [997 / 3000]: Train Loss=4.0717, Val Cor=0.4588, Time=0.1077 sec\n",
      "Epoch [998 / 3000]: Train Loss=3.9226, Val Cor=0.4706, Time=0.1078 sec\n",
      "Epoch [999 / 3000]: Train Loss=4.1149, Val Cor=0.0373, Time=0.1076 sec\n",
      "Epoch [1000 / 3000]: Train Loss=4.1040, Val Cor=0.3815, Time=0.1078 sec\n",
      "Epoch [1001 / 3000]: Train Loss=4.0622, Val Cor=0.2369, Time=0.1076 sec\n",
      "Epoch [1002 / 3000]: Train Loss=4.0416, Val Cor=0.4429, Time=0.1076 sec\n",
      "Epoch [1003 / 3000]: Train Loss=3.9416, Val Cor=0.4832, Time=0.1076 sec\n",
      "Epoch [1004 / 3000]: Train Loss=4.0846, Val Cor=0.4403, Time=0.1079 sec\n",
      "Epoch [1005 / 3000]: Train Loss=4.0658, Val Cor=0.3988, Time=0.1078 sec\n",
      "Epoch [1006 / 3000]: Train Loss=4.0550, Val Cor=0.2569, Time=0.1079 sec\n",
      "Epoch [1007 / 3000]: Train Loss=5.0386, Val Cor=0.1242, Time=0.1076 sec\n",
      "Epoch [1008 / 3000]: Train Loss=5.7150, Val Cor=0.2817, Time=0.1079 sec\n",
      "Epoch [1009 / 3000]: Train Loss=5.6633, Val Cor=0.4014, Time=0.1078 sec\n",
      "Epoch [1010 / 3000]: Train Loss=5.6210, Val Cor=0.4190, Time=0.1078 sec\n",
      "Epoch [1011 / 3000]: Train Loss=5.3700, Val Cor=0.4839, Time=0.1081 sec\n",
      "Epoch [1012 / 3000]: Train Loss=5.1459, Val Cor=0.4981, Time=0.1080 sec\n",
      "Epoch [1013 / 3000]: Train Loss=4.9511, Val Cor=0.5014, Time=0.1078 sec\n",
      "Epoch [1014 / 3000]: Train Loss=4.4266, Val Cor=0.4948, Time=0.1079 sec\n",
      "Epoch [1015 / 3000]: Train Loss=4.6629, Val Cor=0.4979, Time=0.1081 sec\n",
      "Epoch [1016 / 3000]: Train Loss=4.1188, Val Cor=0.5062, Time=0.1084 sec\n",
      "Epoch [1017 / 3000]: Train Loss=4.2797, Val Cor=0.4506, Time=0.1080 sec\n",
      "Epoch [1018 / 3000]: Train Loss=4.1667, Val Cor=0.4744, Time=0.1079 sec\n",
      "Epoch [1019 / 3000]: Train Loss=4.2009, Val Cor=0.4866, Time=0.1080 sec\n",
      "Epoch [1020 / 3000]: Train Loss=4.3865, Val Cor=0.4880, Time=0.1082 sec\n",
      "Epoch [1021 / 3000]: Train Loss=4.2314, Val Cor=0.4192, Time=0.1079 sec\n",
      "Epoch [1022 / 3000]: Train Loss=4.3185, Val Cor=0.4807, Time=0.1082 sec\n",
      "Epoch [1023 / 3000]: Train Loss=4.1568, Val Cor=0.4363, Time=0.1080 sec\n",
      "Epoch [1024 / 3000]: Train Loss=3.9648, Val Cor=0.4476, Time=0.1079 sec\n",
      "Epoch [1025 / 3000]: Train Loss=4.2392, Val Cor=0.2967, Time=0.1080 sec\n",
      "Epoch [1026 / 3000]: Train Loss=4.6676, Val Cor=0.5020, Time=0.1080 sec\n",
      "Epoch [1027 / 3000]: Train Loss=4.3320, Val Cor=0.4954, Time=0.1078 sec\n",
      "Epoch [1028 / 3000]: Train Loss=4.0899, Val Cor=0.4865, Time=0.1081 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1029 / 3000]: Train Loss=4.2761, Val Cor=0.4940, Time=0.1083 sec\n",
      "Epoch [1030 / 3000]: Train Loss=4.0134, Val Cor=0.4695, Time=0.1082 sec\n",
      "Epoch [1031 / 3000]: Train Loss=4.0880, Val Cor=0.4836, Time=0.1080 sec\n",
      "Epoch [1032 / 3000]: Train Loss=4.0589, Val Cor=0.4901, Time=0.1079 sec\n",
      "Epoch [1033 / 3000]: Train Loss=4.1889, Val Cor=0.4926, Time=0.1080 sec\n",
      "Epoch [1034 / 3000]: Train Loss=4.4961, Val Cor=0.4932, Time=0.1082 sec\n",
      "Epoch [1035 / 3000]: Train Loss=4.2736, Val Cor=0.4974, Time=0.1080 sec\n",
      "Epoch [1036 / 3000]: Train Loss=4.0910, Val Cor=0.4342, Time=0.1082 sec\n",
      "Epoch [1037 / 3000]: Train Loss=4.1168, Val Cor=0.4568, Time=0.1079 sec\n",
      "Epoch [1038 / 3000]: Train Loss=4.0728, Val Cor=0.4664, Time=0.1081 sec\n",
      "Epoch [1039 / 3000]: Train Loss=4.1002, Val Cor=0.3971, Time=0.1078 sec\n",
      "Epoch [1040 / 3000]: Train Loss=3.9963, Val Cor=0.4434, Time=0.1082 sec\n",
      "Epoch [1041 / 3000]: Train Loss=3.9052, Val Cor=0.4708, Time=0.1079 sec\n",
      "Epoch [1042 / 3000]: Train Loss=3.9455, Val Cor=0.4520, Time=0.1079 sec\n",
      "Epoch [1043 / 3000]: Train Loss=3.9400, Val Cor=0.4612, Time=0.1081 sec\n",
      "Epoch [1044 / 3000]: Train Loss=4.0806, Val Cor=0.4833, Time=0.1080 sec\n",
      "Epoch [1045 / 3000]: Train Loss=3.9063, Val Cor=0.4922, Time=0.1079 sec\n",
      "Epoch [1046 / 3000]: Train Loss=3.9548, Val Cor=0.3458, Time=0.1081 sec\n",
      "Epoch [1047 / 3000]: Train Loss=3.9434, Val Cor=0.4754, Time=0.1081 sec\n",
      "Epoch [1048 / 3000]: Train Loss=3.8925, Val Cor=0.4753, Time=0.1080 sec\n",
      "Epoch [1049 / 3000]: Train Loss=3.9825, Val Cor=0.3631, Time=0.1079 sec\n",
      "Epoch [1050 / 3000]: Train Loss=5.5156, Val Cor=0.0562, Time=0.1081 sec\n",
      "Epoch [1051 / 3000]: Train Loss=5.5707, Val Cor=0.4074, Time=0.1078 sec\n",
      "Epoch [1052 / 3000]: Train Loss=4.9200, Val Cor=0.4602, Time=0.1082 sec\n",
      "Epoch [1053 / 3000]: Train Loss=4.1262, Val Cor=0.5028, Time=0.1082 sec\n",
      "Epoch [1054 / 3000]: Train Loss=4.2871, Val Cor=0.4764, Time=0.1084 sec\n",
      "Epoch [1055 / 3000]: Train Loss=4.0011, Val Cor=0.4599, Time=0.1078 sec\n",
      "Epoch [1056 / 3000]: Train Loss=3.8812, Val Cor=0.4874, Time=0.1081 sec\n",
      "Epoch [1057 / 3000]: Train Loss=4.1098, Val Cor=0.1697, Time=0.1080 sec\n",
      "Epoch [1058 / 3000]: Train Loss=4.3876, Val Cor=0.4916, Time=0.1080 sec\n",
      "Epoch [1059 / 3000]: Train Loss=3.9976, Val Cor=0.4940, Time=0.1080 sec\n",
      "Epoch [1060 / 3000]: Train Loss=4.1119, Val Cor=0.4587, Time=0.1081 sec\n",
      "Epoch [1061 / 3000]: Train Loss=4.0560, Val Cor=0.4672, Time=0.1082 sec\n",
      "Epoch [1062 / 3000]: Train Loss=4.0198, Val Cor=0.4957, Time=0.1083 sec\n",
      "Epoch [1063 / 3000]: Train Loss=4.0058, Val Cor=0.5096, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [1064 / 3000]: Train Loss=4.2193, Val Cor=0.4749, Time=0.1082 sec\n",
      "Epoch [1065 / 3000]: Train Loss=3.9646, Val Cor=0.4732, Time=0.1080 sec\n",
      "Epoch [1066 / 3000]: Train Loss=4.0512, Val Cor=0.4389, Time=0.1081 sec\n",
      "Epoch [1067 / 3000]: Train Loss=4.0415, Val Cor=0.1837, Time=0.1079 sec\n",
      "Epoch [1068 / 3000]: Train Loss=4.1792, Val Cor=0.3587, Time=0.1082 sec\n",
      "Epoch [1069 / 3000]: Train Loss=3.9220, Val Cor=0.4550, Time=0.1079 sec\n",
      "Epoch [1070 / 3000]: Train Loss=3.8377, Val Cor=0.4610, Time=0.1082 sec\n",
      "Epoch [1071 / 3000]: Train Loss=3.8853, Val Cor=0.4790, Time=0.1079 sec\n",
      "Epoch [1072 / 3000]: Train Loss=4.2385, Val Cor=0.4816, Time=0.1081 sec\n",
      "Epoch [1073 / 3000]: Train Loss=4.3304, Val Cor=0.4911, Time=0.1081 sec\n",
      "Epoch [1074 / 3000]: Train Loss=4.4378, Val Cor=-0.0981, Time=0.1082 sec\n",
      "Epoch [1075 / 3000]: Train Loss=4.3913, Val Cor=0.4613, Time=0.1080 sec\n",
      "Epoch [1076 / 3000]: Train Loss=4.2525, Val Cor=0.4311, Time=0.1082 sec\n",
      "Epoch [1077 / 3000]: Train Loss=4.0822, Val Cor=0.4361, Time=0.1082 sec\n",
      "Epoch [1078 / 3000]: Train Loss=4.0197, Val Cor=0.4919, Time=0.1081 sec\n",
      "Epoch [1079 / 3000]: Train Loss=3.9370, Val Cor=0.4172, Time=0.1080 sec\n",
      "Epoch [1080 / 3000]: Train Loss=4.1378, Val Cor=0.4292, Time=0.1081 sec\n",
      "Epoch [1081 / 3000]: Train Loss=4.1646, Val Cor=0.4177, Time=0.1080 sec\n",
      "Epoch [1082 / 3000]: Train Loss=4.0754, Val Cor=0.4106, Time=0.1080 sec\n",
      "Epoch [1083 / 3000]: Train Loss=4.2161, Val Cor=0.4714, Time=0.1080 sec\n",
      "Epoch [1084 / 3000]: Train Loss=4.2410, Val Cor=0.4289, Time=0.1083 sec\n",
      "Epoch [1085 / 3000]: Train Loss=4.0268, Val Cor=0.4834, Time=0.1082 sec\n",
      "Epoch [1086 / 3000]: Train Loss=4.0116, Val Cor=0.4951, Time=0.1081 sec\n",
      "Epoch [1087 / 3000]: Train Loss=4.1278, Val Cor=0.3051, Time=0.1081 sec\n",
      "Epoch [1088 / 3000]: Train Loss=4.0800, Val Cor=0.4839, Time=0.1080 sec\n",
      "Epoch [1089 / 3000]: Train Loss=3.8382, Val Cor=0.4551, Time=0.1080 sec\n",
      "Epoch [1090 / 3000]: Train Loss=3.9454, Val Cor=0.4414, Time=0.1081 sec\n",
      "Epoch [1091 / 3000]: Train Loss=3.8695, Val Cor=0.4503, Time=0.1080 sec\n",
      "Epoch [1092 / 3000]: Train Loss=4.0384, Val Cor=0.4169, Time=0.1081 sec\n",
      "Epoch [1093 / 3000]: Train Loss=4.2113, Val Cor=0.4066, Time=0.1078 sec\n",
      "Epoch [1094 / 3000]: Train Loss=3.9490, Val Cor=0.4519, Time=0.1080 sec\n",
      "Epoch [1095 / 3000]: Train Loss=4.4103, Val Cor=0.4914, Time=0.1079 sec\n",
      "Epoch [1096 / 3000]: Train Loss=3.9583, Val Cor=0.3549, Time=0.1082 sec\n",
      "Epoch [1097 / 3000]: Train Loss=4.2152, Val Cor=0.4812, Time=0.1080 sec\n",
      "Epoch [1098 / 3000]: Train Loss=4.1773, Val Cor=0.4886, Time=0.1079 sec\n",
      "Epoch [1099 / 3000]: Train Loss=4.2976, Val Cor=0.4683, Time=0.1079 sec\n",
      "Epoch [1100 / 3000]: Train Loss=4.1730, Val Cor=0.4903, Time=0.1082 sec\n",
      "Epoch [1101 / 3000]: Train Loss=4.0942, Val Cor=0.4798, Time=0.1081 sec\n",
      "Epoch [1102 / 3000]: Train Loss=3.8910, Val Cor=0.3127, Time=0.1083 sec\n",
      "Epoch [1103 / 3000]: Train Loss=4.0274, Val Cor=0.4681, Time=0.1080 sec\n",
      "Epoch [1104 / 3000]: Train Loss=3.8631, Val Cor=0.4748, Time=0.1082 sec\n",
      "Epoch [1105 / 3000]: Train Loss=4.4832, Val Cor=0.4778, Time=0.1078 sec\n",
      "Epoch [1106 / 3000]: Train Loss=4.5083, Val Cor=0.4512, Time=0.1082 sec\n",
      "Epoch [1107 / 3000]: Train Loss=3.9664, Val Cor=0.4763, Time=0.1079 sec\n",
      "Epoch [1108 / 3000]: Train Loss=4.2932, Val Cor=0.4346, Time=0.1086 sec\n",
      "Epoch [1109 / 3000]: Train Loss=4.5485, Val Cor=0.5036, Time=0.1082 sec\n",
      "Epoch [1110 / 3000]: Train Loss=4.1952, Val Cor=-0.2064, Time=0.1085 sec\n",
      "Epoch [1111 / 3000]: Train Loss=4.0740, Val Cor=0.4611, Time=0.1082 sec\n",
      "Epoch [1112 / 3000]: Train Loss=4.0012, Val Cor=0.0954, Time=0.1080 sec\n",
      "Epoch [1113 / 3000]: Train Loss=4.0256, Val Cor=0.4676, Time=0.1081 sec\n",
      "Epoch [1114 / 3000]: Train Loss=3.9617, Val Cor=0.4916, Time=0.1081 sec\n",
      "Epoch [1115 / 3000]: Train Loss=4.0616, Val Cor=0.4565, Time=0.1083 sec\n",
      "Epoch [1116 / 3000]: Train Loss=3.7545, Val Cor=0.4789, Time=0.1081 sec\n",
      "Epoch [1117 / 3000]: Train Loss=4.0430, Val Cor=0.4999, Time=0.1080 sec\n",
      "Epoch [1118 / 3000]: Train Loss=3.9977, Val Cor=0.3647, Time=0.1081 sec\n",
      "Epoch [1119 / 3000]: Train Loss=4.2431, Val Cor=0.4967, Time=0.1079 sec\n",
      "Epoch [1120 / 3000]: Train Loss=4.2687, Val Cor=0.4957, Time=0.1081 sec\n",
      "Epoch [1121 / 3000]: Train Loss=4.2799, Val Cor=0.4869, Time=0.1078 sec\n",
      "Epoch [1122 / 3000]: Train Loss=4.0547, Val Cor=0.4957, Time=0.1081 sec\n",
      "Epoch [1123 / 3000]: Train Loss=4.0471, Val Cor=0.4735, Time=0.1082 sec\n",
      "Epoch [1124 / 3000]: Train Loss=3.9747, Val Cor=0.4874, Time=0.1079 sec\n",
      "Epoch [1125 / 3000]: Train Loss=3.8652, Val Cor=0.4873, Time=0.1081 sec\n",
      "Epoch [1126 / 3000]: Train Loss=4.0421, Val Cor=0.4817, Time=0.1080 sec\n",
      "Epoch [1127 / 3000]: Train Loss=3.9039, Val Cor=0.2784, Time=0.1081 sec\n",
      "Epoch [1128 / 3000]: Train Loss=3.8961, Val Cor=0.3691, Time=0.1081 sec\n",
      "Epoch [1129 / 3000]: Train Loss=4.0984, Val Cor=-0.0257, Time=0.1082 sec\n",
      "Epoch [1130 / 3000]: Train Loss=4.0620, Val Cor=0.4587, Time=0.1081 sec\n",
      "Epoch [1131 / 3000]: Train Loss=3.9144, Val Cor=0.4623, Time=0.1080 sec\n",
      "Epoch [1132 / 3000]: Train Loss=4.0055, Val Cor=0.3283, Time=0.1082 sec\n",
      "Epoch [1133 / 3000]: Train Loss=3.9761, Val Cor=0.4578, Time=0.1081 sec\n",
      "Epoch [1134 / 3000]: Train Loss=3.9905, Val Cor=-0.0378, Time=0.1081 sec\n",
      "Epoch [1135 / 3000]: Train Loss=3.9911, Val Cor=0.4876, Time=0.1077 sec\n",
      "Epoch [1136 / 3000]: Train Loss=3.9296, Val Cor=0.4859, Time=0.1081 sec\n",
      "Epoch [1137 / 3000]: Train Loss=4.2030, Val Cor=0.4720, Time=0.1080 sec\n",
      "Epoch [1138 / 3000]: Train Loss=4.2265, Val Cor=-0.0160, Time=0.1080 sec\n",
      "Epoch [1139 / 3000]: Train Loss=4.0522, Val Cor=0.2587, Time=0.1081 sec\n",
      "Epoch [1140 / 3000]: Train Loss=3.9867, Val Cor=0.2103, Time=0.1080 sec\n",
      "Epoch [1141 / 3000]: Train Loss=4.0174, Val Cor=0.4955, Time=0.1080 sec\n",
      "Epoch [1142 / 3000]: Train Loss=3.9483, Val Cor=0.4517, Time=0.1080 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1143 / 3000]: Train Loss=3.9553, Val Cor=0.4767, Time=0.1080 sec\n",
      "Epoch [1144 / 3000]: Train Loss=3.8356, Val Cor=0.4806, Time=0.1079 sec\n",
      "Epoch [1145 / 3000]: Train Loss=4.0640, Val Cor=0.3365, Time=0.1079 sec\n",
      "Epoch [1146 / 3000]: Train Loss=3.9656, Val Cor=0.4628, Time=0.1083 sec\n",
      "Epoch [1147 / 3000]: Train Loss=3.9491, Val Cor=0.4656, Time=0.1080 sec\n",
      "Epoch [1148 / 3000]: Train Loss=3.8121, Val Cor=0.4329, Time=0.1082 sec\n",
      "Epoch [1149 / 3000]: Train Loss=3.8165, Val Cor=0.4708, Time=0.1080 sec\n",
      "Epoch [1150 / 3000]: Train Loss=3.7829, Val Cor=0.3588, Time=0.1081 sec\n",
      "Epoch [1151 / 3000]: Train Loss=3.8108, Val Cor=0.4899, Time=0.1079 sec\n",
      "Epoch [1152 / 3000]: Train Loss=4.1001, Val Cor=0.4692, Time=0.1081 sec\n",
      "Epoch [1153 / 3000]: Train Loss=4.1134, Val Cor=0.4703, Time=0.1080 sec\n",
      "Epoch [1154 / 3000]: Train Loss=4.1482, Val Cor=0.4634, Time=0.1081 sec\n",
      "Epoch [1155 / 3000]: Train Loss=3.9497, Val Cor=0.4954, Time=0.1080 sec\n",
      "Epoch [1156 / 3000]: Train Loss=4.0849, Val Cor=0.4409, Time=0.1082 sec\n",
      "Epoch [1157 / 3000]: Train Loss=3.9453, Val Cor=0.4769, Time=0.1079 sec\n",
      "Epoch [1158 / 3000]: Train Loss=5.1596, Val Cor=0.1277, Time=0.1082 sec\n",
      "Epoch [1159 / 3000]: Train Loss=5.7456, Val Cor=0.1567, Time=0.1078 sec\n",
      "Epoch [1160 / 3000]: Train Loss=5.6020, Val Cor=0.2431, Time=0.1083 sec\n",
      "Epoch [1161 / 3000]: Train Loss=5.5123, Val Cor=0.3386, Time=0.1080 sec\n",
      "Epoch [1162 / 3000]: Train Loss=5.3066, Val Cor=0.4350, Time=0.1079 sec\n",
      "Epoch [1163 / 3000]: Train Loss=4.8839, Val Cor=0.4846, Time=0.1081 sec\n",
      "Epoch [1164 / 3000]: Train Loss=4.1851, Val Cor=0.4986, Time=0.1079 sec\n",
      "Epoch [1165 / 3000]: Train Loss=3.9591, Val Cor=0.4606, Time=0.1080 sec\n",
      "Epoch [1166 / 3000]: Train Loss=4.1963, Val Cor=0.5015, Time=0.1081 sec\n",
      "Epoch [1167 / 3000]: Train Loss=4.4006, Val Cor=0.3990, Time=0.1082 sec\n",
      "Epoch [1168 / 3000]: Train Loss=4.1535, Val Cor=0.4153, Time=0.1080 sec\n",
      "Epoch [1169 / 3000]: Train Loss=4.2596, Val Cor=0.3937, Time=0.1078 sec\n",
      "Epoch [1170 / 3000]: Train Loss=4.0834, Val Cor=0.3888, Time=0.1083 sec\n",
      "Epoch [1171 / 3000]: Train Loss=3.9645, Val Cor=0.4913, Time=0.1081 sec\n",
      "Epoch [1172 / 3000]: Train Loss=4.0258, Val Cor=0.4769, Time=0.1082 sec\n",
      "Epoch [1173 / 3000]: Train Loss=4.0756, Val Cor=0.4605, Time=0.1080 sec\n",
      "Epoch [1174 / 3000]: Train Loss=4.3655, Val Cor=0.3576, Time=0.1082 sec\n",
      "Epoch [1175 / 3000]: Train Loss=4.8782, Val Cor=0.4991, Time=0.1080 sec\n",
      "Epoch [1176 / 3000]: Train Loss=4.2688, Val Cor=0.4656, Time=0.1082 sec\n",
      "Epoch [1177 / 3000]: Train Loss=4.1291, Val Cor=0.4412, Time=0.1079 sec\n",
      "Epoch [1178 / 3000]: Train Loss=4.1036, Val Cor=0.3114, Time=0.1081 sec\n",
      "Epoch [1179 / 3000]: Train Loss=4.0009, Val Cor=0.4603, Time=0.1081 sec\n",
      "Epoch [1180 / 3000]: Train Loss=3.9553, Val Cor=0.4932, Time=0.1081 sec\n",
      "Epoch [1181 / 3000]: Train Loss=3.9984, Val Cor=0.4767, Time=0.1081 sec\n",
      "Epoch [1182 / 3000]: Train Loss=4.4778, Val Cor=0.4890, Time=0.1082 sec\n",
      "Epoch [1183 / 3000]: Train Loss=4.0801, Val Cor=0.4704, Time=0.1080 sec\n",
      "Epoch [1184 / 3000]: Train Loss=4.1094, Val Cor=0.4879, Time=0.1079 sec\n",
      "Epoch [1185 / 3000]: Train Loss=3.9105, Val Cor=0.4840, Time=0.1082 sec\n",
      "Epoch [1186 / 3000]: Train Loss=4.1136, Val Cor=0.4782, Time=0.1082 sec\n",
      "Epoch [1187 / 3000]: Train Loss=3.9504, Val Cor=0.4304, Time=0.1080 sec\n",
      "Epoch [1188 / 3000]: Train Loss=4.1440, Val Cor=0.3801, Time=0.1084 sec\n",
      "Epoch [1189 / 3000]: Train Loss=4.0600, Val Cor=0.5004, Time=0.1079 sec\n",
      "Epoch [1190 / 3000]: Train Loss=4.0803, Val Cor=0.2913, Time=0.1081 sec\n",
      "Epoch [1191 / 3000]: Train Loss=3.8977, Val Cor=0.5005, Time=0.1078 sec\n",
      "Epoch [1192 / 3000]: Train Loss=3.8363, Val Cor=0.4450, Time=0.1080 sec\n",
      "Epoch [1193 / 3000]: Train Loss=3.9435, Val Cor=0.4713, Time=0.1081 sec\n",
      "Epoch [1194 / 3000]: Train Loss=3.8467, Val Cor=0.5000, Time=0.1083 sec\n",
      "Epoch [1195 / 3000]: Train Loss=3.7631, Val Cor=0.5015, Time=0.1079 sec\n",
      "Epoch [1196 / 3000]: Train Loss=4.0521, Val Cor=0.4924, Time=0.1082 sec\n",
      "Epoch [1197 / 3000]: Train Loss=4.0248, Val Cor=0.4776, Time=0.1081 sec\n",
      "Epoch [1198 / 3000]: Train Loss=3.9484, Val Cor=0.4976, Time=0.1081 sec\n",
      "Epoch [1199 / 3000]: Train Loss=3.9279, Val Cor=0.4869, Time=0.1081 sec\n",
      "Epoch [1200 / 3000]: Train Loss=3.9436, Val Cor=0.4924, Time=0.1081 sec\n",
      "Epoch [1201 / 3000]: Train Loss=3.8824, Val Cor=0.3945, Time=0.1080 sec\n",
      "Epoch [1202 / 3000]: Train Loss=3.9435, Val Cor=0.4562, Time=0.1084 sec\n",
      "Epoch [1203 / 3000]: Train Loss=3.7764, Val Cor=0.4430, Time=0.1084 sec\n",
      "Epoch [1204 / 3000]: Train Loss=4.1245, Val Cor=0.2215, Time=0.1082 sec\n",
      "Epoch [1205 / 3000]: Train Loss=3.8292, Val Cor=0.4781, Time=0.1080 sec\n",
      "Epoch [1206 / 3000]: Train Loss=3.8528, Val Cor=0.4280, Time=0.1081 sec\n",
      "Epoch [1207 / 3000]: Train Loss=3.7194, Val Cor=0.4684, Time=0.1081 sec\n",
      "Epoch [1208 / 3000]: Train Loss=4.2572, Val Cor=0.1963, Time=0.1081 sec\n",
      "Epoch [1209 / 3000]: Train Loss=4.1343, Val Cor=0.4046, Time=0.1079 sec\n",
      "Epoch [1210 / 3000]: Train Loss=4.0197, Val Cor=0.2490, Time=0.1082 sec\n",
      "Epoch [1211 / 3000]: Train Loss=3.9063, Val Cor=0.2416, Time=0.1079 sec\n",
      "Epoch [1212 / 3000]: Train Loss=4.0144, Val Cor=0.5001, Time=0.1081 sec\n",
      "Epoch [1213 / 3000]: Train Loss=3.9273, Val Cor=-0.1138, Time=0.1081 sec\n",
      "Epoch [1214 / 3000]: Train Loss=3.8891, Val Cor=0.4613, Time=0.1080 sec\n",
      "Epoch [1215 / 3000]: Train Loss=3.9934, Val Cor=0.4054, Time=0.1080 sec\n",
      "Epoch [1216 / 3000]: Train Loss=3.9164, Val Cor=0.4667, Time=0.1081 sec\n",
      "Epoch [1217 / 3000]: Train Loss=3.9716, Val Cor=0.4234, Time=0.1082 sec\n",
      "Epoch [1218 / 3000]: Train Loss=3.9308, Val Cor=0.4992, Time=0.1080 sec\n",
      "Epoch [1219 / 3000]: Train Loss=3.9831, Val Cor=0.4863, Time=0.1080 sec\n",
      "Epoch [1220 / 3000]: Train Loss=3.9428, Val Cor=0.4969, Time=0.1082 sec\n",
      "Epoch [1221 / 3000]: Train Loss=3.9795, Val Cor=0.5057, Time=0.1081 sec\n",
      "Epoch [1222 / 3000]: Train Loss=3.8525, Val Cor=0.4901, Time=0.1080 sec\n",
      "Epoch [1223 / 3000]: Train Loss=4.0097, Val Cor=0.4460, Time=0.1081 sec\n",
      "Epoch [1224 / 3000]: Train Loss=3.9487, Val Cor=0.4903, Time=0.1082 sec\n",
      "Epoch [1225 / 3000]: Train Loss=3.8247, Val Cor=0.4056, Time=0.1079 sec\n",
      "Epoch [1226 / 3000]: Train Loss=3.9125, Val Cor=0.4742, Time=0.1082 sec\n",
      "Epoch [1227 / 3000]: Train Loss=3.7733, Val Cor=0.4877, Time=0.1079 sec\n",
      "Epoch [1228 / 3000]: Train Loss=3.8352, Val Cor=0.3638, Time=0.1082 sec\n",
      "Epoch [1229 / 3000]: Train Loss=3.8900, Val Cor=0.2968, Time=0.1077 sec\n",
      "Epoch [1230 / 3000]: Train Loss=3.8603, Val Cor=0.4738, Time=0.1081 sec\n",
      "Epoch [1231 / 3000]: Train Loss=3.8619, Val Cor=0.3218, Time=0.1079 sec\n",
      "Epoch [1232 / 3000]: Train Loss=4.1767, Val Cor=0.4875, Time=0.1082 sec\n",
      "Epoch [1233 / 3000]: Train Loss=4.2448, Val Cor=0.4479, Time=0.1078 sec\n",
      "Epoch [1234 / 3000]: Train Loss=4.3364, Val Cor=-0.0619, Time=0.1081 sec\n",
      "Epoch [1235 / 3000]: Train Loss=4.1331, Val Cor=0.3901, Time=0.1080 sec\n",
      "Epoch [1236 / 3000]: Train Loss=4.1366, Val Cor=0.2289, Time=0.1083 sec\n",
      "Epoch [1237 / 3000]: Train Loss=4.0778, Val Cor=0.4172, Time=0.1080 sec\n",
      "Epoch [1238 / 3000]: Train Loss=3.9347, Val Cor=0.0713, Time=0.1082 sec\n",
      "Epoch [1239 / 3000]: Train Loss=3.7936, Val Cor=0.4242, Time=0.1081 sec\n",
      "Epoch [1240 / 3000]: Train Loss=3.8318, Val Cor=0.4803, Time=0.1082 sec\n",
      "Epoch [1241 / 3000]: Train Loss=3.7188, Val Cor=0.4817, Time=0.1082 sec\n",
      "Epoch [1242 / 3000]: Train Loss=3.7314, Val Cor=0.4642, Time=0.1081 sec\n",
      "Epoch [1243 / 3000]: Train Loss=3.9452, Val Cor=0.3934, Time=0.1078 sec\n",
      "Epoch [1244 / 3000]: Train Loss=3.9413, Val Cor=0.3778, Time=0.1082 sec\n",
      "Epoch [1245 / 3000]: Train Loss=3.8634, Val Cor=0.4847, Time=0.1081 sec\n",
      "Epoch [1246 / 3000]: Train Loss=3.7571, Val Cor=0.3189, Time=0.1082 sec\n",
      "Epoch [1247 / 3000]: Train Loss=4.1017, Val Cor=0.4036, Time=0.1080 sec\n",
      "Epoch [1248 / 3000]: Train Loss=3.9594, Val Cor=0.4896, Time=0.1081 sec\n",
      "Epoch [1249 / 3000]: Train Loss=3.8774, Val Cor=0.5073, Time=0.1079 sec\n",
      "Epoch [1250 / 3000]: Train Loss=4.0616, Val Cor=0.4095, Time=0.1081 sec\n",
      "Epoch [1251 / 3000]: Train Loss=3.9080, Val Cor=0.5000, Time=0.1078 sec\n",
      "Epoch [1252 / 3000]: Train Loss=3.8671, Val Cor=0.4125, Time=0.1079 sec\n",
      "Epoch [1253 / 3000]: Train Loss=3.9363, Val Cor=0.4730, Time=0.1078 sec\n",
      "Epoch [1254 / 3000]: Train Loss=3.8114, Val Cor=0.4805, Time=0.1081 sec\n",
      "Epoch [1255 / 3000]: Train Loss=4.0510, Val Cor=0.5097, Time=0.1081 sec\n",
      "model_updated\n",
      "Epoch [1256 / 3000]: Train Loss=3.9087, Val Cor=0.4677, Time=0.1080 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1257 / 3000]: Train Loss=3.8961, Val Cor=0.3679, Time=0.1081 sec\n",
      "Epoch [1258 / 3000]: Train Loss=3.9562, Val Cor=0.4205, Time=0.1080 sec\n",
      "Epoch [1259 / 3000]: Train Loss=3.8185, Val Cor=0.0258, Time=0.1081 sec\n",
      "Epoch [1260 / 3000]: Train Loss=3.9047, Val Cor=0.2577, Time=0.1081 sec\n",
      "Epoch [1261 / 3000]: Train Loss=3.7618, Val Cor=0.4264, Time=0.1085 sec\n",
      "Epoch [1262 / 3000]: Train Loss=3.8869, Val Cor=0.2615, Time=0.1086 sec\n",
      "Epoch [1263 / 3000]: Train Loss=3.9431, Val Cor=-0.2282, Time=0.1086 sec\n",
      "Epoch [1264 / 3000]: Train Loss=4.1055, Val Cor=0.3587, Time=0.1086 sec\n",
      "Epoch [1265 / 3000]: Train Loss=4.1590, Val Cor=0.4845, Time=0.1083 sec\n",
      "Epoch [1266 / 3000]: Train Loss=4.2086, Val Cor=0.3015, Time=0.1086 sec\n",
      "Epoch [1267 / 3000]: Train Loss=3.9502, Val Cor=0.3712, Time=0.1084 sec\n",
      "Epoch [1268 / 3000]: Train Loss=3.9785, Val Cor=-0.1815, Time=0.1084 sec\n",
      "Epoch [1269 / 3000]: Train Loss=4.0540, Val Cor=0.4635, Time=0.1082 sec\n",
      "Epoch [1270 / 3000]: Train Loss=4.0318, Val Cor=0.4382, Time=0.1083 sec\n",
      "Epoch [1271 / 3000]: Train Loss=3.9872, Val Cor=0.0709, Time=0.1083 sec\n",
      "Epoch [1272 / 3000]: Train Loss=3.8781, Val Cor=0.4762, Time=0.1083 sec\n",
      "Epoch [1273 / 3000]: Train Loss=3.7922, Val Cor=0.4418, Time=0.1093 sec\n",
      "Epoch [1274 / 3000]: Train Loss=3.9382, Val Cor=0.2982, Time=0.1105 sec\n",
      "Epoch [1275 / 3000]: Train Loss=3.8150, Val Cor=0.4559, Time=0.1103 sec\n",
      "Epoch [1276 / 3000]: Train Loss=3.7354, Val Cor=0.4906, Time=0.1105 sec\n",
      "Epoch [1277 / 3000]: Train Loss=3.9862, Val Cor=0.4146, Time=0.1101 sec\n",
      "Epoch [1278 / 3000]: Train Loss=3.8200, Val Cor=0.4749, Time=0.1105 sec\n",
      "Epoch [1279 / 3000]: Train Loss=3.7820, Val Cor=0.4093, Time=0.1102 sec\n",
      "Epoch [1280 / 3000]: Train Loss=3.8693, Val Cor=0.4547, Time=0.1114 sec\n",
      "Epoch [1281 / 3000]: Train Loss=3.9371, Val Cor=0.4649, Time=0.1097 sec\n",
      "Epoch [1282 / 3000]: Train Loss=3.8291, Val Cor=0.4205, Time=0.1091 sec\n",
      "Epoch [1283 / 3000]: Train Loss=4.1665, Val Cor=0.4521, Time=0.1087 sec\n",
      "Epoch [1284 / 3000]: Train Loss=3.9739, Val Cor=0.3309, Time=0.1088 sec\n",
      "Epoch [1285 / 3000]: Train Loss=3.8293, Val Cor=0.3605, Time=0.1083 sec\n",
      "Epoch [1286 / 3000]: Train Loss=3.7590, Val Cor=0.4804, Time=0.1086 sec\n",
      "Epoch [1287 / 3000]: Train Loss=4.1397, Val Cor=0.3238, Time=0.1083 sec\n",
      "Epoch [1288 / 3000]: Train Loss=5.0220, Val Cor=0.4472, Time=0.1085 sec\n",
      "Epoch [1289 / 3000]: Train Loss=4.1636, Val Cor=0.2574, Time=0.1082 sec\n",
      "Epoch [1290 / 3000]: Train Loss=4.1952, Val Cor=0.4284, Time=0.1088 sec\n",
      "Epoch [1291 / 3000]: Train Loss=4.2477, Val Cor=0.5071, Time=0.1080 sec\n",
      "Epoch [1292 / 3000]: Train Loss=3.8472, Val Cor=0.3921, Time=0.1086 sec\n",
      "Epoch [1293 / 3000]: Train Loss=4.3508, Val Cor=0.5040, Time=0.1081 sec\n",
      "Epoch [1294 / 3000]: Train Loss=4.3042, Val Cor=0.4983, Time=0.1085 sec\n",
      "Epoch [1295 / 3000]: Train Loss=4.1433, Val Cor=0.4889, Time=0.1081 sec\n",
      "Epoch [1296 / 3000]: Train Loss=4.3531, Val Cor=0.5018, Time=0.1084 sec\n",
      "Epoch [1297 / 3000]: Train Loss=4.3746, Val Cor=0.4936, Time=0.1081 sec\n",
      "Epoch [1298 / 3000]: Train Loss=4.1829, Val Cor=0.4900, Time=0.1100 sec\n",
      "Epoch [1299 / 3000]: Train Loss=4.3177, Val Cor=0.5064, Time=0.2514 sec\n",
      "Epoch [1300 / 3000]: Train Loss=4.1150, Val Cor=0.5111, Time=0.1113 sec\n",
      "model_updated\n",
      "Epoch [1301 / 3000]: Train Loss=3.9949, Val Cor=0.5130, Time=0.1097 sec\n",
      "model_updated\n",
      "Epoch [1302 / 3000]: Train Loss=3.8198, Val Cor=0.5034, Time=0.1097 sec\n",
      "Epoch [1303 / 3000]: Train Loss=4.1012, Val Cor=0.4921, Time=0.1091 sec\n",
      "Epoch [1304 / 3000]: Train Loss=4.0665, Val Cor=0.4949, Time=0.1093 sec\n",
      "Epoch [1305 / 3000]: Train Loss=4.1760, Val Cor=0.5022, Time=0.1089 sec\n",
      "Epoch [1306 / 3000]: Train Loss=4.0556, Val Cor=0.4972, Time=0.1092 sec\n",
      "Epoch [1307 / 3000]: Train Loss=4.0716, Val Cor=0.5087, Time=0.1087 sec\n",
      "Epoch [1308 / 3000]: Train Loss=4.0625, Val Cor=0.5003, Time=0.1089 sec\n",
      "Epoch [1309 / 3000]: Train Loss=3.8958, Val Cor=0.4983, Time=0.1087 sec\n",
      "Epoch [1310 / 3000]: Train Loss=3.8709, Val Cor=0.5024, Time=0.1091 sec\n",
      "Epoch [1311 / 3000]: Train Loss=3.9775, Val Cor=0.4907, Time=0.1088 sec\n",
      "Epoch [1312 / 3000]: Train Loss=3.9501, Val Cor=0.4930, Time=0.1092 sec\n",
      "Epoch [1313 / 3000]: Train Loss=3.8566, Val Cor=0.5070, Time=0.1088 sec\n",
      "Epoch [1314 / 3000]: Train Loss=4.0168, Val Cor=0.5056, Time=0.1092 sec\n",
      "Epoch [1315 / 3000]: Train Loss=3.8530, Val Cor=0.4906, Time=0.1092 sec\n",
      "Epoch [1316 / 3000]: Train Loss=3.8542, Val Cor=0.4816, Time=0.1097 sec\n",
      "Epoch [1317 / 3000]: Train Loss=4.0099, Val Cor=0.5065, Time=0.1095 sec\n",
      "Epoch [1318 / 3000]: Train Loss=3.8892, Val Cor=0.5079, Time=0.1097 sec\n",
      "Epoch [1319 / 3000]: Train Loss=3.8124, Val Cor=0.4860, Time=0.1094 sec\n",
      "Epoch [1320 / 3000]: Train Loss=3.9906, Val Cor=0.4930, Time=0.1094 sec\n",
      "Epoch [1321 / 3000]: Train Loss=3.9620, Val Cor=0.4949, Time=0.1091 sec\n",
      "Epoch [1322 / 3000]: Train Loss=3.8003, Val Cor=0.5055, Time=0.1095 sec\n",
      "Epoch [1323 / 3000]: Train Loss=3.9906, Val Cor=0.5011, Time=0.1093 sec\n",
      "Epoch [1324 / 3000]: Train Loss=4.2366, Val Cor=0.4932, Time=0.1093 sec\n",
      "Epoch [1325 / 3000]: Train Loss=3.8934, Val Cor=0.4923, Time=0.1092 sec\n",
      "Epoch [1326 / 3000]: Train Loss=3.9979, Val Cor=0.4583, Time=0.1101 sec\n",
      "Epoch [1327 / 3000]: Train Loss=3.8383, Val Cor=0.5116, Time=0.1107 sec\n",
      "Epoch [1328 / 3000]: Train Loss=3.8688, Val Cor=0.4984, Time=0.1105 sec\n",
      "Epoch [1329 / 3000]: Train Loss=3.8389, Val Cor=0.3978, Time=0.1112 sec\n",
      "Epoch [1330 / 3000]: Train Loss=3.7787, Val Cor=0.5054, Time=0.1108 sec\n",
      "Epoch [1331 / 3000]: Train Loss=3.6651, Val Cor=0.5034, Time=0.1092 sec\n",
      "Epoch [1332 / 3000]: Train Loss=3.8121, Val Cor=0.5129, Time=0.1088 sec\n",
      "Epoch [1333 / 3000]: Train Loss=4.0599, Val Cor=0.4638, Time=0.1089 sec\n",
      "Epoch [1334 / 3000]: Train Loss=3.9443, Val Cor=0.2002, Time=0.1084 sec\n",
      "Epoch [1335 / 3000]: Train Loss=4.1204, Val Cor=0.4382, Time=0.1089 sec\n",
      "Epoch [1336 / 3000]: Train Loss=3.8972, Val Cor=0.4754, Time=0.1084 sec\n",
      "Epoch [1337 / 3000]: Train Loss=3.6902, Val Cor=0.4918, Time=0.1086 sec\n",
      "Epoch [1338 / 3000]: Train Loss=4.4222, Val Cor=0.4960, Time=0.1085 sec\n",
      "Epoch [1339 / 3000]: Train Loss=4.2472, Val Cor=0.1898, Time=0.1084 sec\n",
      "Epoch [1340 / 3000]: Train Loss=4.0923, Val Cor=0.5058, Time=0.1082 sec\n",
      "Epoch [1341 / 3000]: Train Loss=3.9797, Val Cor=0.4932, Time=0.1087 sec\n",
      "Epoch [1342 / 3000]: Train Loss=3.7707, Val Cor=0.4998, Time=0.1083 sec\n",
      "Epoch [1343 / 3000]: Train Loss=3.8814, Val Cor=0.4983, Time=0.1082 sec\n",
      "Epoch [1344 / 3000]: Train Loss=3.9427, Val Cor=0.4227, Time=0.1081 sec\n",
      "Epoch [1345 / 3000]: Train Loss=3.9597, Val Cor=0.4907, Time=0.1080 sec\n",
      "Epoch [1346 / 3000]: Train Loss=3.8168, Val Cor=0.5060, Time=0.1080 sec\n",
      "Epoch [1347 / 3000]: Train Loss=3.8122, Val Cor=0.4960, Time=0.1088 sec\n",
      "Epoch [1348 / 3000]: Train Loss=3.8577, Val Cor=0.0245, Time=0.1082 sec\n",
      "Epoch [1349 / 3000]: Train Loss=3.6452, Val Cor=0.4679, Time=0.1084 sec\n",
      "Epoch [1350 / 3000]: Train Loss=3.6958, Val Cor=0.4711, Time=0.1082 sec\n",
      "Epoch [1351 / 3000]: Train Loss=3.7858, Val Cor=0.4966, Time=0.1083 sec\n",
      "Epoch [1352 / 3000]: Train Loss=3.8299, Val Cor=0.5147, Time=0.1080 sec\n",
      "model_updated\n",
      "Epoch [1353 / 3000]: Train Loss=3.8020, Val Cor=0.4701, Time=0.1081 sec\n",
      "Epoch [1354 / 3000]: Train Loss=3.7949, Val Cor=0.4856, Time=0.1083 sec\n",
      "Epoch [1355 / 3000]: Train Loss=3.8730, Val Cor=0.4617, Time=0.1086 sec\n",
      "Epoch [1356 / 3000]: Train Loss=4.1379, Val Cor=0.4889, Time=0.1085 sec\n",
      "Epoch [1357 / 3000]: Train Loss=3.8424, Val Cor=0.3728, Time=0.1087 sec\n",
      "Epoch [1358 / 3000]: Train Loss=3.8869, Val Cor=0.4563, Time=0.1086 sec\n",
      "Epoch [1359 / 3000]: Train Loss=3.7361, Val Cor=0.4367, Time=0.1091 sec\n",
      "Epoch [1360 / 3000]: Train Loss=3.8243, Val Cor=0.4937, Time=0.1085 sec\n",
      "Epoch [1361 / 3000]: Train Loss=4.3379, Val Cor=0.4945, Time=0.1085 sec\n",
      "Epoch [1362 / 3000]: Train Loss=5.3694, Val Cor=0.4980, Time=0.1083 sec\n",
      "Epoch [1363 / 3000]: Train Loss=4.7177, Val Cor=0.5052, Time=0.1083 sec\n",
      "Epoch [1364 / 3000]: Train Loss=3.8899, Val Cor=0.5020, Time=0.1089 sec\n",
      "Epoch [1365 / 3000]: Train Loss=3.8015, Val Cor=0.5113, Time=0.1093 sec\n",
      "Epoch [1366 / 3000]: Train Loss=3.7992, Val Cor=0.4864, Time=0.1092 sec\n",
      "Epoch [1367 / 3000]: Train Loss=3.6209, Val Cor=0.4936, Time=0.1088 sec\n",
      "Epoch [1368 / 3000]: Train Loss=4.1625, Val Cor=0.2995, Time=0.1081 sec\n",
      "Epoch [1369 / 3000]: Train Loss=4.0819, Val Cor=0.4790, Time=0.1082 sec\n",
      "Epoch [1370 / 3000]: Train Loss=3.7981, Val Cor=0.4884, Time=0.1080 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1371 / 3000]: Train Loss=3.9232, Val Cor=0.4084, Time=0.1080 sec\n",
      "Epoch [1372 / 3000]: Train Loss=4.0603, Val Cor=-0.1281, Time=0.1079 sec\n",
      "Epoch [1373 / 3000]: Train Loss=5.1620, Val Cor=0.1285, Time=0.1083 sec\n",
      "Epoch [1374 / 3000]: Train Loss=5.5684, Val Cor=0.4582, Time=0.1081 sec\n",
      "Epoch [1375 / 3000]: Train Loss=5.3740, Val Cor=0.4648, Time=0.1082 sec\n",
      "Epoch [1376 / 3000]: Train Loss=5.0860, Val Cor=0.4953, Time=0.1080 sec\n",
      "Epoch [1377 / 3000]: Train Loss=4.3796, Val Cor=0.5052, Time=0.1079 sec\n",
      "Epoch [1378 / 3000]: Train Loss=4.0194, Val Cor=0.5060, Time=0.1078 sec\n",
      "Epoch [1379 / 3000]: Train Loss=4.0755, Val Cor=0.5072, Time=0.1080 sec\n",
      "Epoch [1380 / 3000]: Train Loss=3.8464, Val Cor=0.4919, Time=0.1079 sec\n",
      "Epoch [1381 / 3000]: Train Loss=3.6825, Val Cor=0.4927, Time=0.1079 sec\n",
      "Epoch [1382 / 3000]: Train Loss=3.8696, Val Cor=0.5169, Time=0.1078 sec\n",
      "model_updated\n",
      "Epoch [1383 / 3000]: Train Loss=3.7531, Val Cor=0.5131, Time=0.1080 sec\n",
      "Epoch [1384 / 3000]: Train Loss=3.8223, Val Cor=0.4143, Time=0.1080 sec\n",
      "Epoch [1385 / 3000]: Train Loss=3.8275, Val Cor=0.5036, Time=0.1080 sec\n",
      "Epoch [1386 / 3000]: Train Loss=3.7958, Val Cor=0.4825, Time=0.1077 sec\n",
      "Epoch [1387 / 3000]: Train Loss=3.6729, Val Cor=0.5050, Time=0.1081 sec\n",
      "Epoch [1388 / 3000]: Train Loss=3.7221, Val Cor=0.4914, Time=0.1079 sec\n",
      "Epoch [1389 / 3000]: Train Loss=3.9150, Val Cor=0.4940, Time=0.1081 sec\n",
      "Epoch [1390 / 3000]: Train Loss=3.8822, Val Cor=0.4704, Time=0.1079 sec\n",
      "Epoch [1391 / 3000]: Train Loss=3.9015, Val Cor=0.4728, Time=0.1079 sec\n",
      "Epoch [1392 / 3000]: Train Loss=4.1109, Val Cor=0.4834, Time=0.1077 sec\n",
      "Epoch [1393 / 3000]: Train Loss=3.8147, Val Cor=0.4787, Time=0.1078 sec\n",
      "Epoch [1394 / 3000]: Train Loss=3.7773, Val Cor=0.4932, Time=0.1077 sec\n",
      "Epoch [1395 / 3000]: Train Loss=3.9470, Val Cor=0.5042, Time=0.1079 sec\n",
      "Epoch [1396 / 3000]: Train Loss=3.9931, Val Cor=0.5042, Time=0.1077 sec\n",
      "Epoch [1397 / 3000]: Train Loss=3.7905, Val Cor=0.5069, Time=0.1078 sec\n",
      "Epoch [1398 / 3000]: Train Loss=3.9579, Val Cor=0.5117, Time=0.1077 sec\n",
      "Epoch [1399 / 3000]: Train Loss=3.8858, Val Cor=0.4993, Time=0.1076 sec\n",
      "Epoch [1400 / 3000]: Train Loss=3.7635, Val Cor=0.4666, Time=0.1077 sec\n",
      "Epoch [1401 / 3000]: Train Loss=3.7657, Val Cor=0.4979, Time=0.1079 sec\n",
      "Epoch [1402 / 3000]: Train Loss=3.8390, Val Cor=0.4979, Time=0.1079 sec\n",
      "Epoch [1403 / 3000]: Train Loss=3.7510, Val Cor=0.5048, Time=0.1080 sec\n",
      "Epoch [1404 / 3000]: Train Loss=3.8301, Val Cor=0.5130, Time=0.1076 sec\n",
      "Epoch [1405 / 3000]: Train Loss=3.9117, Val Cor=0.5142, Time=0.1078 sec\n",
      "Epoch [1406 / 3000]: Train Loss=4.1552, Val Cor=0.5061, Time=0.1076 sec\n",
      "Epoch [1407 / 3000]: Train Loss=4.0067, Val Cor=0.5103, Time=0.1078 sec\n",
      "Epoch [1408 / 3000]: Train Loss=3.9481, Val Cor=0.5128, Time=0.1077 sec\n",
      "Epoch [1409 / 3000]: Train Loss=4.0402, Val Cor=0.5099, Time=0.1079 sec\n",
      "Epoch [1410 / 3000]: Train Loss=3.8790, Val Cor=0.5095, Time=0.1077 sec\n",
      "Epoch [1411 / 3000]: Train Loss=4.0289, Val Cor=0.5138, Time=0.1078 sec\n",
      "Epoch [1412 / 3000]: Train Loss=4.1026, Val Cor=0.3955, Time=0.1079 sec\n",
      "Epoch [1413 / 3000]: Train Loss=4.2798, Val Cor=0.5014, Time=0.1079 sec\n",
      "Epoch [1414 / 3000]: Train Loss=4.2958, Val Cor=0.5100, Time=0.1079 sec\n",
      "Epoch [1415 / 3000]: Train Loss=4.0700, Val Cor=0.5058, Time=0.1077 sec\n",
      "Epoch [1416 / 3000]: Train Loss=3.9290, Val Cor=0.4345, Time=0.1077 sec\n",
      "Epoch [1417 / 3000]: Train Loss=4.1895, Val Cor=0.4633, Time=0.1079 sec\n",
      "Epoch [1418 / 3000]: Train Loss=3.9617, Val Cor=0.5052, Time=0.1077 sec\n",
      "Epoch [1419 / 3000]: Train Loss=4.0728, Val Cor=0.5058, Time=0.1081 sec\n",
      "Epoch [1420 / 3000]: Train Loss=3.9410, Val Cor=0.4331, Time=0.1079 sec\n",
      "Epoch [1421 / 3000]: Train Loss=3.7925, Val Cor=0.4937, Time=0.1076 sec\n",
      "Epoch [1422 / 3000]: Train Loss=4.0784, Val Cor=0.5031, Time=0.1080 sec\n",
      "Epoch [1423 / 3000]: Train Loss=3.8587, Val Cor=0.5116, Time=0.1092 sec\n",
      "Epoch [1424 / 3000]: Train Loss=3.9966, Val Cor=0.5079, Time=0.1083 sec\n",
      "Epoch [1425 / 3000]: Train Loss=3.8721, Val Cor=0.5127, Time=0.1081 sec\n",
      "Epoch [1426 / 3000]: Train Loss=3.9971, Val Cor=0.5106, Time=0.1078 sec\n",
      "Epoch [1427 / 3000]: Train Loss=4.2022, Val Cor=0.5126, Time=0.1079 sec\n",
      "Epoch [1428 / 3000]: Train Loss=4.0848, Val Cor=0.5113, Time=0.1079 sec\n",
      "Epoch [1429 / 3000]: Train Loss=3.9140, Val Cor=0.4512, Time=0.1080 sec\n",
      "Epoch [1430 / 3000]: Train Loss=3.8193, Val Cor=0.4329, Time=0.1079 sec\n",
      "Epoch [1431 / 3000]: Train Loss=3.9220, Val Cor=0.5117, Time=0.1080 sec\n",
      "Epoch [1432 / 3000]: Train Loss=3.7749, Val Cor=0.5112, Time=0.1077 sec\n",
      "Epoch [1433 / 3000]: Train Loss=4.0050, Val Cor=0.5042, Time=0.1082 sec\n",
      "Epoch [1434 / 3000]: Train Loss=3.8936, Val Cor=0.5041, Time=0.1078 sec\n",
      "Epoch [1435 / 3000]: Train Loss=3.9971, Val Cor=0.4056, Time=0.1079 sec\n",
      "Epoch [1436 / 3000]: Train Loss=3.9470, Val Cor=0.0607, Time=0.1077 sec\n",
      "Epoch [1437 / 3000]: Train Loss=4.0129, Val Cor=0.5116, Time=0.1082 sec\n",
      "Epoch [1438 / 3000]: Train Loss=3.9816, Val Cor=0.4564, Time=0.1078 sec\n",
      "Epoch [1439 / 3000]: Train Loss=3.8286, Val Cor=0.5055, Time=0.1080 sec\n",
      "Epoch [1440 / 3000]: Train Loss=3.8636, Val Cor=0.4961, Time=0.1075 sec\n",
      "Epoch [1441 / 3000]: Train Loss=3.8028, Val Cor=0.5204, Time=0.1077 sec\n",
      "model_updated\n",
      "Epoch [1442 / 3000]: Train Loss=4.1065, Val Cor=0.4799, Time=0.1076 sec\n",
      "Epoch [1443 / 3000]: Train Loss=3.9122, Val Cor=0.5141, Time=0.1082 sec\n",
      "Epoch [1444 / 3000]: Train Loss=3.7411, Val Cor=0.3504, Time=0.1076 sec\n",
      "Epoch [1445 / 3000]: Train Loss=3.8208, Val Cor=0.4343, Time=0.1077 sec\n",
      "Epoch [1446 / 3000]: Train Loss=4.0838, Val Cor=0.5064, Time=0.1076 sec\n",
      "Epoch [1447 / 3000]: Train Loss=4.0441, Val Cor=0.5080, Time=0.1077 sec\n",
      "Epoch [1448 / 3000]: Train Loss=4.0951, Val Cor=0.2860, Time=0.1074 sec\n",
      "Epoch [1449 / 3000]: Train Loss=4.0492, Val Cor=0.0328, Time=0.1077 sec\n",
      "Epoch [1450 / 3000]: Train Loss=3.9860, Val Cor=0.3949, Time=0.1075 sec\n",
      "Epoch [1451 / 3000]: Train Loss=3.9917, Val Cor=0.4133, Time=0.1076 sec\n",
      "Epoch [1452 / 3000]: Train Loss=3.7804, Val Cor=0.2388, Time=0.1075 sec\n",
      "Epoch [1453 / 3000]: Train Loss=3.7685, Val Cor=0.4996, Time=0.1076 sec\n",
      "Epoch [1454 / 3000]: Train Loss=3.8479, Val Cor=0.4367, Time=0.1075 sec\n",
      "Epoch [1455 / 3000]: Train Loss=3.8656, Val Cor=0.5174, Time=0.1078 sec\n",
      "Epoch [1456 / 3000]: Train Loss=3.8255, Val Cor=0.5147, Time=0.1075 sec\n",
      "Epoch [1457 / 3000]: Train Loss=3.7423, Val Cor=0.3889, Time=0.1077 sec\n",
      "Epoch [1458 / 3000]: Train Loss=4.1815, Val Cor=0.4965, Time=0.1074 sec\n",
      "Epoch [1459 / 3000]: Train Loss=3.8669, Val Cor=0.1294, Time=0.1075 sec\n",
      "Epoch [1460 / 3000]: Train Loss=3.7335, Val Cor=0.4493, Time=0.1074 sec\n",
      "Epoch [1461 / 3000]: Train Loss=3.7902, Val Cor=0.4044, Time=0.1079 sec\n",
      "Epoch [1462 / 3000]: Train Loss=3.6309, Val Cor=0.4523, Time=0.1076 sec\n",
      "Epoch [1463 / 3000]: Train Loss=3.6438, Val Cor=0.4533, Time=0.1077 sec\n",
      "Epoch [1464 / 3000]: Train Loss=3.6051, Val Cor=0.4895, Time=0.1074 sec\n",
      "Epoch [1465 / 3000]: Train Loss=3.7505, Val Cor=0.0380, Time=0.1078 sec\n",
      "Epoch [1466 / 3000]: Train Loss=3.6107, Val Cor=0.4889, Time=0.1076 sec\n",
      "Epoch [1467 / 3000]: Train Loss=3.5129, Val Cor=0.5052, Time=0.1078 sec\n",
      "Epoch [1468 / 3000]: Train Loss=3.7178, Val Cor=0.3466, Time=0.1074 sec\n",
      "Epoch [1469 / 3000]: Train Loss=3.6896, Val Cor=0.2586, Time=0.1075 sec\n",
      "Epoch [1470 / 3000]: Train Loss=3.7475, Val Cor=0.5114, Time=0.1076 sec\n",
      "Epoch [1471 / 3000]: Train Loss=3.8566, Val Cor=0.4628, Time=0.1075 sec\n",
      "Epoch [1472 / 3000]: Train Loss=3.7098, Val Cor=0.3575, Time=0.1075 sec\n",
      "Epoch [1473 / 3000]: Train Loss=3.8319, Val Cor=0.3706, Time=0.1077 sec\n",
      "Epoch [1474 / 3000]: Train Loss=3.7894, Val Cor=0.4259, Time=0.1076 sec\n",
      "Epoch [1475 / 3000]: Train Loss=3.7360, Val Cor=0.4361, Time=0.1076 sec\n",
      "Epoch [1476 / 3000]: Train Loss=3.6311, Val Cor=0.4851, Time=0.1077 sec\n",
      "Epoch [1477 / 3000]: Train Loss=3.6888, Val Cor=0.4716, Time=0.1076 sec\n",
      "Epoch [1478 / 3000]: Train Loss=3.7938, Val Cor=0.2506, Time=0.1075 sec\n",
      "Epoch [1479 / 3000]: Train Loss=3.8781, Val Cor=0.3865, Time=0.1077 sec\n",
      "Epoch [1480 / 3000]: Train Loss=4.1364, Val Cor=0.0013, Time=0.1075 sec\n",
      "Epoch [1481 / 3000]: Train Loss=3.8629, Val Cor=0.4646, Time=0.1075 sec\n",
      "Epoch [1482 / 3000]: Train Loss=3.8897, Val Cor=0.4183, Time=0.1075 sec\n",
      "Epoch [1483 / 3000]: Train Loss=3.7687, Val Cor=0.4765, Time=0.1077 sec\n",
      "Epoch [1484 / 3000]: Train Loss=3.7108, Val Cor=0.4505, Time=0.1075 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1485 / 3000]: Train Loss=4.1460, Val Cor=0.4486, Time=0.1077 sec\n",
      "Epoch [1486 / 3000]: Train Loss=3.7289, Val Cor=0.4375, Time=0.1075 sec\n",
      "Epoch [1487 / 3000]: Train Loss=3.9204, Val Cor=0.5062, Time=0.1074 sec\n",
      "Epoch [1488 / 3000]: Train Loss=3.6380, Val Cor=0.4817, Time=0.1076 sec\n",
      "Epoch [1489 / 3000]: Train Loss=3.7110, Val Cor=0.2559, Time=0.1076 sec\n",
      "Epoch [1490 / 3000]: Train Loss=3.5933, Val Cor=0.4797, Time=0.1077 sec\n",
      "Epoch [1491 / 3000]: Train Loss=3.6562, Val Cor=0.5095, Time=0.1077 sec\n",
      "Epoch [1492 / 3000]: Train Loss=3.4842, Val Cor=0.4878, Time=0.1075 sec\n",
      "Epoch [1493 / 3000]: Train Loss=3.6056, Val Cor=0.4924, Time=0.1076 sec\n",
      "Epoch [1494 / 3000]: Train Loss=3.6214, Val Cor=0.5142, Time=0.1073 sec\n",
      "Epoch [1495 / 3000]: Train Loss=3.8866, Val Cor=0.0315, Time=0.1076 sec\n",
      "Epoch [1496 / 3000]: Train Loss=3.9822, Val Cor=0.1056, Time=0.1075 sec\n",
      "Epoch [1497 / 3000]: Train Loss=4.1572, Val Cor=0.4273, Time=0.1077 sec\n",
      "Epoch [1498 / 3000]: Train Loss=3.7834, Val Cor=0.3278, Time=0.1075 sec\n",
      "Epoch [1499 / 3000]: Train Loss=3.7622, Val Cor=0.4390, Time=0.1077 sec\n",
      "Epoch [1500 / 3000]: Train Loss=4.0710, Val Cor=0.0592, Time=0.1074 sec\n",
      "Epoch [1501 / 3000]: Train Loss=3.9514, Val Cor=0.5149, Time=0.1075 sec\n",
      "Epoch [1502 / 3000]: Train Loss=3.8611, Val Cor=0.5191, Time=0.1075 sec\n",
      "Epoch [1503 / 3000]: Train Loss=4.0101, Val Cor=0.4465, Time=0.1078 sec\n",
      "Epoch [1504 / 3000]: Train Loss=4.1383, Val Cor=0.2697, Time=0.1075 sec\n",
      "Epoch [1505 / 3000]: Train Loss=4.0250, Val Cor=0.3986, Time=0.1076 sec\n",
      "Epoch [1506 / 3000]: Train Loss=3.7875, Val Cor=0.3273, Time=0.1075 sec\n",
      "Epoch [1507 / 3000]: Train Loss=3.7202, Val Cor=0.4408, Time=0.1077 sec\n",
      "Epoch [1508 / 3000]: Train Loss=3.5674, Val Cor=0.4369, Time=0.1076 sec\n",
      "Epoch [1509 / 3000]: Train Loss=3.7451, Val Cor=0.4261, Time=0.1076 sec\n",
      "Epoch [1510 / 3000]: Train Loss=3.6161, Val Cor=0.4261, Time=0.1074 sec\n",
      "Epoch [1511 / 3000]: Train Loss=3.4965, Val Cor=0.2802, Time=0.1077 sec\n",
      "Epoch [1512 / 3000]: Train Loss=3.5612, Val Cor=0.4320, Time=0.1075 sec\n",
      "Epoch [1513 / 3000]: Train Loss=3.9619, Val Cor=0.2902, Time=0.1076 sec\n",
      "Epoch [1514 / 3000]: Train Loss=5.1135, Val Cor=0.3712, Time=0.1075 sec\n",
      "Epoch [1515 / 3000]: Train Loss=5.8349, Val Cor=0.4645, Time=0.1076 sec\n",
      "Epoch [1516 / 3000]: Train Loss=5.7848, Val Cor=0.4701, Time=0.1078 sec\n",
      "Epoch [1517 / 3000]: Train Loss=5.7820, Val Cor=0.4579, Time=0.1080 sec\n",
      "Epoch [1518 / 3000]: Train Loss=5.7814, Val Cor=0.4452, Time=0.1078 sec\n",
      "Epoch [1519 / 3000]: Train Loss=5.7813, Val Cor=0.4467, Time=0.1078 sec\n",
      "Epoch [1520 / 3000]: Train Loss=5.7591, Val Cor=0.4491, Time=0.1076 sec\n",
      "Epoch [1521 / 3000]: Train Loss=5.7287, Val Cor=0.4571, Time=0.1077 sec\n",
      "Epoch [1522 / 3000]: Train Loss=5.7744, Val Cor=0.4393, Time=0.1079 sec\n",
      "Epoch [1523 / 3000]: Train Loss=5.6984, Val Cor=0.4639, Time=0.1080 sec\n",
      "Epoch [1524 / 3000]: Train Loss=5.4609, Val Cor=0.4770, Time=0.1075 sec\n",
      "Epoch [1525 / 3000]: Train Loss=5.3245, Val Cor=0.4642, Time=0.1076 sec\n",
      "Epoch [1526 / 3000]: Train Loss=5.0883, Val Cor=0.4759, Time=0.1077 sec\n",
      "Epoch [1527 / 3000]: Train Loss=4.9554, Val Cor=0.4913, Time=0.1075 sec\n",
      "Epoch [1528 / 3000]: Train Loss=4.8259, Val Cor=0.5012, Time=0.1075 sec\n",
      "Epoch [1529 / 3000]: Train Loss=4.9313, Val Cor=0.5022, Time=0.1075 sec\n",
      "Epoch [1530 / 3000]: Train Loss=4.9263, Val Cor=0.5028, Time=0.1076 sec\n",
      "Epoch [1531 / 3000]: Train Loss=4.8522, Val Cor=0.5043, Time=0.1078 sec\n",
      "Epoch [1532 / 3000]: Train Loss=4.9445, Val Cor=0.5055, Time=0.1076 sec\n",
      "Epoch [1533 / 3000]: Train Loss=4.7078, Val Cor=0.5090, Time=0.1076 sec\n",
      "Epoch [1534 / 3000]: Train Loss=4.9009, Val Cor=0.5043, Time=0.1075 sec\n",
      "Epoch [1535 / 3000]: Train Loss=4.7527, Val Cor=0.5079, Time=0.1079 sec\n",
      "Epoch [1536 / 3000]: Train Loss=4.6536, Val Cor=0.5111, Time=0.1076 sec\n",
      "Epoch [1537 / 3000]: Train Loss=4.5665, Val Cor=0.5119, Time=0.1078 sec\n",
      "Epoch [1538 / 3000]: Train Loss=4.5508, Val Cor=0.5117, Time=0.1076 sec\n",
      "Epoch [1539 / 3000]: Train Loss=4.4532, Val Cor=0.5101, Time=0.1077 sec\n",
      "Epoch [1540 / 3000]: Train Loss=4.5531, Val Cor=0.5124, Time=0.1077 sec\n",
      "Epoch [1541 / 3000]: Train Loss=4.7171, Val Cor=0.5081, Time=0.1077 sec\n",
      "Epoch [1542 / 3000]: Train Loss=4.7083, Val Cor=0.5132, Time=0.1075 sec\n",
      "Epoch [1543 / 3000]: Train Loss=4.5115, Val Cor=0.5109, Time=0.1076 sec\n",
      "Epoch [1544 / 3000]: Train Loss=4.4031, Val Cor=0.5119, Time=0.1077 sec\n",
      "Epoch [1545 / 3000]: Train Loss=4.7957, Val Cor=0.5101, Time=0.1078 sec\n",
      "Epoch [1546 / 3000]: Train Loss=4.6357, Val Cor=0.5116, Time=0.1075 sec\n",
      "Epoch [1547 / 3000]: Train Loss=4.5412, Val Cor=0.5137, Time=0.1078 sec\n",
      "Epoch [1548 / 3000]: Train Loss=4.5003, Val Cor=0.5080, Time=0.1075 sec\n",
      "Epoch [1549 / 3000]: Train Loss=4.6945, Val Cor=0.5119, Time=0.1078 sec\n",
      "Epoch [1550 / 3000]: Train Loss=4.6514, Val Cor=0.5160, Time=0.1076 sec\n",
      "Epoch [1551 / 3000]: Train Loss=4.6868, Val Cor=0.5108, Time=0.1078 sec\n",
      "Epoch [1552 / 3000]: Train Loss=4.3480, Val Cor=0.5161, Time=0.1074 sec\n",
      "Epoch [1553 / 3000]: Train Loss=4.4773, Val Cor=0.5173, Time=0.1076 sec\n",
      "Epoch [1554 / 3000]: Train Loss=4.4280, Val Cor=0.5131, Time=0.1075 sec\n",
      "Epoch [1555 / 3000]: Train Loss=4.3876, Val Cor=0.5118, Time=0.1075 sec\n",
      "Epoch [1556 / 3000]: Train Loss=4.5786, Val Cor=0.5145, Time=0.1077 sec\n",
      "Epoch [1557 / 3000]: Train Loss=4.5398, Val Cor=0.5154, Time=0.1078 sec\n",
      "Epoch [1558 / 3000]: Train Loss=4.4778, Val Cor=0.5156, Time=0.1077 sec\n",
      "Epoch [1559 / 3000]: Train Loss=4.5149, Val Cor=0.5114, Time=0.1078 sec\n",
      "Epoch [1560 / 3000]: Train Loss=4.3423, Val Cor=0.5142, Time=0.1077 sec\n",
      "Epoch [1561 / 3000]: Train Loss=4.4438, Val Cor=0.5076, Time=0.1077 sec\n",
      "Epoch [1562 / 3000]: Train Loss=4.4914, Val Cor=0.5110, Time=0.1075 sec\n",
      "Epoch [1563 / 3000]: Train Loss=4.2824, Val Cor=0.5134, Time=0.1078 sec\n",
      "Epoch [1564 / 3000]: Train Loss=4.5347, Val Cor=0.5092, Time=0.1076 sec\n",
      "Epoch [1565 / 3000]: Train Loss=4.6617, Val Cor=0.5081, Time=0.1077 sec\n",
      "Epoch [1566 / 3000]: Train Loss=4.4245, Val Cor=0.5083, Time=0.1075 sec\n",
      "Epoch [1567 / 3000]: Train Loss=4.1805, Val Cor=0.5120, Time=0.1079 sec\n",
      "Epoch [1568 / 3000]: Train Loss=4.3563, Val Cor=0.5145, Time=0.1084 sec\n",
      "Epoch [1569 / 3000]: Train Loss=4.3476, Val Cor=0.5161, Time=0.1103 sec\n",
      "Epoch [1570 / 3000]: Train Loss=4.2764, Val Cor=0.5134, Time=0.1098 sec\n",
      "Epoch [1571 / 3000]: Train Loss=4.3376, Val Cor=0.5132, Time=0.1094 sec\n",
      "Epoch [1572 / 3000]: Train Loss=4.4359, Val Cor=0.5098, Time=0.1102 sec\n",
      "Epoch [1573 / 3000]: Train Loss=4.2452, Val Cor=0.5006, Time=0.1090 sec\n",
      "Epoch [1574 / 3000]: Train Loss=4.1042, Val Cor=0.5027, Time=0.1097 sec\n",
      "Epoch [1575 / 3000]: Train Loss=4.5173, Val Cor=0.4928, Time=0.1106 sec\n",
      "Epoch [1576 / 3000]: Train Loss=4.2954, Val Cor=0.4778, Time=0.1091 sec\n",
      "Epoch [1577 / 3000]: Train Loss=4.1519, Val Cor=0.4993, Time=0.1079 sec\n",
      "Epoch [1578 / 3000]: Train Loss=4.3117, Val Cor=0.4973, Time=0.1078 sec\n",
      "Epoch [1579 / 3000]: Train Loss=4.5291, Val Cor=0.4955, Time=0.1078 sec\n",
      "Epoch [1580 / 3000]: Train Loss=4.3984, Val Cor=0.4932, Time=0.1076 sec\n",
      "Epoch [1581 / 3000]: Train Loss=4.1762, Val Cor=0.5014, Time=0.1077 sec\n",
      "Epoch [1582 / 3000]: Train Loss=4.1840, Val Cor=0.5013, Time=0.1076 sec\n",
      "Epoch [1583 / 3000]: Train Loss=4.1214, Val Cor=0.5096, Time=0.1077 sec\n",
      "Epoch [1584 / 3000]: Train Loss=4.3282, Val Cor=0.5046, Time=0.1074 sec\n",
      "Epoch [1585 / 3000]: Train Loss=4.1023, Val Cor=0.5008, Time=0.1077 sec\n",
      "Epoch [1586 / 3000]: Train Loss=4.1698, Val Cor=0.5048, Time=0.1075 sec\n",
      "Epoch [1587 / 3000]: Train Loss=4.1917, Val Cor=0.5031, Time=0.1077 sec\n",
      "Epoch [1588 / 3000]: Train Loss=3.9853, Val Cor=0.5022, Time=0.1075 sec\n",
      "Epoch [1589 / 3000]: Train Loss=4.0706, Val Cor=0.4874, Time=0.1077 sec\n",
      "Epoch [1590 / 3000]: Train Loss=4.0882, Val Cor=0.4961, Time=0.1075 sec\n",
      "Epoch [1591 / 3000]: Train Loss=4.1006, Val Cor=0.5047, Time=0.1076 sec\n",
      "Epoch [1592 / 3000]: Train Loss=4.0799, Val Cor=0.5082, Time=0.1076 sec\n",
      "Epoch [1593 / 3000]: Train Loss=4.1791, Val Cor=0.5005, Time=0.1076 sec\n",
      "Epoch [1594 / 3000]: Train Loss=4.0749, Val Cor=0.4970, Time=0.1075 sec\n",
      "Epoch [1595 / 3000]: Train Loss=4.3148, Val Cor=0.5050, Time=0.1076 sec\n",
      "Epoch [1596 / 3000]: Train Loss=4.1930, Val Cor=0.4978, Time=0.1075 sec\n",
      "Epoch [1597 / 3000]: Train Loss=4.0584, Val Cor=0.5069, Time=0.1077 sec\n",
      "Epoch [1598 / 3000]: Train Loss=4.3409, Val Cor=0.4317, Time=0.1076 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1599 / 3000]: Train Loss=4.7739, Val Cor=0.5096, Time=0.1077 sec\n",
      "Epoch [1600 / 3000]: Train Loss=4.3426, Val Cor=0.5060, Time=0.1073 sec\n",
      "Epoch [1601 / 3000]: Train Loss=4.2025, Val Cor=0.4792, Time=0.1077 sec\n",
      "Epoch [1602 / 3000]: Train Loss=4.4718, Val Cor=0.4955, Time=0.1076 sec\n",
      "Epoch [1603 / 3000]: Train Loss=4.3043, Val Cor=0.4941, Time=0.1076 sec\n",
      "Epoch [1604 / 3000]: Train Loss=4.2405, Val Cor=0.4972, Time=0.1076 sec\n",
      "Epoch [1605 / 3000]: Train Loss=3.9931, Val Cor=0.4974, Time=0.1075 sec\n",
      "Epoch [1606 / 3000]: Train Loss=4.0272, Val Cor=0.4865, Time=0.1076 sec\n",
      "Epoch [1607 / 3000]: Train Loss=3.7407, Val Cor=0.4924, Time=0.1075 sec\n",
      "Epoch [1608 / 3000]: Train Loss=3.8865, Val Cor=0.5010, Time=0.1076 sec\n",
      "Epoch [1609 / 3000]: Train Loss=4.1951, Val Cor=0.4980, Time=0.1074 sec\n",
      "Epoch [1610 / 3000]: Train Loss=3.9806, Val Cor=0.4876, Time=0.1076 sec\n",
      "Epoch [1611 / 3000]: Train Loss=3.9484, Val Cor=0.4642, Time=0.1080 sec\n",
      "Epoch [1612 / 3000]: Train Loss=4.0807, Val Cor=0.4782, Time=0.1077 sec\n",
      "Epoch [1613 / 3000]: Train Loss=3.9596, Val Cor=0.5027, Time=0.1077 sec\n",
      "Epoch [1614 / 3000]: Train Loss=3.8873, Val Cor=0.4687, Time=0.1075 sec\n",
      "Epoch [1615 / 3000]: Train Loss=4.3430, Val Cor=0.4880, Time=0.1077 sec\n",
      "Epoch [1616 / 3000]: Train Loss=4.2158, Val Cor=0.4917, Time=0.1078 sec\n",
      "Epoch [1617 / 3000]: Train Loss=4.0646, Val Cor=0.5013, Time=0.1078 sec\n",
      "Epoch [1618 / 3000]: Train Loss=4.1517, Val Cor=0.4755, Time=0.1075 sec\n",
      "Epoch [1619 / 3000]: Train Loss=3.9533, Val Cor=0.5047, Time=0.1078 sec\n",
      "Epoch [1620 / 3000]: Train Loss=4.0720, Val Cor=0.2256, Time=0.1076 sec\n",
      "Epoch [1621 / 3000]: Train Loss=3.9389, Val Cor=0.4429, Time=0.1076 sec\n",
      "Epoch [1622 / 3000]: Train Loss=4.1058, Val Cor=0.4885, Time=0.1075 sec\n",
      "Epoch [1623 / 3000]: Train Loss=4.0777, Val Cor=0.1447, Time=0.1079 sec\n",
      "Epoch [1624 / 3000]: Train Loss=3.9857, Val Cor=0.5003, Time=0.1075 sec\n",
      "Epoch [1625 / 3000]: Train Loss=3.9395, Val Cor=0.4873, Time=0.1077 sec\n",
      "Epoch [1626 / 3000]: Train Loss=4.0945, Val Cor=0.2621, Time=0.1075 sec\n",
      "Epoch [1627 / 3000]: Train Loss=3.9212, Val Cor=0.2733, Time=0.1076 sec\n",
      "Epoch [1628 / 3000]: Train Loss=3.9791, Val Cor=0.4937, Time=0.1076 sec\n",
      "Epoch [1629 / 3000]: Train Loss=3.9381, Val Cor=0.4435, Time=0.1077 sec\n",
      "Epoch [1630 / 3000]: Train Loss=4.1988, Val Cor=0.4693, Time=0.1076 sec\n",
      "Epoch [1631 / 3000]: Train Loss=4.0538, Val Cor=0.4594, Time=0.1078 sec\n",
      "Epoch [1632 / 3000]: Train Loss=3.9466, Val Cor=0.4854, Time=0.1076 sec\n",
      "Epoch [1633 / 3000]: Train Loss=3.8230, Val Cor=0.4835, Time=0.1078 sec\n",
      "Epoch [1634 / 3000]: Train Loss=3.8340, Val Cor=-0.3357, Time=0.1074 sec\n",
      "Epoch [1635 / 3000]: Train Loss=4.1173, Val Cor=0.4898, Time=0.1076 sec\n",
      "Epoch [1636 / 3000]: Train Loss=4.0509, Val Cor=0.4947, Time=0.1075 sec\n",
      "Epoch [1637 / 3000]: Train Loss=3.8609, Val Cor=0.4640, Time=0.1078 sec\n",
      "Epoch [1638 / 3000]: Train Loss=3.8897, Val Cor=0.4690, Time=0.1078 sec\n",
      "Epoch [1639 / 3000]: Train Loss=4.0947, Val Cor=0.4882, Time=0.1075 sec\n",
      "Epoch [1640 / 3000]: Train Loss=4.0639, Val Cor=0.4906, Time=0.1075 sec\n",
      "Epoch [1641 / 3000]: Train Loss=3.7876, Val Cor=0.4500, Time=0.1076 sec\n",
      "Epoch [1642 / 3000]: Train Loss=3.8281, Val Cor=-0.2733, Time=0.1074 sec\n",
      "Epoch [1643 / 3000]: Train Loss=4.0007, Val Cor=0.4687, Time=0.1075 sec\n",
      "Epoch [1644 / 3000]: Train Loss=3.8526, Val Cor=0.4817, Time=0.1075 sec\n",
      "Epoch [1645 / 3000]: Train Loss=3.8617, Val Cor=0.4566, Time=0.1076 sec\n",
      "Epoch [1646 / 3000]: Train Loss=3.8266, Val Cor=0.4863, Time=0.1075 sec\n",
      "Epoch [1647 / 3000]: Train Loss=3.7604, Val Cor=0.0891, Time=0.1077 sec\n",
      "Epoch [1648 / 3000]: Train Loss=3.8675, Val Cor=0.4886, Time=0.1075 sec\n",
      "Epoch [1649 / 3000]: Train Loss=3.8612, Val Cor=0.4715, Time=0.1075 sec\n",
      "Epoch [1650 / 3000]: Train Loss=3.7609, Val Cor=0.1956, Time=0.1075 sec\n",
      "Epoch [1651 / 3000]: Train Loss=3.9614, Val Cor=0.4107, Time=0.1078 sec\n",
      "Epoch [1652 / 3000]: Train Loss=3.8462, Val Cor=0.4765, Time=0.1075 sec\n",
      "Epoch [1653 / 3000]: Train Loss=3.9032, Val Cor=0.5109, Time=0.1075 sec\n",
      "Epoch [1654 / 3000]: Train Loss=3.8231, Val Cor=0.4942, Time=0.1076 sec\n",
      "Epoch [1655 / 3000]: Train Loss=3.8665, Val Cor=0.4844, Time=0.1074 sec\n",
      "Epoch [1656 / 3000]: Train Loss=3.7908, Val Cor=0.4890, Time=0.1075 sec\n",
      "Epoch [1657 / 3000]: Train Loss=3.8053, Val Cor=0.2949, Time=0.1074 sec\n",
      "Epoch [1658 / 3000]: Train Loss=3.7831, Val Cor=0.2868, Time=0.1076 sec\n",
      "Epoch [1659 / 3000]: Train Loss=3.8460, Val Cor=-0.3923, Time=0.1076 sec\n",
      "Epoch [1660 / 3000]: Train Loss=3.9390, Val Cor=0.4842, Time=0.1075 sec\n",
      "Epoch [1661 / 3000]: Train Loss=3.9123, Val Cor=-0.1590, Time=0.1077 sec\n",
      "Epoch [1662 / 3000]: Train Loss=3.8280, Val Cor=0.4850, Time=0.1076 sec\n",
      "Epoch [1663 / 3000]: Train Loss=3.5970, Val Cor=0.5005, Time=0.1077 sec\n",
      "Epoch [1664 / 3000]: Train Loss=4.1283, Val Cor=-0.1479, Time=0.1076 sec\n",
      "Epoch [1665 / 3000]: Train Loss=4.5031, Val Cor=0.4140, Time=0.1077 sec\n",
      "Epoch [1666 / 3000]: Train Loss=4.2012, Val Cor=0.4973, Time=0.1075 sec\n",
      "Epoch [1667 / 3000]: Train Loss=4.0807, Val Cor=-0.1408, Time=0.1075 sec\n",
      "Epoch [1668 / 3000]: Train Loss=3.9341, Val Cor=0.5037, Time=0.1077 sec\n",
      "Epoch [1669 / 3000]: Train Loss=3.9388, Val Cor=0.4960, Time=0.1076 sec\n",
      "Epoch [1670 / 3000]: Train Loss=3.7717, Val Cor=0.4921, Time=0.1077 sec\n",
      "Epoch [1671 / 3000]: Train Loss=3.6097, Val Cor=0.4961, Time=0.1075 sec\n",
      "Epoch [1672 / 3000]: Train Loss=3.8960, Val Cor=-0.4060, Time=0.1076 sec\n",
      "Epoch [1673 / 3000]: Train Loss=4.0103, Val Cor=-0.2914, Time=0.1076 sec\n",
      "Epoch [1674 / 3000]: Train Loss=3.9614, Val Cor=-0.2907, Time=0.1075 sec\n",
      "Epoch [1675 / 3000]: Train Loss=3.8544, Val Cor=0.3126, Time=0.1077 sec\n",
      "Epoch [1676 / 3000]: Train Loss=4.0736, Val Cor=0.4895, Time=0.1074 sec\n",
      "Epoch [1677 / 3000]: Train Loss=4.0015, Val Cor=0.4614, Time=0.1078 sec\n",
      "Epoch [1678 / 3000]: Train Loss=3.9825, Val Cor=0.2620, Time=0.1075 sec\n",
      "Epoch [1679 / 3000]: Train Loss=4.0200, Val Cor=-0.1542, Time=0.1075 sec\n",
      "Epoch [1680 / 3000]: Train Loss=3.9794, Val Cor=0.5083, Time=0.1074 sec\n",
      "Epoch [1681 / 3000]: Train Loss=3.8909, Val Cor=0.4857, Time=0.1074 sec\n",
      "Epoch [1682 / 3000]: Train Loss=3.7628, Val Cor=0.5036, Time=0.1075 sec\n",
      "Epoch [1683 / 3000]: Train Loss=3.8141, Val Cor=0.5076, Time=0.1076 sec\n",
      "Epoch [1684 / 3000]: Train Loss=3.6691, Val Cor=0.4205, Time=0.1076 sec\n",
      "Epoch [1685 / 3000]: Train Loss=3.8065, Val Cor=0.4733, Time=0.1075 sec\n",
      "Epoch [1686 / 3000]: Train Loss=3.8730, Val Cor=-0.3588, Time=0.1074 sec\n",
      "Epoch [1687 / 3000]: Train Loss=3.9335, Val Cor=0.5006, Time=0.1075 sec\n",
      "Epoch [1688 / 3000]: Train Loss=3.6579, Val Cor=0.5107, Time=0.1074 sec\n",
      "Epoch [1689 / 3000]: Train Loss=3.8290, Val Cor=-0.2882, Time=0.1077 sec\n",
      "Epoch [1690 / 3000]: Train Loss=3.8971, Val Cor=0.5112, Time=0.1074 sec\n",
      "Epoch [1691 / 3000]: Train Loss=3.7331, Val Cor=-0.1317, Time=0.1078 sec\n",
      "Epoch [1692 / 3000]: Train Loss=4.8809, Val Cor=-0.3434, Time=0.1075 sec\n",
      "Epoch [1693 / 3000]: Train Loss=3.9978, Val Cor=0.5021, Time=0.1077 sec\n",
      "Epoch [1694 / 3000]: Train Loss=3.9535, Val Cor=0.4627, Time=0.1075 sec\n",
      "Epoch [1695 / 3000]: Train Loss=3.9394, Val Cor=0.5043, Time=0.1077 sec\n",
      "Epoch [1696 / 3000]: Train Loss=3.9101, Val Cor=-0.4468, Time=0.1076 sec\n",
      "Epoch [1697 / 3000]: Train Loss=4.0484, Val Cor=0.4465, Time=0.1077 sec\n",
      "Epoch [1698 / 3000]: Train Loss=3.8767, Val Cor=0.4980, Time=0.1075 sec\n",
      "Epoch [1699 / 3000]: Train Loss=3.9575, Val Cor=0.5016, Time=0.1075 sec\n",
      "Epoch [1700 / 3000]: Train Loss=3.6676, Val Cor=0.5044, Time=0.1073 sec\n",
      "Epoch [1701 / 3000]: Train Loss=3.6511, Val Cor=-0.3733, Time=0.1076 sec\n",
      "Epoch [1702 / 3000]: Train Loss=3.9214, Val Cor=0.4835, Time=0.1076 sec\n",
      "Epoch [1703 / 3000]: Train Loss=3.7404, Val Cor=0.5060, Time=0.1077 sec\n",
      "Epoch [1704 / 3000]: Train Loss=3.7567, Val Cor=-0.4473, Time=0.1077 sec\n",
      "Epoch [1705 / 3000]: Train Loss=4.0262, Val Cor=0.5004, Time=0.1078 sec\n",
      "Epoch [1706 / 3000]: Train Loss=3.8824, Val Cor=0.5060, Time=0.1076 sec\n",
      "Epoch [1707 / 3000]: Train Loss=3.5915, Val Cor=0.4763, Time=0.1077 sec\n",
      "Epoch [1708 / 3000]: Train Loss=3.7594, Val Cor=0.5081, Time=0.1077 sec\n",
      "Epoch [1709 / 3000]: Train Loss=3.7129, Val Cor=0.2309, Time=0.1077 sec\n",
      "Epoch [1710 / 3000]: Train Loss=3.6891, Val Cor=0.5041, Time=0.1075 sec\n",
      "Epoch [1711 / 3000]: Train Loss=4.0532, Val Cor=0.4922, Time=0.1078 sec\n",
      "Epoch [1712 / 3000]: Train Loss=4.0410, Val Cor=-0.1189, Time=0.1076 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1713 / 3000]: Train Loss=4.0005, Val Cor=-0.3289, Time=0.1077 sec\n",
      "Epoch [1714 / 3000]: Train Loss=4.1360, Val Cor=0.4326, Time=0.1074 sec\n",
      "Epoch [1715 / 3000]: Train Loss=3.9774, Val Cor=0.4629, Time=0.1084 sec\n",
      "Epoch [1716 / 3000]: Train Loss=3.9879, Val Cor=0.4877, Time=0.1079 sec\n",
      "Epoch [1717 / 3000]: Train Loss=3.6835, Val Cor=0.4957, Time=0.1080 sec\n",
      "Epoch [1718 / 3000]: Train Loss=3.8397, Val Cor=0.4907, Time=0.1080 sec\n",
      "Epoch [1719 / 3000]: Train Loss=4.0054, Val Cor=0.5055, Time=0.1078 sec\n",
      "Epoch [1720 / 3000]: Train Loss=3.7243, Val Cor=0.4804, Time=0.1079 sec\n",
      "Epoch [1721 / 3000]: Train Loss=3.6328, Val Cor=0.4913, Time=0.1077 sec\n",
      "Epoch [1722 / 3000]: Train Loss=3.6010, Val Cor=0.5046, Time=0.1078 sec\n",
      "Epoch [1723 / 3000]: Train Loss=6.6314, Val Cor=-0.2737, Time=0.1078 sec\n",
      "Epoch [1724 / 3000]: Train Loss=5.9201, Val Cor=0.2684, Time=0.1079 sec\n",
      "Epoch [1725 / 3000]: Train Loss=5.9188, Val Cor=0.3271, Time=0.1079 sec\n",
      "Epoch [1726 / 3000]: Train Loss=5.8492, Val Cor=0.3131, Time=0.1080 sec\n",
      "Epoch [1727 / 3000]: Train Loss=5.8313, Val Cor=0.1880, Time=0.1078 sec\n",
      "Epoch [1728 / 3000]: Train Loss=5.8023, Val Cor=0.3735, Time=0.1076 sec\n",
      "Epoch [1729 / 3000]: Train Loss=5.8257, Val Cor=0.3815, Time=0.1076 sec\n",
      "Epoch [1730 / 3000]: Train Loss=5.7855, Val Cor=0.3866, Time=0.1074 sec\n",
      "Epoch [1731 / 3000]: Train Loss=5.7263, Val Cor=0.3877, Time=0.1077 sec\n",
      "Epoch [1732 / 3000]: Train Loss=5.8287, Val Cor=0.4098, Time=0.1075 sec\n",
      "Epoch [1733 / 3000]: Train Loss=5.8786, Val Cor=0.4131, Time=0.1074 sec\n",
      "Epoch [1734 / 3000]: Train Loss=5.7931, Val Cor=0.2693, Time=0.1076 sec\n",
      "Epoch [1735 / 3000]: Train Loss=5.7765, Val Cor=0.4264, Time=0.1077 sec\n",
      "Epoch [1736 / 3000]: Train Loss=5.8262, Val Cor=0.4010, Time=0.1076 sec\n",
      "Epoch [1737 / 3000]: Train Loss=5.7441, Val Cor=0.3203, Time=0.1075 sec\n",
      "Epoch [1738 / 3000]: Train Loss=5.8348, Val Cor=-0.4732, Time=0.1073 sec\n",
      "Epoch [1739 / 3000]: Train Loss=5.7218, Val Cor=0.4431, Time=0.1076 sec\n",
      "Epoch [1740 / 3000]: Train Loss=5.8269, Val Cor=0.2617, Time=0.1074 sec\n",
      "Epoch [1741 / 3000]: Train Loss=5.8316, Val Cor=-0.4646, Time=0.1077 sec\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RNNRegressor().to(device)\n",
    "model, best_ckpt = train_model_ESM(model, train_dataset, val_dataset, epochs=3000, batch_size=32, lr=5e-4, patience=300, device=device)\n",
    "model.load_state_dict(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5204107282163262"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_esm(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 992/992 [00:00<00:00, 1343.92it/s]\n",
      "Loading esm embeddings: 100%|██████████| 248/248 [00:00<00:00, 1409.90it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences = df_train['sequence']\n",
    "fitness_list = df_train['DMS_score'].tolist()\n",
    "\n",
    "seq_train, seq_val, fitness_train, fitness_val = train_test_split(sequences, fitness_list, test_size=0.2, random_state=16)\n",
    "\n",
    "train_seq2name = train_seq2name = {seq: f'seq_{i}' for i, seq in enumerate(sequences)}\n",
    "fitness2idx = {fitness: fitness for idx, fitness in enumerate(fitness_list)}\n",
    "\n",
    "emb_dir = 'esm_embeddings_test'\n",
    "train_dataset = ProteinESMDataset(seq_train, train_seq2name, emb_dir, fitness_train, fitness2idx)\n",
    "val_dataset = ProteinESMDataset(seq_val, train_seq2name, emb_dir, fitness_val, fitness2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 2000]: Train Loss=6.1966, Val Cor=0.3562, Time=0.3995 sec\n",
      "Epoch [2 / 2000]: Train Loss=5.8942, Val Cor=0.3597, Time=0.3979 sec\n",
      "Epoch [3 / 2000]: Train Loss=5.8698, Val Cor=0.2552, Time=0.3992 sec\n",
      "Epoch [4 / 2000]: Train Loss=5.9208, Val Cor=0.3362, Time=0.3986 sec\n",
      "Epoch [5 / 2000]: Train Loss=5.8688, Val Cor=0.3286, Time=0.3986 sec\n",
      "Epoch [6 / 2000]: Train Loss=5.9122, Val Cor=0.3117, Time=0.3992 sec\n",
      "Epoch [7 / 2000]: Train Loss=5.8723, Val Cor=0.3022, Time=0.3989 sec\n",
      "Epoch [8 / 2000]: Train Loss=5.8230, Val Cor=0.3142, Time=0.3984 sec\n",
      "Epoch [9 / 2000]: Train Loss=5.8465, Val Cor=0.3003, Time=0.3996 sec\n",
      "Epoch [10 / 2000]: Train Loss=5.9294, Val Cor=0.2879, Time=0.3987 sec\n",
      "Epoch [11 / 2000]: Train Loss=5.9595, Val Cor=0.2789, Time=0.3990 sec\n",
      "Epoch [12 / 2000]: Train Loss=5.8492, Val Cor=0.2876, Time=0.3992 sec\n",
      "Epoch [13 / 2000]: Train Loss=5.8379, Val Cor=0.3230, Time=0.3984 sec\n",
      "Epoch [14 / 2000]: Train Loss=5.9288, Val Cor=0.2790, Time=0.3995 sec\n",
      "Epoch [15 / 2000]: Train Loss=5.8421, Val Cor=0.2751, Time=0.3979 sec\n",
      "Epoch [16 / 2000]: Train Loss=5.8306, Val Cor=0.2809, Time=0.3999 sec\n",
      "Epoch [17 / 2000]: Train Loss=5.7863, Val Cor=0.2809, Time=0.3991 sec\n",
      "Epoch [18 / 2000]: Train Loss=5.8648, Val Cor=0.2655, Time=0.3997 sec\n",
      "Epoch [19 / 2000]: Train Loss=5.8086, Val Cor=0.2567, Time=0.3994 sec\n",
      "Epoch [20 / 2000]: Train Loss=5.8642, Val Cor=0.3045, Time=0.3991 sec\n",
      "Epoch [21 / 2000]: Train Loss=5.8351, Val Cor=0.2951, Time=0.4013 sec\n",
      "Epoch [22 / 2000]: Train Loss=5.8123, Val Cor=0.2934, Time=0.4014 sec\n",
      "Epoch [23 / 2000]: Train Loss=5.8335, Val Cor=0.2744, Time=0.4003 sec\n",
      "Epoch [24 / 2000]: Train Loss=5.8351, Val Cor=0.2707, Time=0.4003 sec\n",
      "Epoch [25 / 2000]: Train Loss=5.8441, Val Cor=0.2529, Time=0.4000 sec\n",
      "Epoch [26 / 2000]: Train Loss=5.8577, Val Cor=0.2393, Time=0.5630 sec\n",
      "Epoch [27 / 2000]: Train Loss=5.8513, Val Cor=0.2483, Time=0.4008 sec\n",
      "Epoch [28 / 2000]: Train Loss=5.8201, Val Cor=0.2328, Time=0.4008 sec\n",
      "Epoch [29 / 2000]: Train Loss=5.8232, Val Cor=0.1837, Time=0.4016 sec\n",
      "Epoch [30 / 2000]: Train Loss=5.8688, Val Cor=0.2024, Time=0.4011 sec\n",
      "Epoch [31 / 2000]: Train Loss=5.7965, Val Cor=0.2055, Time=0.4013 sec\n",
      "Epoch [32 / 2000]: Train Loss=5.8636, Val Cor=0.2307, Time=0.4013 sec\n",
      "Epoch [33 / 2000]: Train Loss=5.8444, Val Cor=0.2980, Time=0.4004 sec\n",
      "Epoch [34 / 2000]: Train Loss=5.8052, Val Cor=0.2488, Time=0.4012 sec\n",
      "Epoch [35 / 2000]: Train Loss=5.8462, Val Cor=0.2733, Time=0.4005 sec\n",
      "Epoch [36 / 2000]: Train Loss=5.8080, Val Cor=0.2475, Time=0.4007 sec\n",
      "Epoch [37 / 2000]: Train Loss=5.8381, Val Cor=0.2444, Time=0.4014 sec\n",
      "Epoch [38 / 2000]: Train Loss=5.8016, Val Cor=0.2362, Time=0.4010 sec\n",
      "Epoch [39 / 2000]: Train Loss=5.8960, Val Cor=0.2565, Time=0.4021 sec\n",
      "Epoch [40 / 2000]: Train Loss=5.8403, Val Cor=0.2427, Time=0.4008 sec\n",
      "Epoch [41 / 2000]: Train Loss=5.8626, Val Cor=0.2572, Time=0.4008 sec\n",
      "Epoch [42 / 2000]: Train Loss=5.8546, Val Cor=0.2564, Time=0.4019 sec\n",
      "Epoch [43 / 2000]: Train Loss=5.8035, Val Cor=0.2682, Time=0.4021 sec\n",
      "Epoch [44 / 2000]: Train Loss=5.8090, Val Cor=0.2451, Time=0.4009 sec\n",
      "Epoch [45 / 2000]: Train Loss=5.8270, Val Cor=0.2931, Time=0.4012 sec\n",
      "Epoch [46 / 2000]: Train Loss=5.8072, Val Cor=0.3016, Time=0.4002 sec\n",
      "Epoch [47 / 2000]: Train Loss=5.8126, Val Cor=0.2805, Time=0.4016 sec\n",
      "Epoch [48 / 2000]: Train Loss=5.7924, Val Cor=0.2478, Time=0.4007 sec\n",
      "Epoch [49 / 2000]: Train Loss=5.8208, Val Cor=0.2376, Time=0.4008 sec\n",
      "Epoch [50 / 2000]: Train Loss=5.8114, Val Cor=0.2283, Time=0.4005 sec\n",
      "Epoch [51 / 2000]: Train Loss=5.7984, Val Cor=0.2259, Time=0.5630 sec\n",
      "Epoch [52 / 2000]: Train Loss=5.8029, Val Cor=0.2405, Time=0.4024 sec\n",
      "Epoch [53 / 2000]: Train Loss=5.7817, Val Cor=0.2621, Time=0.4006 sec\n",
      "Epoch [54 / 2000]: Train Loss=5.8149, Val Cor=0.2522, Time=0.4005 sec\n",
      "Epoch [55 / 2000]: Train Loss=5.8420, Val Cor=0.2228, Time=0.4015 sec\n",
      "Epoch [56 / 2000]: Train Loss=5.8278, Val Cor=0.2154, Time=0.4004 sec\n",
      "Epoch [57 / 2000]: Train Loss=5.8637, Val Cor=0.2014, Time=0.4009 sec\n",
      "Epoch [58 / 2000]: Train Loss=5.7983, Val Cor=0.2137, Time=0.4010 sec\n",
      "Epoch [59 / 2000]: Train Loss=5.8138, Val Cor=0.1959, Time=0.4006 sec\n",
      "Epoch [60 / 2000]: Train Loss=5.8268, Val Cor=0.1935, Time=0.4012 sec\n",
      "Epoch [61 / 2000]: Train Loss=5.8344, Val Cor=0.2487, Time=0.4012 sec\n",
      "Epoch [62 / 2000]: Train Loss=5.8045, Val Cor=0.2317, Time=0.4010 sec\n",
      "Epoch [63 / 2000]: Train Loss=5.7995, Val Cor=0.2027, Time=0.4011 sec\n",
      "Epoch [64 / 2000]: Train Loss=5.8398, Val Cor=0.2273, Time=0.4019 sec\n",
      "Epoch [65 / 2000]: Train Loss=5.8409, Val Cor=0.2361, Time=0.4013 sec\n",
      "Epoch [66 / 2000]: Train Loss=5.8158, Val Cor=0.2484, Time=0.4028 sec\n",
      "Epoch [67 / 2000]: Train Loss=5.8013, Val Cor=0.2284, Time=0.4008 sec\n",
      "Epoch [68 / 2000]: Train Loss=5.8587, Val Cor=0.2337, Time=0.4009 sec\n",
      "Epoch [69 / 2000]: Train Loss=5.8001, Val Cor=0.2614, Time=0.4007 sec\n",
      "Epoch [70 / 2000]: Train Loss=5.7928, Val Cor=0.2958, Time=0.4013 sec\n",
      "Epoch [71 / 2000]: Train Loss=5.7884, Val Cor=0.3001, Time=0.4015 sec\n",
      "Epoch [72 / 2000]: Train Loss=5.7843, Val Cor=0.2934, Time=0.4010 sec\n",
      "Epoch [73 / 2000]: Train Loss=5.8104, Val Cor=0.2759, Time=0.4021 sec\n",
      "Epoch [74 / 2000]: Train Loss=5.8069, Val Cor=0.3137, Time=0.4003 sec\n",
      "Epoch [75 / 2000]: Train Loss=5.8025, Val Cor=0.3230, Time=0.4023 sec\n",
      "Epoch [76 / 2000]: Train Loss=5.8347, Val Cor=0.2860, Time=0.4012 sec\n",
      "Epoch [77 / 2000]: Train Loss=5.7640, Val Cor=0.2726, Time=0.4009 sec\n",
      "Epoch [78 / 2000]: Train Loss=5.7943, Val Cor=0.2903, Time=0.4013 sec\n",
      "Epoch [79 / 2000]: Train Loss=5.8047, Val Cor=0.2935, Time=0.4010 sec\n",
      "Epoch [80 / 2000]: Train Loss=5.8051, Val Cor=0.3093, Time=0.4009 sec\n",
      "Epoch [81 / 2000]: Train Loss=5.8070, Val Cor=0.3219, Time=0.5627 sec\n",
      "Epoch [82 / 2000]: Train Loss=5.8351, Val Cor=0.3217, Time=0.4018 sec\n",
      "Epoch [83 / 2000]: Train Loss=5.8210, Val Cor=0.3204, Time=0.4019 sec\n",
      "Epoch [84 / 2000]: Train Loss=5.8118, Val Cor=0.2845, Time=0.4006 sec\n",
      "Epoch [85 / 2000]: Train Loss=5.8437, Val Cor=0.3461, Time=0.4012 sec\n",
      "Epoch [86 / 2000]: Train Loss=5.8655, Val Cor=0.3526, Time=0.4013 sec\n",
      "Epoch [87 / 2000]: Train Loss=5.7775, Val Cor=0.3268, Time=0.4008 sec\n",
      "Epoch [88 / 2000]: Train Loss=5.7855, Val Cor=0.3060, Time=0.4012 sec\n",
      "Epoch [89 / 2000]: Train Loss=5.8142, Val Cor=0.3327, Time=0.4004 sec\n",
      "Epoch [90 / 2000]: Train Loss=5.8118, Val Cor=0.3110, Time=0.4032 sec\n",
      "Epoch [91 / 2000]: Train Loss=5.7953, Val Cor=0.3184, Time=0.4026 sec\n",
      "Epoch [92 / 2000]: Train Loss=5.7931, Val Cor=0.2979, Time=0.4006 sec\n",
      "Epoch [93 / 2000]: Train Loss=5.7914, Val Cor=0.3052, Time=0.4004 sec\n",
      "Epoch [94 / 2000]: Train Loss=5.7828, Val Cor=0.3256, Time=0.4009 sec\n",
      "Epoch [95 / 2000]: Train Loss=5.8104, Val Cor=0.2972, Time=0.4007 sec\n",
      "Epoch [96 / 2000]: Train Loss=5.8278, Val Cor=0.3474, Time=0.4018 sec\n",
      "Epoch [97 / 2000]: Train Loss=5.8075, Val Cor=0.3403, Time=0.4005 sec\n",
      "Epoch [98 / 2000]: Train Loss=5.8001, Val Cor=0.3388, Time=0.4011 sec\n",
      "Epoch [99 / 2000]: Train Loss=5.8131, Val Cor=0.3365, Time=0.4017 sec\n",
      "Epoch [100 / 2000]: Train Loss=5.7908, Val Cor=0.3427, Time=0.4008 sec\n",
      "Epoch [101 / 2000]: Train Loss=5.8143, Val Cor=0.2617, Time=0.4010 sec\n",
      "Epoch [102 / 2000]: Train Loss=5.8183, Val Cor=0.3000, Time=0.4014 sec\n",
      "Epoch [103 / 2000]: Train Loss=5.7821, Val Cor=0.3274, Time=0.4004 sec\n",
      "Epoch [104 / 2000]: Train Loss=5.7951, Val Cor=0.3067, Time=0.4013 sec\n",
      "Epoch [105 / 2000]: Train Loss=5.8028, Val Cor=0.2592, Time=0.4001 sec\n",
      "Epoch [106 / 2000]: Train Loss=5.8540, Val Cor=0.2999, Time=0.4009 sec\n",
      "Epoch [107 / 2000]: Train Loss=5.7896, Val Cor=0.3153, Time=0.4011 sec\n",
      "Epoch [108 / 2000]: Train Loss=5.8156, Val Cor=0.3148, Time=0.5632 sec\n",
      "Epoch [109 / 2000]: Train Loss=5.7877, Val Cor=0.3140, Time=0.4023 sec\n",
      "Epoch [110 / 2000]: Train Loss=5.8339, Val Cor=0.3207, Time=0.4011 sec\n",
      "Epoch [111 / 2000]: Train Loss=5.8054, Val Cor=0.3062, Time=0.4013 sec\n",
      "Epoch [112 / 2000]: Train Loss=5.8031, Val Cor=0.2781, Time=0.4011 sec\n",
      "Epoch [113 / 2000]: Train Loss=5.8067, Val Cor=0.2330, Time=0.4007 sec\n",
      "Epoch [114 / 2000]: Train Loss=5.8097, Val Cor=0.2342, Time=0.4013 sec\n",
      "Epoch [115 / 2000]: Train Loss=5.8174, Val Cor=0.2792, Time=0.4017 sec\n",
      "Epoch [116 / 2000]: Train Loss=5.7860, Val Cor=0.2325, Time=0.4031 sec\n",
      "Epoch [117 / 2000]: Train Loss=5.7934, Val Cor=0.2501, Time=0.4013 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [118 / 2000]: Train Loss=5.8113, Val Cor=0.2282, Time=0.4005 sec\n",
      "Epoch [119 / 2000]: Train Loss=5.7970, Val Cor=0.2210, Time=0.4009 sec\n",
      "Epoch [120 / 2000]: Train Loss=5.8008, Val Cor=0.2166, Time=0.4008 sec\n",
      "Epoch [121 / 2000]: Train Loss=5.7864, Val Cor=0.2510, Time=0.4010 sec\n",
      "Epoch [122 / 2000]: Train Loss=5.8390, Val Cor=0.2457, Time=0.4012 sec\n",
      "Epoch [123 / 2000]: Train Loss=5.8239, Val Cor=0.2247, Time=0.4008 sec\n",
      "Epoch [124 / 2000]: Train Loss=5.8186, Val Cor=0.2043, Time=0.4011 sec\n",
      "Epoch [125 / 2000]: Train Loss=5.8004, Val Cor=0.1832, Time=0.4019 sec\n",
      "Epoch [126 / 2000]: Train Loss=5.7803, Val Cor=0.1923, Time=0.4004 sec\n",
      "Epoch [127 / 2000]: Train Loss=5.7975, Val Cor=0.2423, Time=0.4017 sec\n",
      "Epoch [128 / 2000]: Train Loss=5.8044, Val Cor=0.2453, Time=0.4006 sec\n",
      "Epoch [129 / 2000]: Train Loss=5.8152, Val Cor=0.2594, Time=0.4006 sec\n",
      "Epoch [130 / 2000]: Train Loss=5.8011, Val Cor=0.2647, Time=0.4010 sec\n",
      "Epoch [131 / 2000]: Train Loss=5.8252, Val Cor=0.2847, Time=0.4012 sec\n",
      "Epoch [132 / 2000]: Train Loss=5.7957, Val Cor=0.3316, Time=0.4013 sec\n",
      "Epoch [133 / 2000]: Train Loss=5.7951, Val Cor=0.2458, Time=0.5618 sec\n",
      "Epoch [134 / 2000]: Train Loss=5.7999, Val Cor=0.3096, Time=0.4018 sec\n",
      "Epoch [135 / 2000]: Train Loss=5.7769, Val Cor=0.2758, Time=0.4000 sec\n",
      "Epoch [136 / 2000]: Train Loss=5.8143, Val Cor=0.3103, Time=0.4007 sec\n",
      "Epoch [137 / 2000]: Train Loss=5.8290, Val Cor=0.3079, Time=0.4013 sec\n",
      "Epoch [138 / 2000]: Train Loss=5.8308, Val Cor=0.2979, Time=0.4003 sec\n",
      "Epoch [139 / 2000]: Train Loss=5.8172, Val Cor=0.3167, Time=0.4002 sec\n",
      "Epoch [140 / 2000]: Train Loss=5.8178, Val Cor=0.3580, Time=0.4018 sec\n",
      "Epoch [141 / 2000]: Train Loss=5.8034, Val Cor=0.3518, Time=0.4014 sec\n",
      "Epoch [142 / 2000]: Train Loss=5.7963, Val Cor=0.3445, Time=0.4014 sec\n",
      "Epoch [143 / 2000]: Train Loss=5.8015, Val Cor=0.3243, Time=0.4007 sec\n",
      "Epoch [144 / 2000]: Train Loss=5.8008, Val Cor=0.3433, Time=0.4007 sec\n",
      "Epoch [145 / 2000]: Train Loss=5.8352, Val Cor=0.2865, Time=0.4010 sec\n",
      "Epoch [146 / 2000]: Train Loss=5.8023, Val Cor=0.2888, Time=0.4006 sec\n",
      "Epoch [147 / 2000]: Train Loss=5.7646, Val Cor=0.2785, Time=0.4012 sec\n",
      "Epoch [148 / 2000]: Train Loss=5.7837, Val Cor=0.2753, Time=0.4017 sec\n",
      "Epoch [149 / 2000]: Train Loss=5.7976, Val Cor=0.3171, Time=0.4011 sec\n",
      "Epoch [150 / 2000]: Train Loss=5.7913, Val Cor=0.3638, Time=0.4010 sec\n",
      "Epoch [151 / 2000]: Train Loss=5.7840, Val Cor=0.3089, Time=0.4003 sec\n",
      "Epoch [152 / 2000]: Train Loss=5.8029, Val Cor=0.3390, Time=0.4011 sec\n",
      "Epoch [153 / 2000]: Train Loss=5.8055, Val Cor=0.2446, Time=0.4018 sec\n",
      "Epoch [154 / 2000]: Train Loss=5.8159, Val Cor=0.2763, Time=0.4016 sec\n",
      "Epoch [155 / 2000]: Train Loss=5.8205, Val Cor=0.2822, Time=0.4000 sec\n",
      "Epoch [156 / 2000]: Train Loss=5.7867, Val Cor=0.3105, Time=0.4003 sec\n",
      "Epoch [157 / 2000]: Train Loss=5.7802, Val Cor=0.2806, Time=0.4003 sec\n",
      "Epoch [158 / 2000]: Train Loss=5.7886, Val Cor=0.2912, Time=0.4003 sec\n",
      "Epoch [159 / 2000]: Train Loss=5.8030, Val Cor=0.2862, Time=0.3998 sec\n",
      "Epoch [160 / 2000]: Train Loss=5.8338, Val Cor=0.2450, Time=0.4006 sec\n",
      "Epoch [161 / 2000]: Train Loss=5.8104, Val Cor=0.1716, Time=0.3998 sec\n",
      "Epoch [162 / 2000]: Train Loss=5.7970, Val Cor=0.2338, Time=0.3999 sec\n",
      "Epoch [163 / 2000]: Train Loss=5.8089, Val Cor=0.2113, Time=0.3999 sec\n",
      "Epoch [164 / 2000]: Train Loss=5.8037, Val Cor=0.1337, Time=0.5630 sec\n",
      "Epoch [165 / 2000]: Train Loss=5.8032, Val Cor=0.1818, Time=0.4025 sec\n",
      "Epoch [166 / 2000]: Train Loss=5.7938, Val Cor=0.2043, Time=0.4007 sec\n",
      "Epoch [167 / 2000]: Train Loss=5.8022, Val Cor=0.2129, Time=0.4004 sec\n",
      "Epoch [168 / 2000]: Train Loss=5.8026, Val Cor=0.1879, Time=0.4012 sec\n",
      "Epoch [169 / 2000]: Train Loss=5.7938, Val Cor=0.2160, Time=0.4016 sec\n",
      "Epoch [170 / 2000]: Train Loss=5.7886, Val Cor=0.1149, Time=0.4009 sec\n",
      "Epoch [171 / 2000]: Train Loss=5.8300, Val Cor=0.2764, Time=0.4004 sec\n",
      "Epoch [172 / 2000]: Train Loss=5.7760, Val Cor=0.2612, Time=0.4005 sec\n",
      "Epoch [173 / 2000]: Train Loss=5.7901, Val Cor=0.2778, Time=0.4011 sec\n",
      "Epoch [174 / 2000]: Train Loss=5.8086, Val Cor=0.2922, Time=0.4005 sec\n",
      "Epoch [175 / 2000]: Train Loss=5.8076, Val Cor=0.2877, Time=0.4007 sec\n",
      "Epoch [176 / 2000]: Train Loss=5.8034, Val Cor=0.3009, Time=0.4004 sec\n",
      "Epoch [177 / 2000]: Train Loss=5.7975, Val Cor=0.3521, Time=0.4002 sec\n",
      "Epoch [178 / 2000]: Train Loss=5.7863, Val Cor=0.2671, Time=0.4007 sec\n",
      "Epoch [179 / 2000]: Train Loss=5.7861, Val Cor=0.1999, Time=0.4003 sec\n",
      "Epoch [180 / 2000]: Train Loss=5.8114, Val Cor=0.2743, Time=0.4013 sec\n",
      "Epoch [181 / 2000]: Train Loss=5.7988, Val Cor=0.3454, Time=0.4005 sec\n",
      "Epoch [182 / 2000]: Train Loss=5.8130, Val Cor=0.3673, Time=0.4007 sec\n",
      "Epoch [183 / 2000]: Train Loss=5.8364, Val Cor=0.2563, Time=0.4011 sec\n",
      "Epoch [184 / 2000]: Train Loss=5.7954, Val Cor=0.2932, Time=0.4011 sec\n",
      "Epoch [185 / 2000]: Train Loss=5.7887, Val Cor=0.2937, Time=0.4004 sec\n",
      "Epoch [186 / 2000]: Train Loss=5.8084, Val Cor=0.2802, Time=0.4011 sec\n",
      "Epoch [187 / 2000]: Train Loss=5.7885, Val Cor=0.1484, Time=0.3998 sec\n",
      "Epoch [188 / 2000]: Train Loss=5.8019, Val Cor=0.2575, Time=0.4006 sec\n",
      "Epoch [189 / 2000]: Train Loss=5.7934, Val Cor=0.1847, Time=0.4012 sec\n",
      "Epoch [190 / 2000]: Train Loss=5.8094, Val Cor=0.2209, Time=0.4040 sec\n",
      "Epoch [191 / 2000]: Train Loss=5.7995, Val Cor=0.2988, Time=0.4015 sec\n",
      "Epoch [192 / 2000]: Train Loss=5.7897, Val Cor=0.3052, Time=0.5634 sec\n",
      "Epoch [193 / 2000]: Train Loss=5.7898, Val Cor=0.2598, Time=0.4019 sec\n",
      "Epoch [194 / 2000]: Train Loss=5.7973, Val Cor=0.2415, Time=0.4003 sec\n",
      "Epoch [195 / 2000]: Train Loss=5.7969, Val Cor=0.2450, Time=0.4004 sec\n",
      "Epoch [196 / 2000]: Train Loss=5.8321, Val Cor=0.3386, Time=0.4010 sec\n",
      "Epoch [197 / 2000]: Train Loss=5.8019, Val Cor=0.3102, Time=0.4003 sec\n",
      "Epoch [198 / 2000]: Train Loss=5.8169, Val Cor=0.0437, Time=0.4022 sec\n",
      "Epoch [199 / 2000]: Train Loss=5.7994, Val Cor=0.1776, Time=0.4026 sec\n",
      "Epoch [200 / 2000]: Train Loss=5.7807, Val Cor=0.0740, Time=0.4001 sec\n",
      "Epoch [201 / 2000]: Train Loss=5.8287, Val Cor=0.1860, Time=0.4002 sec\n",
      "Epoch [202 / 2000]: Train Loss=5.7707, Val Cor=0.2422, Time=0.4002 sec\n",
      "Epoch [203 / 2000]: Train Loss=5.7936, Val Cor=0.2494, Time=0.4010 sec\n",
      "Epoch [204 / 2000]: Train Loss=5.7616, Val Cor=0.2024, Time=0.4007 sec\n",
      "Epoch [205 / 2000]: Train Loss=5.7946, Val Cor=0.2128, Time=0.4004 sec\n",
      "Epoch [206 / 2000]: Train Loss=5.7999, Val Cor=0.1725, Time=0.4003 sec\n",
      "Epoch [207 / 2000]: Train Loss=5.7977, Val Cor=0.2160, Time=0.4000 sec\n",
      "Epoch [208 / 2000]: Train Loss=5.7879, Val Cor=0.2163, Time=0.4005 sec\n",
      "Epoch [209 / 2000]: Train Loss=5.8124, Val Cor=0.1715, Time=0.4012 sec\n",
      "Epoch [210 / 2000]: Train Loss=5.7820, Val Cor=0.2765, Time=0.4012 sec\n",
      "Epoch [211 / 2000]: Train Loss=5.7868, Val Cor=0.2292, Time=0.3999 sec\n",
      "Epoch [212 / 2000]: Train Loss=5.7891, Val Cor=0.2314, Time=0.4011 sec\n",
      "Epoch [213 / 2000]: Train Loss=5.7737, Val Cor=0.2127, Time=0.3998 sec\n",
      "Epoch [214 / 2000]: Train Loss=5.8126, Val Cor=0.1498, Time=0.4012 sec\n",
      "Epoch [215 / 2000]: Train Loss=5.7967, Val Cor=0.1091, Time=0.4012 sec\n",
      "Epoch [216 / 2000]: Train Loss=5.7977, Val Cor=0.1422, Time=0.4001 sec\n",
      "Epoch [217 / 2000]: Train Loss=5.7903, Val Cor=0.1492, Time=0.5646 sec\n",
      "Epoch [218 / 2000]: Train Loss=5.7825, Val Cor=0.1465, Time=0.4022 sec\n",
      "Epoch [219 / 2000]: Train Loss=5.8068, Val Cor=0.0161, Time=0.4006 sec\n",
      "Epoch [220 / 2000]: Train Loss=5.7753, Val Cor=0.0806, Time=0.4005 sec\n",
      "Epoch [221 / 2000]: Train Loss=5.7787, Val Cor=0.1146, Time=0.3997 sec\n",
      "Epoch [222 / 2000]: Train Loss=5.8147, Val Cor=-0.0313, Time=0.4009 sec\n",
      "Epoch [223 / 2000]: Train Loss=5.7802, Val Cor=0.0802, Time=0.3996 sec\n",
      "Epoch [224 / 2000]: Train Loss=5.7978, Val Cor=0.0872, Time=0.3999 sec\n",
      "Epoch [225 / 2000]: Train Loss=5.8337, Val Cor=0.0743, Time=0.3992 sec\n",
      "Epoch [226 / 2000]: Train Loss=5.7789, Val Cor=0.0747, Time=0.3998 sec\n",
      "Epoch [227 / 2000]: Train Loss=5.8190, Val Cor=0.0783, Time=0.4005 sec\n",
      "Epoch [228 / 2000]: Train Loss=5.8117, Val Cor=0.1157, Time=0.4005 sec\n",
      "Epoch [229 / 2000]: Train Loss=5.7914, Val Cor=0.1133, Time=0.4019 sec\n",
      "Epoch [230 / 2000]: Train Loss=5.8066, Val Cor=-0.0601, Time=0.3999 sec\n",
      "Epoch [231 / 2000]: Train Loss=5.7748, Val Cor=-0.0521, Time=0.3999 sec\n",
      "Epoch [232 / 2000]: Train Loss=5.7864, Val Cor=-0.0437, Time=0.4003 sec\n",
      "Epoch [233 / 2000]: Train Loss=5.8047, Val Cor=0.0943, Time=0.3998 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [234 / 2000]: Train Loss=5.7666, Val Cor=-0.0355, Time=0.4008 sec\n",
      "Epoch [235 / 2000]: Train Loss=5.7906, Val Cor=0.1785, Time=0.4009 sec\n",
      "Epoch [236 / 2000]: Train Loss=5.7891, Val Cor=-0.0629, Time=0.3994 sec\n",
      "Epoch [237 / 2000]: Train Loss=5.8043, Val Cor=-0.0644, Time=0.4000 sec\n",
      "Epoch [238 / 2000]: Train Loss=5.7791, Val Cor=0.0915, Time=0.4004 sec\n",
      "Epoch [239 / 2000]: Train Loss=5.8389, Val Cor=0.1350, Time=0.4005 sec\n",
      "Epoch [240 / 2000]: Train Loss=5.8121, Val Cor=-0.0448, Time=0.4017 sec\n",
      "Epoch [241 / 2000]: Train Loss=5.7996, Val Cor=0.0885, Time=0.4006 sec\n",
      "Epoch [242 / 2000]: Train Loss=5.8047, Val Cor=-0.0293, Time=0.4001 sec\n",
      "Epoch [243 / 2000]: Train Loss=5.7735, Val Cor=0.0797, Time=0.4016 sec\n",
      "Epoch [244 / 2000]: Train Loss=5.7954, Val Cor=-0.0331, Time=0.4002 sec\n",
      "Epoch [245 / 2000]: Train Loss=5.7946, Val Cor=0.1655, Time=0.4008 sec\n",
      "Epoch [246 / 2000]: Train Loss=5.8074, Val Cor=-0.0377, Time=0.4008 sec\n",
      "Epoch [247 / 2000]: Train Loss=5.8140, Val Cor=0.0554, Time=0.5616 sec\n",
      "Epoch [248 / 2000]: Train Loss=5.7809, Val Cor=0.1069, Time=0.4014 sec\n",
      "Epoch [249 / 2000]: Train Loss=5.7898, Val Cor=0.1037, Time=0.4008 sec\n",
      "Epoch [250 / 2000]: Train Loss=5.8119, Val Cor=-0.0213, Time=0.4007 sec\n",
      "Epoch [251 / 2000]: Train Loss=5.8386, Val Cor=0.1098, Time=0.4020 sec\n",
      "Epoch [252 / 2000]: Train Loss=5.7992, Val Cor=0.0146, Time=0.4006 sec\n",
      "Epoch [253 / 2000]: Train Loss=5.7932, Val Cor=0.1433, Time=0.4003 sec\n",
      "Epoch [254 / 2000]: Train Loss=5.8136, Val Cor=-0.0477, Time=0.4002 sec\n",
      "Epoch [255 / 2000]: Train Loss=5.7866, Val Cor=0.0565, Time=0.4009 sec\n",
      "Epoch [256 / 2000]: Train Loss=5.7803, Val Cor=-0.0405, Time=0.3998 sec\n",
      "Epoch [257 / 2000]: Train Loss=5.8030, Val Cor=0.2508, Time=0.4001 sec\n",
      "Epoch [258 / 2000]: Train Loss=5.7766, Val Cor=0.2377, Time=0.4012 sec\n",
      "Epoch [259 / 2000]: Train Loss=5.7914, Val Cor=0.0077, Time=0.3996 sec\n",
      "Epoch [260 / 2000]: Train Loss=5.7994, Val Cor=0.1421, Time=0.4015 sec\n",
      "Epoch [261 / 2000]: Train Loss=5.7913, Val Cor=0.2192, Time=0.3994 sec\n",
      "Epoch [262 / 2000]: Train Loss=5.7928, Val Cor=0.1738, Time=0.4006 sec\n",
      "Epoch [263 / 2000]: Train Loss=5.7902, Val Cor=0.1210, Time=0.4010 sec\n",
      "Epoch [264 / 2000]: Train Loss=5.7761, Val Cor=0.0964, Time=0.4019 sec\n",
      "Epoch [265 / 2000]: Train Loss=5.7977, Val Cor=0.0853, Time=0.4015 sec\n",
      "Epoch [266 / 2000]: Train Loss=5.7948, Val Cor=0.0970, Time=0.4009 sec\n",
      "Epoch [267 / 2000]: Train Loss=5.7687, Val Cor=0.1068, Time=0.3997 sec\n",
      "Epoch [268 / 2000]: Train Loss=5.8193, Val Cor=0.1093, Time=0.4008 sec\n",
      "Epoch [269 / 2000]: Train Loss=5.7907, Val Cor=0.0959, Time=0.4006 sec\n",
      "Epoch [270 / 2000]: Train Loss=5.7801, Val Cor=0.0967, Time=0.4010 sec\n",
      "Epoch [271 / 2000]: Train Loss=5.8093, Val Cor=0.1265, Time=0.4000 sec\n",
      "Epoch [272 / 2000]: Train Loss=5.7849, Val Cor=0.1247, Time=0.4000 sec\n",
      "Epoch [273 / 2000]: Train Loss=5.8208, Val Cor=0.1328, Time=0.4010 sec\n",
      "Epoch [274 / 2000]: Train Loss=5.8044, Val Cor=0.2296, Time=0.5634 sec\n",
      "Epoch [275 / 2000]: Train Loss=5.7765, Val Cor=0.2874, Time=0.4002 sec\n",
      "Epoch [276 / 2000]: Train Loss=5.7865, Val Cor=0.2470, Time=0.4003 sec\n",
      "Epoch [277 / 2000]: Train Loss=5.8087, Val Cor=0.1873, Time=0.3994 sec\n",
      "Epoch [278 / 2000]: Train Loss=5.7933, Val Cor=0.1516, Time=0.3998 sec\n",
      "Epoch [279 / 2000]: Train Loss=5.7785, Val Cor=0.2295, Time=0.4000 sec\n",
      "Epoch [280 / 2000]: Train Loss=5.8017, Val Cor=0.1546, Time=0.3999 sec\n",
      "Epoch [281 / 2000]: Train Loss=5.8066, Val Cor=0.0257, Time=0.4004 sec\n",
      "Epoch [282 / 2000]: Train Loss=5.7859, Val Cor=0.1308, Time=0.4003 sec\n",
      "Epoch [283 / 2000]: Train Loss=5.8076, Val Cor=0.0426, Time=0.4000 sec\n",
      "Epoch [284 / 2000]: Train Loss=5.7840, Val Cor=0.1690, Time=0.4006 sec\n",
      "Epoch [285 / 2000]: Train Loss=5.7832, Val Cor=0.1077, Time=0.4001 sec\n",
      "Epoch [286 / 2000]: Train Loss=5.7957, Val Cor=0.0906, Time=0.4001 sec\n",
      "Epoch [287 / 2000]: Train Loss=5.7941, Val Cor=0.0344, Time=0.4001 sec\n",
      "Epoch [288 / 2000]: Train Loss=5.7995, Val Cor=0.0186, Time=0.3998 sec\n",
      "Epoch [289 / 2000]: Train Loss=5.7950, Val Cor=0.0011, Time=0.4007 sec\n",
      "Epoch [290 / 2000]: Train Loss=5.7926, Val Cor=0.0878, Time=0.3990 sec\n",
      "Epoch [291 / 2000]: Train Loss=5.8013, Val Cor=0.1900, Time=0.4001 sec\n",
      "Epoch [292 / 2000]: Train Loss=5.8060, Val Cor=0.0471, Time=0.3992 sec\n",
      "Epoch [293 / 2000]: Train Loss=5.7865, Val Cor=0.0221, Time=0.4001 sec\n",
      "Epoch [294 / 2000]: Train Loss=5.7727, Val Cor=0.0006, Time=0.4002 sec\n",
      "Epoch [295 / 2000]: Train Loss=5.7777, Val Cor=-0.0565, Time=0.4001 sec\n",
      "Epoch [296 / 2000]: Train Loss=5.7975, Val Cor=0.0159, Time=0.3997 sec\n",
      "Epoch [297 / 2000]: Train Loss=5.7925, Val Cor=-0.0427, Time=0.4000 sec\n",
      "Epoch [298 / 2000]: Train Loss=5.7823, Val Cor=-0.0417, Time=0.4000 sec\n",
      "Epoch [299 / 2000]: Train Loss=5.8200, Val Cor=-0.0254, Time=0.5614 sec\n",
      "Epoch [300 / 2000]: Train Loss=5.7986, Val Cor=-0.0406, Time=0.3998 sec\n",
      "Epoch [301 / 2000]: Train Loss=5.7895, Val Cor=-0.0183, Time=0.3996 sec\n",
      "Epoch [302 / 2000]: Train Loss=5.8106, Val Cor=0.0375, Time=0.4003 sec\n",
      "Epoch [303 / 2000]: Train Loss=5.7752, Val Cor=0.0629, Time=0.4002 sec\n",
      "Epoch [304 / 2000]: Train Loss=5.7898, Val Cor=0.0384, Time=0.3998 sec\n",
      "Epoch [305 / 2000]: Train Loss=5.7950, Val Cor=0.1657, Time=0.3997 sec\n",
      "Epoch [306 / 2000]: Train Loss=5.7857, Val Cor=0.0600, Time=0.4001 sec\n",
      "Epoch [307 / 2000]: Train Loss=5.7917, Val Cor=0.1592, Time=0.4001 sec\n",
      "Epoch [308 / 2000]: Train Loss=5.7907, Val Cor=0.0439, Time=0.3993 sec\n",
      "Epoch [309 / 2000]: Train Loss=5.7660, Val Cor=0.0720, Time=0.3998 sec\n",
      "Epoch [310 / 2000]: Train Loss=5.8051, Val Cor=0.1818, Time=0.4005 sec\n",
      "Epoch [311 / 2000]: Train Loss=5.7916, Val Cor=0.1950, Time=0.4023 sec\n",
      "Epoch [312 / 2000]: Train Loss=5.7813, Val Cor=0.1721, Time=0.4012 sec\n",
      "Epoch [313 / 2000]: Train Loss=5.7990, Val Cor=0.1905, Time=0.4000 sec\n",
      "Epoch [314 / 2000]: Train Loss=5.8014, Val Cor=0.1746, Time=0.4013 sec\n",
      "Epoch [315 / 2000]: Train Loss=5.8061, Val Cor=0.0747, Time=0.3999 sec\n",
      "Epoch [316 / 2000]: Train Loss=5.7796, Val Cor=0.0754, Time=0.4021 sec\n",
      "Epoch [317 / 2000]: Train Loss=5.7961, Val Cor=-0.0068, Time=0.4010 sec\n",
      "Epoch [318 / 2000]: Train Loss=5.7921, Val Cor=-0.0200, Time=0.4001 sec\n",
      "Epoch [319 / 2000]: Train Loss=5.8002, Val Cor=-0.0055, Time=0.4004 sec\n",
      "Epoch [320 / 2000]: Train Loss=5.8058, Val Cor=0.0282, Time=0.4002 sec\n",
      "Epoch [321 / 2000]: Train Loss=5.7981, Val Cor=0.0013, Time=0.3991 sec\n",
      "Epoch [322 / 2000]: Train Loss=5.7946, Val Cor=0.0000, Time=0.4010 sec\n",
      "Epoch [323 / 2000]: Train Loss=5.8042, Val Cor=0.0074, Time=0.4008 sec\n",
      "Epoch [324 / 2000]: Train Loss=5.8062, Val Cor=-0.0108, Time=0.3999 sec\n",
      "Epoch [325 / 2000]: Train Loss=5.7810, Val Cor=0.1553, Time=0.4003 sec\n",
      "Epoch [326 / 2000]: Train Loss=5.7860, Val Cor=0.0268, Time=0.4005 sec\n",
      "Epoch [327 / 2000]: Train Loss=5.7812, Val Cor=0.0363, Time=0.4013 sec\n",
      "Epoch [328 / 2000]: Train Loss=5.7816, Val Cor=-0.0234, Time=0.4005 sec\n",
      "Epoch [329 / 2000]: Train Loss=5.7730, Val Cor=-0.0196, Time=0.4000 sec\n",
      "Epoch [330 / 2000]: Train Loss=5.7943, Val Cor=-0.0353, Time=0.5631 sec\n",
      "Epoch [331 / 2000]: Train Loss=5.7987, Val Cor=-0.0290, Time=0.4009 sec\n",
      "Epoch [332 / 2000]: Train Loss=5.7854, Val Cor=-0.0293, Time=0.4008 sec\n",
      "Epoch [333 / 2000]: Train Loss=5.7736, Val Cor=-0.0364, Time=0.3994 sec\n",
      "Epoch [334 / 2000]: Train Loss=5.8136, Val Cor=-0.0083, Time=0.4003 sec\n",
      "Epoch [335 / 2000]: Train Loss=5.7991, Val Cor=-0.0369, Time=0.4006 sec\n",
      "Epoch [336 / 2000]: Train Loss=5.7785, Val Cor=-0.0279, Time=0.3995 sec\n",
      "Epoch [337 / 2000]: Train Loss=5.7926, Val Cor=-0.0454, Time=0.3993 sec\n",
      "Epoch [338 / 2000]: Train Loss=5.7936, Val Cor=-0.0462, Time=0.4012 sec\n",
      "Epoch [339 / 2000]: Train Loss=5.8045, Val Cor=-0.0472, Time=0.4018 sec\n",
      "Epoch [340 / 2000]: Train Loss=5.7998, Val Cor=-0.0132, Time=0.4003 sec\n",
      "Epoch [341 / 2000]: Train Loss=5.7939, Val Cor=-0.0154, Time=0.4006 sec\n",
      "Epoch [342 / 2000]: Train Loss=5.8099, Val Cor=0.1026, Time=0.4013 sec\n",
      "Epoch [343 / 2000]: Train Loss=5.7970, Val Cor=0.0678, Time=0.4007 sec\n",
      "Epoch [344 / 2000]: Train Loss=5.8017, Val Cor=-0.0796, Time=0.4005 sec\n",
      "Epoch [345 / 2000]: Train Loss=5.7932, Val Cor=0.0654, Time=0.4015 sec\n",
      "Epoch [346 / 2000]: Train Loss=5.7930, Val Cor=0.0620, Time=0.4010 sec\n",
      "Epoch [347 / 2000]: Train Loss=5.7776, Val Cor=0.0791, Time=0.4008 sec\n",
      "Epoch [348 / 2000]: Train Loss=5.7861, Val Cor=-0.0361, Time=0.4014 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [349 / 2000]: Train Loss=5.7969, Val Cor=0.0245, Time=0.4006 sec\n",
      "Epoch [350 / 2000]: Train Loss=5.7925, Val Cor=0.0591, Time=0.4014 sec\n",
      "Epoch [351 / 2000]: Train Loss=5.7767, Val Cor=0.1056, Time=0.4011 sec\n",
      "Epoch [352 / 2000]: Train Loss=5.7780, Val Cor=0.0930, Time=0.3997 sec\n",
      "Epoch [353 / 2000]: Train Loss=5.7785, Val Cor=0.0802, Time=0.4005 sec\n",
      "Epoch [354 / 2000]: Train Loss=5.7942, Val Cor=0.1358, Time=0.4004 sec\n",
      "Epoch [355 / 2000]: Train Loss=5.7999, Val Cor=-0.1536, Time=0.4001 sec\n",
      "Epoch [356 / 2000]: Train Loss=5.7953, Val Cor=0.0131, Time=0.4007 sec\n",
      "Epoch [357 / 2000]: Train Loss=5.8145, Val Cor=-0.0205, Time=0.3999 sec\n",
      "Epoch [358 / 2000]: Train Loss=5.7795, Val Cor=0.0865, Time=0.5635 sec\n",
      "Epoch [359 / 2000]: Train Loss=5.7824, Val Cor=0.0611, Time=0.4026 sec\n",
      "Epoch [360 / 2000]: Train Loss=5.8049, Val Cor=0.0290, Time=0.4011 sec\n",
      "Epoch [361 / 2000]: Train Loss=5.7943, Val Cor=0.0455, Time=0.4011 sec\n",
      "Epoch [362 / 2000]: Train Loss=5.7809, Val Cor=-0.0755, Time=0.4013 sec\n",
      "Epoch [363 / 2000]: Train Loss=5.7959, Val Cor=-0.1083, Time=0.4014 sec\n",
      "Epoch [364 / 2000]: Train Loss=5.7901, Val Cor=-0.0378, Time=0.4018 sec\n",
      "Epoch [365 / 2000]: Train Loss=5.8122, Val Cor=-0.0794, Time=0.4005 sec\n",
      "Epoch [366 / 2000]: Train Loss=5.7899, Val Cor=-0.0869, Time=0.4014 sec\n",
      "Epoch [367 / 2000]: Train Loss=5.7936, Val Cor=-0.0474, Time=0.4002 sec\n",
      "Epoch [368 / 2000]: Train Loss=5.8038, Val Cor=0.0511, Time=0.4013 sec\n",
      "Epoch [369 / 2000]: Train Loss=5.7777, Val Cor=-0.0235, Time=0.4006 sec\n",
      "Epoch [370 / 2000]: Train Loss=5.7840, Val Cor=-0.0478, Time=0.4009 sec\n",
      "Epoch [371 / 2000]: Train Loss=5.7875, Val Cor=-0.0573, Time=0.4013 sec\n",
      "Epoch [372 / 2000]: Train Loss=5.7869, Val Cor=-0.0741, Time=0.4003 sec\n",
      "Epoch [373 / 2000]: Train Loss=5.7808, Val Cor=-0.0239, Time=0.3998 sec\n",
      "Epoch [374 / 2000]: Train Loss=5.8092, Val Cor=-0.0694, Time=0.4018 sec\n",
      "Epoch [375 / 2000]: Train Loss=5.7983, Val Cor=-0.1465, Time=0.3996 sec\n",
      "Epoch [376 / 2000]: Train Loss=5.7944, Val Cor=-0.0234, Time=0.4002 sec\n",
      "Epoch [377 / 2000]: Train Loss=5.7927, Val Cor=-0.0261, Time=0.4001 sec\n",
      "Epoch [378 / 2000]: Train Loss=5.7936, Val Cor=0.0908, Time=0.4003 sec\n",
      "Epoch [379 / 2000]: Train Loss=5.7784, Val Cor=0.0694, Time=0.4014 sec\n",
      "Epoch [380 / 2000]: Train Loss=5.7858, Val Cor=-0.0166, Time=0.3999 sec\n",
      "Epoch [381 / 2000]: Train Loss=5.7976, Val Cor=0.0311, Time=0.4008 sec\n",
      "Epoch [382 / 2000]: Train Loss=5.7785, Val Cor=-0.0038, Time=0.4008 sec\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerRegressor().to(device)\n",
    "model, best_ckpt = train_model_ESM(model, train_dataset, val_dataset, epochs=2000, batch_size=32, lr=5e-5, patience=200, device=device)\n",
    "model.load_state_dict(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3672901054568896"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_esm(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val = train_test_split(X, Y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GB(model, X_test, Y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    corr = spearmanr(preds, Y_test)\n",
    "    return (corr.statistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992, 3) (248, 3) (992,) (248,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape,Y_train.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 1280)\n",
      "(228,)\n",
      "[[ 0.02760313 -0.07387351  0.02632519 ... -0.17728937  0.00693432\n",
      "   0.14927322]\n",
      " [ 0.02774767 -0.07886593  0.02485685 ... -0.17778333  0.00544644\n",
      "   0.15013547]\n",
      " [ 0.02778614 -0.07698066  0.02444576 ... -0.17596288  0.00500626\n",
      "   0.14988568]\n",
      " ...\n",
      " [ 0.02909854 -0.0790172   0.02478098 ... -0.17759982  0.00528359\n",
      "   0.15115647]\n",
      " [ 0.0280756  -0.07748885  0.02418508 ... -0.17688388  0.00527793\n",
      "   0.15222098]\n",
      " [ 0.02687751 -0.07627581  0.02395615 ... -0.176063    0.00379012\n",
      "   0.14865206]]\n",
      "[0.55010003 0.1777     0.61879998 0.0908     0.30160001 0.1382\n",
      " 0.0171     0.077      0.1856     0.1525     0.26300001 0.25729999\n",
      " 0.33700001 0.185      0.0439     0.1235     0.59740001 0.0214\n",
      " 0.36559999 0.36219999 0.63550001 0.36700001 0.0163     0.3096\n",
      " 0.1179     0.0391     0.45480001 0.6462     0.94150001 0.0333\n",
      " 0.1497     0.0317     0.2112     0.1019     0.1373     0.0999\n",
      " 0.84140003 0.045      0.0232     0.33419999 0.0585     0.25389999\n",
      " 0.0579     0.13240001 0.3617     0.0676     0.58420002 0.0481\n",
      " 0.27410001 0.28839999 0.2086     0.3953     0.0879     0.0332\n",
      " 0.038      0.0814     0.0299     0.0171     0.1512     0.47639999\n",
      " 0.0305     0.96569997 0.1629     0.63919997 0.0105     0.0904\n",
      " 0.0195     0.0612     0.0111     0.1161     0.0579     0.26480001\n",
      " 0.4305     0.0277     0.3651     0.62529999 0.0737     0.1073\n",
      " 0.1118     0.37509999 0.0193     0.0214     0.0259     0.0286\n",
      " 0.3601     0.3522     0.51609999 0.2476     0.59439999 0.02\n",
      " 0.0675     0.1812     0.1998     0.47850001 0.0772     0.75160003\n",
      " 0.25940001 0.0221     0.287      0.1327     0.46419999 0.0342\n",
      " 0.1382     0.2579     0.2441     0.15899999 0.80610001 0.30149999\n",
      " 0.0813     0.368      0.0475     0.9957     0.1092     0.1231\n",
      " 0.1349     0.0277     0.27790001 0.2473     0.22050001 0.5323\n",
      " 0.1244     0.68180001 0.13519999 0.28569999 0.45289999 0.1605\n",
      " 0.73210001 0.0221     0.0145     0.0269     0.1506     0.0286\n",
      " 0.0223     0.64520001 0.15710001 0.1513     0.0257     0.64630002\n",
      " 0.4156     0.17919999 0.087      0.12989999 0.0335     0.0942\n",
      " 0.0429     0.4725     0.74870002 0.0255     0.0664     0.1603\n",
      " 0.0344     0.514      0.028      0.25780001 0.2764     0.32609999\n",
      " 0.0169     0.1388     0.4131     0.72710001 0.0913     0.2985\n",
      " 0.0412     0.2412     0.0353     0.25819999 0.2388     0.47400001\n",
      " 0.2208     0.0331     0.2103     0.1189     0.025      0.0276\n",
      " 0.1199     0.94989997 0.33199999 0.0676     0.25920001 0.048\n",
      " 0.2492     0.2445     0.23630001 0.28749999 0.55080003 0.20730001\n",
      " 0.0797     0.52969998 0.0981     0.0481     0.27270001 0.0214\n",
      " 0.0913     0.0412     0.3055     0.1222     0.1653     0.18089999\n",
      " 0.1725     0.78659999 0.4691     0.55919999 0.0195     0.0297\n",
      " 0.4862     0.1434     0.41659999 0.0245     0.54269999 0.0294\n",
      " 0.0256     0.0144     0.1728     0.0161     0.1128     0.2924\n",
      " 0.495      0.0186     0.53359997 0.1155     0.3669     0.48089999\n",
      " 0.2983     0.47780001 0.3134     0.38940001 0.0298     0.0491    ]\n"
     ]
    }
   ],
   "source": [
    "X_val = np.zeros(shape=(val_dataset.__len__(), 1280))\n",
    "Y_val = np.zeros(shape=(val_dataset.__len__(),))\n",
    "for i in range(0, val_dataset.__len__()):\n",
    "    X_row = val_dataset.__getitem__(i)[0].detach().cpu().numpy()#.flatten()\n",
    "    Y_row = val_dataset.__getitem__(i)[1].detach().cpu().numpy()\n",
    "    X_val[i] = X_row\n",
    "    Y_val[i] = Y_row\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_val)\n",
    "print(Y_val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 1280)\n",
      "(912,)\n",
      "[[ 0.02758145 -0.07752989  0.0261986  ... -0.17473149  0.00351624\n",
      "   0.15196908]\n",
      " [ 0.02670845 -0.07564262  0.02456556 ... -0.17363183  0.00384603\n",
      "   0.1469208 ]\n",
      " [ 0.02794668 -0.08093263  0.02550585 ... -0.17754811  0.00492499\n",
      "   0.14913122]\n",
      " ...\n",
      " [ 0.0253938  -0.07835097  0.02320869 ... -0.17715487  0.00485733\n",
      "   0.14893161]\n",
      " [ 0.02810415 -0.07836012  0.02577844 ... -0.1771356   0.00528115\n",
      "   0.14885642]\n",
      " [ 0.02643437 -0.0775612   0.02527846 ... -0.17677799  0.0042864\n",
      "   0.15230393]]\n",
      "[0.2078     0.14470001 0.1725     0.50830001 0.59219998 0.52969998\n",
      " 0.0533     0.66839999 0.026      0.3899     0.0312     0.0386\n",
      " 0.4084     0.034      0.13850001 0.32370001 0.3784     0.0268\n",
      " 0.28049999 0.0234     0.25920001 0.21529999 0.83170003 0.13160001\n",
      " 0.44080001 0.28850001 0.13150001 0.1215     0.2374     0.0426\n",
      " 0.1168     0.2931     0.0883     0.24789999 0.1787     0.1032\n",
      " 0.0427     0.0527     0.0211     0.0219     0.35569999 0.1842\n",
      " 0.57770002 0.0332     0.0284     0.50629997 0.0228     0.2305\n",
      " 0.0891     0.1612     0.331      0.0191     0.0453     0.0287\n",
      " 0.0638     0.61900002 0.0907     0.0348     0.2024     0.0091\n",
      " 0.1085     0.0727     0.33430001 0.2113     0.25119999 0.0831\n",
      " 0.094      0.0273     0.27309999 0.1346     0.1098     0.0235\n",
      " 0.27360001 0.30379999 0.4434     0.1363     0.31850001 0.3087\n",
      " 0.38229999 0.1278     0.0067     0.505      0.322      0.171\n",
      " 0.62050003 0.0173     0.0221     0.016      0.41510001 0.15090001\n",
      " 0.52520001 0.1025     0.018      0.024      0.50510001 0.1654\n",
      " 0.0243     0.0274     0.0513     0.1427     0.1156     0.1654\n",
      " 0.2423     0.1156     0.2005     0.27759999 0.0188     0.0549\n",
      " 0.2617     0.0209     0.0353     0.6796     0.0292     0.51059997\n",
      " 0.41949999 0.18089999 0.34439999 0.0351     0.0811     0.1016\n",
      " 0.0568     0.0232     0.32890001 0.31       0.0377     0.64310002\n",
      " 0.1566     0.4307     0.0894     0.0223     0.34909999 0.1476\n",
      " 0.36629999 0.55940002 0.116      0.90130001 0.17739999 0.1313\n",
      " 0.0262     0.60530001 0.39289999 0.52219999 0.0851     0.55409998\n",
      " 0.41150001 0.38069999 0.36129999 0.75169998 0.1203     0.2748\n",
      " 0.0244     0.50980002 0.63200003 0.0922     0.6322     0.14910001\n",
      " 0.65700001 0.29809999 0.2193     0.84719998 0.18430001 0.1041\n",
      " 0.1416     0.27559999 0.1673     0.33039999 0.3249     0.1812\n",
      " 0.0348     0.40079999 0.0348     0.244      0.0332     0.23540001\n",
      " 0.0187     0.1409     0.58490002 0.23459999 0.2879     0.3427\n",
      " 0.0595     0.1735     0.74080002 0.0673     0.03       0.3258\n",
      " 0.1552     0.2753     0.4921     0.31850001 0.42609999 0.0256\n",
      " 0.0589     0.14740001 0.2086     0.0096     0.31729999 0.76609999\n",
      " 0.0368     0.0641     0.51459998 0.3301     0.17       0.034\n",
      " 0.1371     0.3461     0.1181     0.2069     0.33329999 0.0855\n",
      " 0.2401     0.1104     0.21160001 0.038      0.1355     0.14920001\n",
      " 0.0571     0.47330001 0.0643     0.0594     0.0833     0.84960002\n",
      " 0.63450003 0.0883     0.0223     0.0411     0.22139999 0.61589998\n",
      " 0.0686     0.29949999 0.0278     0.0364     0.1149     0.041\n",
      " 0.2273     0.0291     0.2755     0.0318     0.2599     0.0084\n",
      " 0.052      0.12620001 0.16779999 0.038      0.0427     0.3856\n",
      " 0.0179     0.0239     0.273      0.28049999 0.27630001 0.65799999\n",
      " 0.98329997 0.0313     0.12710001 0.0201     0.1181     0.0772\n",
      " 0.2234     0.1008     0.19859999 0.0161     0.61690003 0.1246\n",
      " 0.1206     0.32870001 0.0263     0.0265     0.2044     0.1696\n",
      " 0.0859     0.3768     0.1911     0.37459999 0.0215     0.1938\n",
      " 0.0636     0.1626     0.0574     0.0864     0.1047     0.51410002\n",
      " 0.41280001 0.38299999 0.1495     0.2186     0.4339     0.38069999\n",
      " 0.0273     0.94230002 0.31369999 0.40720001 0.0483     0.18700001\n",
      " 0.065      0.1177     0.0663     0.0762     0.1169     0.26499999\n",
      " 0.4815     0.212      0.3603     0.0264     0.0247     0.2243\n",
      " 0.26679999 0.48069999 0.1302     0.082      0.0997     0.3795\n",
      " 0.273      0.60070002 0.0191     0.25229999 0.4862     0.29409999\n",
      " 0.1156     0.0413     0.18520001 0.1956     0.48140001 0.0864\n",
      " 0.45269999 0.14569999 0.0877     0.0217     0.1239     0.2386\n",
      " 0.1407     0.60960001 0.08       0.1444     0.37380001 0.0181\n",
      " 0.0209     0.35210001 0.57160002 0.52149999 0.61909997 0.4842\n",
      " 0.0468     0.5108     0.95700002 0.095      0.096      0.28799999\n",
      " 0.29350001 0.16689999 0.2111     0.27329999 0.0199     0.48730001\n",
      " 0.062      0.0247     0.0184     0.4648     0.5        0.0234\n",
      " 0.47420001 0.0896     0.1608     0.64819998 0.088      0.1145\n",
      " 0.0571     0.0351     0.116      0.0089     0.1046     0.1376\n",
      " 0.113      0.0997     0.0945     0.223      0.0553     0.1399\n",
      " 0.25099999 0.2035     0.11       0.0478     0.1647     0.24439999\n",
      " 0.3917     0.0674     0.33360001 0.1636     0.0904     0.1217\n",
      " 0.0268     0.1122     0.6142     0.0195     0.0325     0.31130001\n",
      " 0.0862     0.37029999 0.068      0.3168     0.19660001 0.3867\n",
      " 0.2066     0.1026     0.048      0.0714     0.1639     0.1054\n",
      " 0.20119999 0.23100001 0.1344     0.068      0.0157     0.33590001\n",
      " 0.0209     0.26539999 0.0439     0.0831     0.0328     0.1198\n",
      " 0.3926     0.19       0.41569999 0.273      0.0936     0.0258\n",
      " 0.1717     0.2577     0.3114     0.19059999 0.1795     0.1772\n",
      " 0.17730001 0.1331     0.248      0.023      0.21259999 0.0325\n",
      " 0.0262     0.97970003 0.21269999 0.54360002 0.21799999 0.51270002\n",
      " 0.0223     0.77509999 0.0432     0.4973     0.1991     0.2287\n",
      " 0.0728     0.2852     0.40970001 0.142      0.0281     0.17\n",
      " 0.2449     0.28529999 0.31220001 0.15090001 0.0731     0.0474\n",
      " 0.31439999 0.1041     0.1036     0.0234     0.0871     0.0804\n",
      " 0.034      0.14560001 0.0952     0.095      0.1521     0.33039999\n",
      " 0.26280001 0.1365     0.0263     0.023      0.45829999 0.0193\n",
      " 0.89099997 0.0394     0.1988     0.49129999 0.0147     0.2331\n",
      " 0.17900001 0.25189999 0.4842     0.109      0.7482     0.2217\n",
      " 0.2366     0.0268     0.26050001 0.4894     0.062      0.1417\n",
      " 0.49340001 0.0789     0.0361     0.2084     0.15000001 0.0251\n",
      " 0.0554     0.24690001 0.0648     0.0266     0.0909     0.0212\n",
      " 0.1136     0.1167     0.46709999 0.89410001 0.0128     0.094\n",
      " 0.22750001 0.0236     0.27860001 0.0698     0.171      0.0402\n",
      " 0.1227     0.15989999 0.3154     0.0752     0.0285     0.2252\n",
      " 0.0824     0.46110001 0.2649     0.1523     0.1204     0.0239\n",
      " 0.0379     0.1275     0.31900001 0.4355     0.149      0.80800003\n",
      " 0.20819999 0.0522     0.0427     0.1044     0.1069     0.322\n",
      " 0.21950001 0.35139999 0.013      0.0236     0.1182     0.51880002\n",
      " 0.27379999 0.4276     0.3653     0.21799999 0.0183     0.51709998\n",
      " 0.034      0.0292     0.0988     0.0877     0.0294     0.2414\n",
      " 0.61619997 0.0355     0.0215     0.42160001 0.4041     0.1151\n",
      " 0.3795     0.0223     0.0262     0.0823     0.5557     0.27770001\n",
      " 0.1172     0.93730003 0.39269999 0.0734     0.36610001 0.2483\n",
      " 0.62919998 0.83459997 0.155      0.0486     0.0316     0.62830001\n",
      " 0.41760001 0.15260001 0.0483     0.0255     0.1005     0.16159999\n",
      " 0.0224     0.0197     0.0232     0.2958     0.2386     0.27000001\n",
      " 0.1963     0.0235     0.40540001 0.0244     0.1309     0.014\n",
      " 0.057      0.62900001 0.0549     0.33160001 0.30630001 0.0695\n",
      " 0.0188     0.354      0.62019998 0.0179     0.0899     0.99089998\n",
      " 0.0622     0.1116     0.52109998 0.28740001 0.0782     0.32929999\n",
      " 0.1258     0.0943     0.26949999 0.1726     0.2967     0.2766\n",
      " 0.0503     0.54470003 0.1109     0.1716     0.43619999 0.1134\n",
      " 0.087      0.44080001 0.2201     0.40540001 0.40700001 0.42539999\n",
      " 0.45070001 0.15019999 0.44530001 0.2358     0.44780001 0.1549\n",
      " 0.47510001 0.65200001 0.32769999 0.54159999 0.0743     0.1275\n",
      " 0.2638     0.23469999 0.40000001 0.29859999 0.1044     0.3592\n",
      " 0.1429     0.0143     0.1592     0.3917     0.125      0.59310001\n",
      " 0.23649999 0.2485     0.52899998 0.35640001 0.51419997 0.54460001\n",
      " 0.0241     0.1979     0.537      0.0923     0.33880001 0.0281\n",
      " 0.053      0.5151     0.0938     0.0147     0.15449999 0.26190001\n",
      " 0.08       0.0557     0.0794     0.70459998 0.1276     0.2271\n",
      " 0.0398     0.1176     0.1797     0.3312     0.92320001 0.1948\n",
      " 0.23890001 0.2429     0.175      0.016      0.0186     0.1068\n",
      " 0.0337     0.0716     0.0275     0.1402     0.28060001 0.51969999\n",
      " 0.0237     0.0315     0.0356     0.1019     0.185      0.4145\n",
      " 0.86320001 0.0346     0.0979     0.0369     0.0205     0.0888\n",
      " 0.3215     0.29229999 0.0225     0.0323     0.1452     0.0313\n",
      " 0.1161     0.1128     0.0727     0.20100001 0.1018     0.2476\n",
      " 0.2413     0.0993     0.59560001 0.086      0.52640003 0.31380001\n",
      " 0.0244     0.1892     0.1063     0.38710001 0.0667     0.2104\n",
      " 0.0199     0.113      0.9594     0.1837     0.59109998 0.35699999\n",
      " 0.0351     0.0249     0.45910001 0.66350001 0.1113     0.38659999\n",
      " 0.0219     0.29069999 0.0309     0.2322     0.134      0.141\n",
      " 0.37439999 0.51499999 0.19499999 0.61150002 0.0226     0.32390001\n",
      " 0.048      0.1164     0.0075     0.0477     0.22830001 0.2375\n",
      " 0.39199999 0.0234     0.0872     0.49079999 0.47279999 0.0177\n",
      " 0.6494     0.1533     0.1017     0.077      0.02       0.21160001\n",
      " 0.027      0.1171     0.0774     0.0271     0.16580001 0.34029999\n",
      " 0.0675     0.0095     0.49759999 0.0391     0.3876     0.1575\n",
      " 0.1196     0.59939998 0.1156     0.76920003 0.2958     0.41150001\n",
      " 0.1092     0.1779     0.22759999 0.47459999 0.49579999 0.58609998\n",
      " 0.45230001 0.0829     0.198      0.058      0.20039999 0.0308\n",
      " 0.3987     0.14229999 0.4136     0.0526     0.0795     0.1107\n",
      " 0.16599999 0.4314     0.0535     0.27959999 0.0257     0.3538\n",
      " 0.44459999 0.28       0.1374     0.22830001 0.79350001 0.0618\n",
      " 0.2023     0.0934     0.63419998 0.2798     0.1205     0.1533\n",
      " 0.28929999 0.0989     0.0217     0.134      0.53500003 0.13600001\n",
      " 0.084      0.46630001 0.1043     0.1042     0.25740001 0.0649\n",
      " 0.0355     0.0258     0.3951     0.48800001 0.0314     0.0872\n",
      " 0.2164     0.95300001 0.27070001 0.0846     0.54210001 0.0714\n",
      " 0.0696     0.64029998 0.47870001 0.0336     0.294      0.0316\n",
      " 0.0426     0.3662     0.2809     0.1266     0.1085     0.2956\n",
      " 0.0363     0.0067     0.0742     0.0262     0.0269     0.0301\n",
      " 0.0936     0.2854     0.2005     0.0279     0.0171     0.53310001\n",
      " 0.67750001 0.03       0.39300001 0.34630001 0.2139     0.1175\n",
      " 0.38550001 0.1525     0.0307     0.1857     0.1569     0.25549999\n",
      " 0.0407     0.0777     0.0325     0.0572     0.0221     0.2052\n",
      " 0.33309999 0.1079     0.1231     0.9716     0.0458     0.0281    ]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.zeros(shape=(train_dataset.__len__(), 1280))\n",
    "Y_train = np.zeros(shape=(train_dataset.__len__(),))\n",
    "for i in range(0, train_dataset.__len__()):\n",
    "    X_row = train_dataset.__getitem__(i)[0].detach().cpu().numpy()#.flatten()\n",
    "    Y_row = train_dataset.__getitem__(i)[1].detach().cpu().numpy()\n",
    "    X_train[i] = X_row\n",
    "    Y_train[i] = Y_row\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Best Hyperparameters: lr: 0.01, trees: 5, depth: 1, \n",
      "                        split: 2, leaf: 1 | Current Best Val Corr: 0.5152983227525537\n",
      "2\n",
      "3\n",
      "4\n",
      "Current Best Hyperparameters: lr: 0.01, trees: 5, depth: 2, \n",
      "                        split: 2, leaf: 1 | Current Best Val Corr: 0.735909032134385\n",
      "6\n",
      "7\n",
      "8\n",
      "Current Best Hyperparameters: lr: 0.01, trees: 5, depth: 3, \n",
      "                        split: 2, leaf: 1 | Current Best Val Corr: 0.7898072055309913\n",
      "Current Best Hyperparameters: lr: 0.01, trees: 5, depth: 3, \n",
      "                        split: 2, leaf: 2 | Current Best Val Corr: 0.7898505575786545\n",
      "11\n",
      "12\n",
      "Current Best Hyperparameters: lr: 0.01, trees: 5, depth: 4, \n",
      "                        split: 2, leaf: 1 | Current Best Val Corr: 0.7977865033980587\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "Current Best Hyperparameters: lr: 0.01, trees: 10, depth: 4, \n",
      "                        split: 2, leaf: 1 | Current Best Val Corr: 0.8011653311837318\n",
      "34\n",
      "Current Best Hyperparameters: lr: 0.01, trees: 10, depth: 4, \n",
      "                        split: 5, leaf: 1 | Current Best Val Corr: 0.8012091440033647\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "Current Best Hyperparameters: lr: 0.05, trees: 5, depth: 4, \n",
      "                        split: 2, leaf: 1 | Current Best Val Corr: 0.8014439955095174\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n"
     ]
    }
   ],
   "source": [
    "learning_rate_range = [0.01,0.02,0.03,0.04,0.05,0.06,0.07]\n",
    "n_estimators_range = [5,10,20,50,100,200]\n",
    "depth_range = [1,2,3,4,5]\n",
    "min_samples_split_range = [2,5]\n",
    "min_samples_leaf_range = [1,2]\n",
    "best_corr = -1\n",
    "counter = 0\n",
    "for lr in learning_rate_range:\n",
    "    for n_estimator in n_estimators_range:\n",
    "        for depth in depth_range:\n",
    "            for min_samples_split in min_samples_split_range:\n",
    "                for min_samples_leaf in min_samples_leaf_range:\n",
    "                    model = GradientBoostingRegressor(learning_rate=lr,n_estimators=n_estimator,\n",
    "                                                       max_depth=depth,min_samples_split=min_samples_split, \n",
    "                                                       min_samples_leaf=min_samples_leaf)\n",
    "                    model.fit(X_val, Y_val)\n",
    "                    counter = counter + 1\n",
    "                    val_corr = evaluate_GB(model, X_train, Y_train)\n",
    "                    if val_corr > best_corr:\n",
    "                        best_corr = val_corr\n",
    "                        print(f\"\"\"Current Best Hyperparameters: lr: {lr}, trees: {n_estimator}, depth: {depth}, \n",
    "                        split: {min_samples_split}, leaf: {min_samples_leaf} | Current Best Val Corr: {best_corr}\"\"\")\n",
    "                    else:\n",
    "                        print(counter)\n",
    "                        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12143923 0.07700172 0.18719986]\n",
      " [0.12123349 0.13049431 0.18720004]\n",
      " [0.12169383 0.06313442 0.18719965]\n",
      " ...\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]]\n",
      "[0.273      0.28569999 0.21529999 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros(shape=(total_dataset.__len__(), 3))\n",
    "Y = np.zeros(shape=(total_dataset.__len__(),))\n",
    "for i in range(0, train_dataset.__len__()):\n",
    "    X_row = [preds_MLP[i], preds_RNN[i], preds_transformer[i]]\n",
    "    Y_row = total_dataset.__getitem__(i)[1].detach().cpu().numpy()\n",
    "    X[i] = X_row\n",
    "    Y[i] = Y_row\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_by_column(arr):\n",
    "    mins = arr.min(axis=0)\n",
    "    maxs = arr.max(axis=0)\n",
    "    return (arr - mins) / (maxs - mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98754399 0.30408035 0.99999363]\n",
      " [0.98587097 0.37663063 0.99999459]\n",
      " [0.98961441 0.28527258 0.99999252]\n",
      " ...\n",
      " [0.         0.1996454  0.        ]\n",
      " [0.         0.1996454  0.        ]\n",
      " [0.         0.1996454  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X = min_max_scale_by_column(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,\n",
       "                          n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,\n",
       "                          n_estimators=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,\n",
       "                          n_estimators=10)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ensemble = GradientBoostingRegressor(learning_rate=0.01,n_estimators=10,\n",
    "                                        max_depth=4,min_samples_split=5, \n",
    "                                        min_samples_leaf=1)\n",
    "model_ensemble.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,\n",
       "                          n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,\n",
       "                          n_estimators=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,\n",
       "                          n_estimators=10)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ensemble_full = GradientBoostingRegressor(learning_rate=0.01,n_estimators=10,\n",
    "                                        max_depth=4,min_samples_split=5, \n",
    "                                        min_samples_leaf=1)\n",
    "model_ensemble_full.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_dataset(df_train, emb_dir='esm_embeddings_test', seed=0, test_split=0.1):\n",
    "    sequences = df_train['sequence']\n",
    "    fitness_list = df_train['DMS_score'].tolist()\n",
    "\n",
    "    seq_train, seq_val, fitness_train, fitness_val = train_test_split(sequences, fitness_list, test_size=test_split, random_state=seed)\n",
    "\n",
    "    train_seq2name = train_seq2name = {seq: f'seq_{i}' for i, seq in enumerate(sequences)}\n",
    "    fitness2idx = {fitness: fitness for idx, fitness in enumerate(fitness_list)}\n",
    "\n",
    "    emb_dir = emb_dir\n",
    "    train_dataset = ProteinESMDataset(seq_train, train_seq2name, emb_dir, fitness_train, fitness2idx)\n",
    "    val_dataset = ProteinESMDataset(seq_val, train_seq2name, emb_dir, fitness_val, fitness2idx)\n",
    "    \n",
    "    return (train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 1206/1206 [00:09<00:00, 127.14it/s]\n",
      "Loading esm embeddings: 100%|██████████| 134/134 [00:00<00:00, 140.21it/s]\n",
      "Loading esm embeddings: 100%|██████████| 1206/1206 [00:03<00:00, 393.51it/s]\n",
      "Loading esm embeddings: 100%|██████████| 134/134 [00:00<00:00, 1607.58it/s]\n",
      "Loading esm embeddings: 100%|██████████| 1206/1206 [00:02<00:00, 438.95it/s] \n",
      "Loading esm embeddings: 100%|██████████| 134/134 [00:00<00:00, 1655.41it/s]\n",
      "Loading esm embeddings: 100%|██████████| 1206/1206 [00:00<00:00, 1652.40it/s]\n",
      "Loading esm embeddings: 100%|██████████| 134/134 [00:00<00:00, 1626.61it/s]\n",
      "Loading esm embeddings: 100%|██████████| 1206/1206 [00:00<00:00, 1551.18it/s]\n",
      "Loading esm embeddings: 100%|██████████| 134/134 [00:00<00:00, 1751.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sets = []\n",
    "val_sets = []\n",
    "for i in range(5):\n",
    "    train_set, val_set = bootstrap_dataset(df_train, seed=i,test_split=0.1)\n",
    "    train_sets.append(train_set)\n",
    "    val_sets.append(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN_ensemble(input_dim, hidden_dim, epochs, batch, lr, patience, train_data, val_data, seed):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = RNNRegressor().to(device)\n",
    "    model, best_ckpt = train_model_ESM(model, train_data, val_data, epochs=epochs, batch_size=batch, lr=lr, patience=200, device=device)\n",
    "    model.load_state_dict(best_ckpt)\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model 0:\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1 / 2000]: Train Loss=8.9274, Val Cor=0.0718, Time=2.0089 sec\n",
      "Epoch [2 / 2000]: Train Loss=8.1782, Val Cor=0.2308, Time=0.2780 sec\n",
      "Epoch [3 / 2000]: Train Loss=8.1082, Val Cor=0.2825, Time=0.2763 sec\n",
      "Epoch [4 / 2000]: Train Loss=8.1835, Val Cor=0.2898, Time=0.2755 sec\n",
      "Epoch [5 / 2000]: Train Loss=8.1175, Val Cor=0.3270, Time=0.2756 sec\n",
      "Epoch [6 / 2000]: Train Loss=8.1644, Val Cor=0.3076, Time=0.2757 sec\n",
      "Epoch [7 / 2000]: Train Loss=8.1114, Val Cor=0.3103, Time=0.2757 sec\n",
      "Epoch [8 / 2000]: Train Loss=8.1985, Val Cor=0.3352, Time=0.2757 sec\n",
      "Epoch [9 / 2000]: Train Loss=8.0887, Val Cor=0.3511, Time=0.2749 sec\n",
      "Epoch [10 / 2000]: Train Loss=8.1345, Val Cor=0.3569, Time=0.2742 sec\n",
      "Epoch [11 / 2000]: Train Loss=8.1127, Val Cor=0.3576, Time=0.2828 sec\n",
      "Epoch [12 / 2000]: Train Loss=8.0897, Val Cor=0.3546, Time=0.2741 sec\n",
      "Epoch [13 / 2000]: Train Loss=8.0745, Val Cor=0.3552, Time=0.2742 sec\n",
      "Epoch [14 / 2000]: Train Loss=8.0565, Val Cor=0.3582, Time=0.2798 sec\n",
      "Epoch [15 / 2000]: Train Loss=8.0082, Val Cor=0.3602, Time=0.2736 sec\n",
      "Epoch [16 / 2000]: Train Loss=7.9791, Val Cor=0.3630, Time=0.3939 sec\n",
      "Epoch [17 / 2000]: Train Loss=8.0456, Val Cor=0.3692, Time=0.2749 sec\n",
      "Epoch [18 / 2000]: Train Loss=7.9285, Val Cor=0.3874, Time=0.2742 sec\n",
      "Epoch [19 / 2000]: Train Loss=7.8082, Val Cor=0.4005, Time=0.2753 sec\n",
      "Epoch [20 / 2000]: Train Loss=7.9273, Val Cor=0.4047, Time=0.2758 sec\n",
      "Epoch [21 / 2000]: Train Loss=7.7983, Val Cor=0.4126, Time=0.2759 sec\n",
      "Epoch [22 / 2000]: Train Loss=7.6562, Val Cor=0.4215, Time=0.2758 sec\n",
      "Epoch [23 / 2000]: Train Loss=7.5896, Val Cor=0.4317, Time=0.2759 sec\n",
      "Epoch [24 / 2000]: Train Loss=7.5203, Val Cor=0.4365, Time=0.2761 sec\n",
      "Epoch [25 / 2000]: Train Loss=7.9710, Val Cor=0.4413, Time=0.2760 sec\n",
      "Epoch [26 / 2000]: Train Loss=7.4968, Val Cor=0.4427, Time=0.2758 sec\n",
      "Epoch [27 / 2000]: Train Loss=7.4752, Val Cor=0.4442, Time=0.2759 sec\n",
      "Epoch [28 / 2000]: Train Loss=7.4632, Val Cor=0.4472, Time=0.2759 sec\n",
      "Epoch [29 / 2000]: Train Loss=7.3867, Val Cor=0.4506, Time=0.2754 sec\n",
      "Epoch [30 / 2000]: Train Loss=7.3434, Val Cor=0.4508, Time=0.2756 sec\n",
      "Epoch [31 / 2000]: Train Loss=7.3900, Val Cor=0.4529, Time=0.2754 sec\n",
      "Epoch [32 / 2000]: Train Loss=7.1703, Val Cor=0.4538, Time=0.2807 sec\n",
      "Epoch [33 / 2000]: Train Loss=7.9709, Val Cor=0.4541, Time=0.2758 sec\n",
      "Epoch [34 / 2000]: Train Loss=7.4955, Val Cor=0.4538, Time=0.2753 sec\n",
      "Epoch [35 / 2000]: Train Loss=7.1090, Val Cor=0.4597, Time=0.2755 sec\n",
      "Epoch [36 / 2000]: Train Loss=7.6790, Val Cor=0.4576, Time=0.2759 sec\n",
      "Epoch [37 / 2000]: Train Loss=7.2936, Val Cor=0.4573, Time=0.2758 sec\n",
      "Epoch [38 / 2000]: Train Loss=7.1351, Val Cor=0.4595, Time=0.2759 sec\n",
      "Epoch [39 / 2000]: Train Loss=7.3836, Val Cor=0.4611, Time=0.2755 sec\n",
      "Epoch [40 / 2000]: Train Loss=7.0821, Val Cor=0.4615, Time=0.2756 sec\n",
      "Epoch [41 / 2000]: Train Loss=6.9845, Val Cor=0.4642, Time=0.2751 sec\n",
      "Epoch [42 / 2000]: Train Loss=7.3879, Val Cor=0.4647, Time=0.2758 sec\n",
      "Epoch [43 / 2000]: Train Loss=7.3684, Val Cor=0.4646, Time=0.2753 sec\n",
      "Epoch [44 / 2000]: Train Loss=7.0549, Val Cor=0.4647, Time=0.2754 sec\n",
      "Epoch [45 / 2000]: Train Loss=7.1845, Val Cor=0.4651, Time=0.2743 sec\n",
      "Epoch [46 / 2000]: Train Loss=7.3605, Val Cor=0.4687, Time=0.2737 sec\n",
      "Epoch [47 / 2000]: Train Loss=6.9544, Val Cor=0.4692, Time=0.2736 sec\n",
      "Epoch [48 / 2000]: Train Loss=7.0684, Val Cor=0.4691, Time=0.2739 sec\n",
      "Epoch [49 / 2000]: Train Loss=7.4014, Val Cor=0.4671, Time=0.2740 sec\n",
      "Epoch [50 / 2000]: Train Loss=7.0555, Val Cor=0.4653, Time=0.2743 sec\n",
      "Epoch [51 / 2000]: Train Loss=7.1443, Val Cor=0.4644, Time=0.2740 sec\n",
      "Epoch [52 / 2000]: Train Loss=7.0381, Val Cor=0.4635, Time=0.2739 sec\n",
      "Epoch [53 / 2000]: Train Loss=6.9974, Val Cor=0.4675, Time=0.2739 sec\n",
      "Epoch [54 / 2000]: Train Loss=6.8971, Val Cor=0.4659, Time=0.2739 sec\n",
      "Epoch [55 / 2000]: Train Loss=7.1187, Val Cor=0.4682, Time=0.2739 sec\n",
      "Epoch [56 / 2000]: Train Loss=6.7165, Val Cor=0.4702, Time=0.2739 sec\n",
      "Epoch [57 / 2000]: Train Loss=6.7951, Val Cor=0.4711, Time=0.2736 sec\n",
      "Epoch [58 / 2000]: Train Loss=7.0677, Val Cor=0.4720, Time=0.2740 sec\n",
      "Epoch [59 / 2000]: Train Loss=7.0987, Val Cor=0.4682, Time=0.2743 sec\n",
      "Epoch [60 / 2000]: Train Loss=6.9981, Val Cor=0.4686, Time=0.2742 sec\n",
      "Epoch [61 / 2000]: Train Loss=7.1006, Val Cor=0.4698, Time=0.2737 sec\n",
      "Epoch [62 / 2000]: Train Loss=6.8175, Val Cor=0.4716, Time=0.2738 sec\n",
      "Epoch [63 / 2000]: Train Loss=6.6020, Val Cor=0.4725, Time=0.2738 sec\n",
      "Epoch [64 / 2000]: Train Loss=6.8314, Val Cor=0.4738, Time=0.2736 sec\n",
      "Epoch [65 / 2000]: Train Loss=7.4637, Val Cor=0.4746, Time=0.2743 sec\n",
      "Epoch [66 / 2000]: Train Loss=7.1852, Val Cor=0.4732, Time=0.2737 sec\n",
      "Epoch [67 / 2000]: Train Loss=6.6693, Val Cor=0.4745, Time=0.2742 sec\n",
      "Epoch [68 / 2000]: Train Loss=6.9777, Val Cor=0.4801, Time=0.2745 sec\n",
      "Epoch [69 / 2000]: Train Loss=7.0287, Val Cor=0.4775, Time=0.2747 sec\n",
      "Epoch [70 / 2000]: Train Loss=6.8460, Val Cor=0.4817, Time=0.2744 sec\n",
      "Epoch [71 / 2000]: Train Loss=6.8675, Val Cor=0.4829, Time=0.2743 sec\n",
      "Epoch [72 / 2000]: Train Loss=6.9895, Val Cor=0.4816, Time=0.2736 sec\n",
      "Epoch [73 / 2000]: Train Loss=7.0517, Val Cor=0.4841, Time=0.2744 sec\n",
      "Epoch [74 / 2000]: Train Loss=6.8541, Val Cor=0.4839, Time=0.2742 sec\n",
      "Epoch [75 / 2000]: Train Loss=6.7985, Val Cor=0.4818, Time=0.2743 sec\n",
      "Epoch [76 / 2000]: Train Loss=7.0426, Val Cor=0.4791, Time=0.2746 sec\n",
      "Epoch [77 / 2000]: Train Loss=7.0159, Val Cor=0.4771, Time=0.2738 sec\n",
      "Epoch [78 / 2000]: Train Loss=6.9263, Val Cor=0.4771, Time=0.2737 sec\n",
      "Epoch [79 / 2000]: Train Loss=6.4314, Val Cor=0.4775, Time=0.2742 sec\n",
      "Epoch [80 / 2000]: Train Loss=6.8960, Val Cor=0.4763, Time=0.2738 sec\n",
      "Epoch [81 / 2000]: Train Loss=6.5906, Val Cor=0.4787, Time=0.2737 sec\n",
      "Epoch [82 / 2000]: Train Loss=6.5713, Val Cor=0.4770, Time=0.2736 sec\n",
      "Epoch [83 / 2000]: Train Loss=6.5734, Val Cor=0.4770, Time=0.2737 sec\n",
      "Epoch [84 / 2000]: Train Loss=7.1613, Val Cor=0.4780, Time=0.2738 sec\n",
      "Epoch [85 / 2000]: Train Loss=6.7318, Val Cor=0.4792, Time=0.2737 sec\n",
      "Epoch [86 / 2000]: Train Loss=6.9821, Val Cor=0.4807, Time=0.2741 sec\n",
      "Epoch [87 / 2000]: Train Loss=6.4795, Val Cor=0.4805, Time=0.2739 sec\n",
      "Epoch [88 / 2000]: Train Loss=6.9884, Val Cor=0.4810, Time=0.2738 sec\n",
      "Epoch [89 / 2000]: Train Loss=6.7143, Val Cor=0.4811, Time=0.2741 sec\n",
      "Epoch [90 / 2000]: Train Loss=6.5588, Val Cor=0.4834, Time=0.2739 sec\n",
      "Epoch [91 / 2000]: Train Loss=7.0778, Val Cor=0.4815, Time=0.2739 sec\n",
      "Epoch [92 / 2000]: Train Loss=6.7373, Val Cor=0.4810, Time=0.2739 sec\n",
      "Epoch [93 / 2000]: Train Loss=6.7743, Val Cor=0.4815, Time=0.2741 sec\n",
      "Epoch [94 / 2000]: Train Loss=6.4020, Val Cor=0.4831, Time=0.2736 sec\n",
      "Epoch [95 / 2000]: Train Loss=6.8198, Val Cor=0.4823, Time=0.2739 sec\n",
      "Epoch [96 / 2000]: Train Loss=6.9358, Val Cor=0.4841, Time=0.2737 sec\n",
      "Epoch [97 / 2000]: Train Loss=6.5846, Val Cor=0.4844, Time=0.2737 sec\n",
      "Epoch [98 / 2000]: Train Loss=6.9308, Val Cor=0.4853, Time=0.2741 sec\n",
      "Epoch [99 / 2000]: Train Loss=6.5990, Val Cor=0.4809, Time=0.2731 sec\n",
      "Epoch [100 / 2000]: Train Loss=6.9221, Val Cor=0.4846, Time=0.2735 sec\n",
      "Epoch [101 / 2000]: Train Loss=6.5416, Val Cor=0.4817, Time=0.2734 sec\n",
      "Epoch [102 / 2000]: Train Loss=6.3104, Val Cor=0.4875, Time=0.2730 sec\n",
      "Epoch [103 / 2000]: Train Loss=6.8309, Val Cor=0.4878, Time=0.2734 sec\n",
      "Epoch [104 / 2000]: Train Loss=6.4367, Val Cor=0.4882, Time=0.2731 sec\n",
      "Epoch [105 / 2000]: Train Loss=6.3124, Val Cor=0.4859, Time=0.2733 sec\n",
      "Epoch [106 / 2000]: Train Loss=6.5967, Val Cor=0.4867, Time=0.2732 sec\n",
      "Epoch [107 / 2000]: Train Loss=6.8033, Val Cor=0.4927, Time=0.2742 sec\n",
      "Epoch [108 / 2000]: Train Loss=6.5889, Val Cor=0.4929, Time=0.2736 sec\n",
      "Epoch [109 / 2000]: Train Loss=6.3589, Val Cor=0.4921, Time=0.2735 sec\n",
      "Epoch [110 / 2000]: Train Loss=6.4297, Val Cor=0.4962, Time=0.2735 sec\n",
      "Epoch [111 / 2000]: Train Loss=6.4287, Val Cor=0.4976, Time=0.2734 sec\n",
      "Epoch [112 / 2000]: Train Loss=6.4393, Val Cor=0.4974, Time=0.2731 sec\n",
      "Epoch [113 / 2000]: Train Loss=6.5697, Val Cor=0.4973, Time=0.2730 sec\n",
      "Epoch [114 / 2000]: Train Loss=6.6998, Val Cor=0.4978, Time=0.2736 sec\n",
      "Epoch [115 / 2000]: Train Loss=6.7056, Val Cor=0.4929, Time=0.2734 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [116 / 2000]: Train Loss=6.4691, Val Cor=0.4958, Time=0.2732 sec\n",
      "Epoch [117 / 2000]: Train Loss=6.5635, Val Cor=0.4980, Time=0.2732 sec\n",
      "Epoch [118 / 2000]: Train Loss=6.5396, Val Cor=0.4996, Time=0.2729 sec\n",
      "Epoch [119 / 2000]: Train Loss=6.4711, Val Cor=0.4978, Time=0.2729 sec\n",
      "Epoch [120 / 2000]: Train Loss=6.6828, Val Cor=0.5000, Time=0.2729 sec\n",
      "Epoch [121 / 2000]: Train Loss=6.3643, Val Cor=0.5022, Time=0.2730 sec\n",
      "Epoch [122 / 2000]: Train Loss=6.7714, Val Cor=0.4978, Time=0.2733 sec\n",
      "Epoch [123 / 2000]: Train Loss=6.4213, Val Cor=0.4997, Time=0.2732 sec\n",
      "Epoch [124 / 2000]: Train Loss=6.6804, Val Cor=0.4973, Time=0.2732 sec\n",
      "Epoch [125 / 2000]: Train Loss=6.3936, Val Cor=0.4957, Time=0.2731 sec\n",
      "Epoch [126 / 2000]: Train Loss=6.4974, Val Cor=0.4975, Time=0.2732 sec\n",
      "Epoch [127 / 2000]: Train Loss=6.9871, Val Cor=0.4979, Time=0.2735 sec\n",
      "Epoch [128 / 2000]: Train Loss=6.5239, Val Cor=0.4952, Time=0.2731 sec\n",
      "Epoch [129 / 2000]: Train Loss=6.3465, Val Cor=0.4977, Time=0.2799 sec\n",
      "Epoch [130 / 2000]: Train Loss=6.4084, Val Cor=0.4974, Time=0.2792 sec\n",
      "Epoch [131 / 2000]: Train Loss=6.2932, Val Cor=0.4990, Time=0.2790 sec\n",
      "Epoch [132 / 2000]: Train Loss=6.6768, Val Cor=0.5002, Time=0.2790 sec\n",
      "Epoch [133 / 2000]: Train Loss=6.8795, Val Cor=0.4997, Time=0.2790 sec\n",
      "Epoch [134 / 2000]: Train Loss=6.6245, Val Cor=0.5011, Time=0.2789 sec\n",
      "Epoch [135 / 2000]: Train Loss=6.3685, Val Cor=0.4997, Time=0.2789 sec\n",
      "Epoch [136 / 2000]: Train Loss=6.6206, Val Cor=0.5007, Time=0.2789 sec\n",
      "Epoch [137 / 2000]: Train Loss=6.4867, Val Cor=0.5015, Time=0.2790 sec\n",
      "Epoch [138 / 2000]: Train Loss=6.4612, Val Cor=0.5016, Time=0.2728 sec\n",
      "Epoch [139 / 2000]: Train Loss=6.7232, Val Cor=0.5009, Time=0.2726 sec\n",
      "Epoch [140 / 2000]: Train Loss=6.3451, Val Cor=0.5031, Time=0.2729 sec\n",
      "Epoch [141 / 2000]: Train Loss=6.2611, Val Cor=0.5051, Time=0.2725 sec\n",
      "Epoch [142 / 2000]: Train Loss=6.2582, Val Cor=0.5066, Time=0.2725 sec\n",
      "Epoch [143 / 2000]: Train Loss=6.7380, Val Cor=0.5049, Time=0.2728 sec\n",
      "Epoch [144 / 2000]: Train Loss=6.4847, Val Cor=0.5044, Time=0.2794 sec\n",
      "Epoch [145 / 2000]: Train Loss=6.4321, Val Cor=0.5056, Time=0.2723 sec\n",
      "Epoch [146 / 2000]: Train Loss=6.3839, Val Cor=0.5049, Time=0.2729 sec\n",
      "Epoch [147 / 2000]: Train Loss=6.2635, Val Cor=0.5050, Time=0.2724 sec\n",
      "Epoch [148 / 2000]: Train Loss=6.1487, Val Cor=0.5078, Time=0.2729 sec\n",
      "Epoch [149 / 2000]: Train Loss=6.4960, Val Cor=0.5058, Time=0.2722 sec\n",
      "Epoch [150 / 2000]: Train Loss=6.2700, Val Cor=0.5060, Time=0.2729 sec\n",
      "Epoch [151 / 2000]: Train Loss=6.3141, Val Cor=0.5082, Time=0.2723 sec\n",
      "Epoch [152 / 2000]: Train Loss=6.3689, Val Cor=0.5058, Time=0.2722 sec\n",
      "Epoch [153 / 2000]: Train Loss=6.3833, Val Cor=0.5059, Time=0.2722 sec\n",
      "Epoch [154 / 2000]: Train Loss=6.2706, Val Cor=0.5091, Time=0.2723 sec\n",
      "Epoch [155 / 2000]: Train Loss=6.3719, Val Cor=0.5071, Time=0.2728 sec\n",
      "Epoch [156 / 2000]: Train Loss=6.2607, Val Cor=0.5061, Time=0.2724 sec\n",
      "Epoch [157 / 2000]: Train Loss=6.0363, Val Cor=0.5108, Time=0.2723 sec\n",
      "Epoch [158 / 2000]: Train Loss=6.1983, Val Cor=0.5068, Time=0.2721 sec\n",
      "Epoch [159 / 2000]: Train Loss=6.0560, Val Cor=0.5060, Time=0.2726 sec\n",
      "Epoch [160 / 2000]: Train Loss=6.1282, Val Cor=0.5105, Time=0.2723 sec\n",
      "Epoch [161 / 2000]: Train Loss=6.2645, Val Cor=0.5075, Time=0.2729 sec\n",
      "Epoch [162 / 2000]: Train Loss=6.1196, Val Cor=0.5061, Time=0.2728 sec\n",
      "Epoch [163 / 2000]: Train Loss=7.1509, Val Cor=0.5036, Time=0.2728 sec\n",
      "Epoch [164 / 2000]: Train Loss=6.8236, Val Cor=0.5080, Time=0.2723 sec\n",
      "Epoch [165 / 2000]: Train Loss=6.3604, Val Cor=0.5101, Time=0.2726 sec\n",
      "Epoch [166 / 2000]: Train Loss=6.3437, Val Cor=0.5044, Time=0.2723 sec\n",
      "Epoch [167 / 2000]: Train Loss=6.3836, Val Cor=0.5061, Time=0.2727 sec\n",
      "Epoch [168 / 2000]: Train Loss=6.4591, Val Cor=0.5059, Time=0.2724 sec\n",
      "Epoch [169 / 2000]: Train Loss=6.2896, Val Cor=0.5047, Time=0.2728 sec\n",
      "Epoch [170 / 2000]: Train Loss=6.2533, Val Cor=0.5065, Time=0.2727 sec\n",
      "Epoch [171 / 2000]: Train Loss=6.4222, Val Cor=0.5061, Time=0.2724 sec\n",
      "Epoch [172 / 2000]: Train Loss=6.1201, Val Cor=0.5056, Time=0.2727 sec\n",
      "Epoch [173 / 2000]: Train Loss=6.2111, Val Cor=0.5070, Time=0.2728 sec\n",
      "Epoch [174 / 2000]: Train Loss=6.1542, Val Cor=0.5087, Time=0.2726 sec\n",
      "Epoch [175 / 2000]: Train Loss=6.1776, Val Cor=0.5056, Time=0.2724 sec\n",
      "Epoch [176 / 2000]: Train Loss=6.3532, Val Cor=0.5058, Time=0.2728 sec\n",
      "Epoch [177 / 2000]: Train Loss=6.0645, Val Cor=0.5079, Time=0.2723 sec\n",
      "Epoch [178 / 2000]: Train Loss=6.8327, Val Cor=0.5066, Time=0.2726 sec\n",
      "Epoch [179 / 2000]: Train Loss=6.2459, Val Cor=0.5050, Time=0.2723 sec\n",
      "Epoch [180 / 2000]: Train Loss=6.2134, Val Cor=0.5061, Time=0.2723 sec\n",
      "Epoch [181 / 2000]: Train Loss=6.1387, Val Cor=0.5074, Time=0.2789 sec\n",
      "Epoch [182 / 2000]: Train Loss=6.4063, Val Cor=0.5085, Time=0.2723 sec\n",
      "Epoch [183 / 2000]: Train Loss=6.2836, Val Cor=0.5089, Time=0.2723 sec\n",
      "Epoch [184 / 2000]: Train Loss=6.4689, Val Cor=0.5100, Time=0.2722 sec\n",
      "Epoch [185 / 2000]: Train Loss=6.2766, Val Cor=0.5105, Time=0.2724 sec\n",
      "Epoch [186 / 2000]: Train Loss=6.4269, Val Cor=0.5116, Time=0.2723 sec\n",
      "Epoch [187 / 2000]: Train Loss=6.3473, Val Cor=0.5145, Time=0.2722 sec\n",
      "Epoch [188 / 2000]: Train Loss=6.3441, Val Cor=0.5141, Time=0.2723 sec\n",
      "Epoch [189 / 2000]: Train Loss=6.4087, Val Cor=0.5163, Time=0.2723 sec\n",
      "Epoch [190 / 2000]: Train Loss=6.1939, Val Cor=0.5112, Time=0.2722 sec\n",
      "Epoch [191 / 2000]: Train Loss=6.3623, Val Cor=0.5136, Time=0.2722 sec\n",
      "Epoch [192 / 2000]: Train Loss=6.1509, Val Cor=0.5156, Time=0.2720 sec\n",
      "Epoch [193 / 2000]: Train Loss=6.2605, Val Cor=0.5142, Time=0.2722 sec\n",
      "Epoch [194 / 2000]: Train Loss=6.4415, Val Cor=0.5151, Time=0.2723 sec\n",
      "Epoch [195 / 2000]: Train Loss=6.1407, Val Cor=0.5168, Time=0.2722 sec\n",
      "Epoch [196 / 2000]: Train Loss=6.5275, Val Cor=0.5157, Time=0.2723 sec\n",
      "Epoch [197 / 2000]: Train Loss=6.2728, Val Cor=0.5141, Time=0.2723 sec\n",
      "Epoch [198 / 2000]: Train Loss=6.8558, Val Cor=0.5161, Time=0.2729 sec\n",
      "Epoch [199 / 2000]: Train Loss=6.1946, Val Cor=0.5148, Time=0.2779 sec\n",
      "Epoch [200 / 2000]: Train Loss=6.0772, Val Cor=0.5164, Time=0.3002 sec\n",
      "Epoch [201 / 2000]: Train Loss=6.3640, Val Cor=0.5157, Time=0.2909 sec\n",
      "Epoch [202 / 2000]: Train Loss=6.7188, Val Cor=0.5145, Time=0.2984 sec\n",
      "Epoch [203 / 2000]: Train Loss=6.3346, Val Cor=0.5165, Time=0.2980 sec\n",
      "Epoch [204 / 2000]: Train Loss=6.2307, Val Cor=0.5143, Time=0.2986 sec\n",
      "Epoch [205 / 2000]: Train Loss=6.1812, Val Cor=0.5189, Time=0.2961 sec\n",
      "Epoch [206 / 2000]: Train Loss=6.2767, Val Cor=0.5197, Time=0.3095 sec\n",
      "Epoch [207 / 2000]: Train Loss=6.0044, Val Cor=0.5201, Time=0.3129 sec\n",
      "Epoch [208 / 2000]: Train Loss=6.3882, Val Cor=0.5177, Time=0.3018 sec\n",
      "Epoch [209 / 2000]: Train Loss=6.1424, Val Cor=0.5180, Time=0.3092 sec\n",
      "Epoch [210 / 2000]: Train Loss=6.0415, Val Cor=0.5192, Time=0.3115 sec\n",
      "Epoch [211 / 2000]: Train Loss=6.0798, Val Cor=0.5214, Time=0.3173 sec\n",
      "Epoch [212 / 2000]: Train Loss=6.2848, Val Cor=0.5213, Time=0.2940 sec\n",
      "Epoch [213 / 2000]: Train Loss=6.3302, Val Cor=0.5204, Time=0.2938 sec\n",
      "Epoch [214 / 2000]: Train Loss=6.3305, Val Cor=0.5204, Time=0.2986 sec\n",
      "Epoch [215 / 2000]: Train Loss=6.5671, Val Cor=0.5142, Time=0.3053 sec\n",
      "Epoch [216 / 2000]: Train Loss=6.2706, Val Cor=0.5189, Time=0.2993 sec\n",
      "Epoch [217 / 2000]: Train Loss=5.9939, Val Cor=0.5197, Time=0.3117 sec\n",
      "Epoch [218 / 2000]: Train Loss=6.4258, Val Cor=0.5185, Time=0.3063 sec\n",
      "Epoch [219 / 2000]: Train Loss=6.2595, Val Cor=0.5198, Time=0.2924 sec\n",
      "Epoch [220 / 2000]: Train Loss=6.6099, Val Cor=0.5219, Time=0.2970 sec\n",
      "Epoch [221 / 2000]: Train Loss=6.4691, Val Cor=0.5210, Time=0.2984 sec\n",
      "Epoch [222 / 2000]: Train Loss=6.3097, Val Cor=0.5209, Time=0.2903 sec\n",
      "Epoch [223 / 2000]: Train Loss=6.0669, Val Cor=0.5213, Time=0.2943 sec\n",
      "Epoch [224 / 2000]: Train Loss=6.3811, Val Cor=0.5178, Time=0.2841 sec\n",
      "Epoch [225 / 2000]: Train Loss=6.1282, Val Cor=0.5202, Time=0.2820 sec\n",
      "Epoch [226 / 2000]: Train Loss=6.2287, Val Cor=0.5211, Time=0.2839 sec\n",
      "Epoch [227 / 2000]: Train Loss=6.1647, Val Cor=0.5233, Time=0.2870 sec\n",
      "Epoch [228 / 2000]: Train Loss=6.1642, Val Cor=0.5209, Time=0.2879 sec\n",
      "Epoch [229 / 2000]: Train Loss=6.1938, Val Cor=0.5240, Time=0.3047 sec\n",
      "Epoch [230 / 2000]: Train Loss=6.3735, Val Cor=0.5225, Time=0.3032 sec\n",
      "Epoch [231 / 2000]: Train Loss=6.2507, Val Cor=0.5222, Time=0.2938 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [232 / 2000]: Train Loss=6.1534, Val Cor=0.5253, Time=0.2942 sec\n",
      "Epoch [233 / 2000]: Train Loss=6.0050, Val Cor=0.5248, Time=0.2887 sec\n",
      "Epoch [234 / 2000]: Train Loss=6.3738, Val Cor=0.5242, Time=0.2928 sec\n",
      "Epoch [235 / 2000]: Train Loss=6.2917, Val Cor=0.5210, Time=0.2781 sec\n",
      "Epoch [236 / 2000]: Train Loss=5.9928, Val Cor=0.5214, Time=0.2757 sec\n",
      "Epoch [237 / 2000]: Train Loss=6.0266, Val Cor=0.5236, Time=0.2749 sec\n",
      "Epoch [238 / 2000]: Train Loss=5.9887, Val Cor=0.5252, Time=0.2756 sec\n",
      "Epoch [239 / 2000]: Train Loss=6.1785, Val Cor=0.5228, Time=0.2771 sec\n",
      "Epoch [240 / 2000]: Train Loss=6.1606, Val Cor=0.5225, Time=0.2761 sec\n",
      "Epoch [241 / 2000]: Train Loss=5.9410, Val Cor=0.5223, Time=0.2762 sec\n",
      "Epoch [242 / 2000]: Train Loss=6.4262, Val Cor=0.5225, Time=0.2908 sec\n",
      "Epoch [243 / 2000]: Train Loss=6.0335, Val Cor=0.5231, Time=0.3141 sec\n",
      "Epoch [244 / 2000]: Train Loss=5.7443, Val Cor=0.5231, Time=0.3102 sec\n",
      "Epoch [245 / 2000]: Train Loss=6.1447, Val Cor=0.5252, Time=0.3276 sec\n",
      "Epoch [246 / 2000]: Train Loss=5.7567, Val Cor=0.5264, Time=0.3135 sec\n",
      "Epoch [247 / 2000]: Train Loss=5.8522, Val Cor=0.5290, Time=0.3209 sec\n",
      "Epoch [248 / 2000]: Train Loss=6.3517, Val Cor=0.5273, Time=0.3133 sec\n",
      "Epoch [249 / 2000]: Train Loss=6.1999, Val Cor=0.5264, Time=0.3184 sec\n",
      "Epoch [250 / 2000]: Train Loss=6.3134, Val Cor=0.5243, Time=0.3104 sec\n",
      "Epoch [251 / 2000]: Train Loss=5.8577, Val Cor=0.5321, Time=0.3142 sec\n",
      "Epoch [252 / 2000]: Train Loss=6.0293, Val Cor=0.5248, Time=0.3131 sec\n",
      "Epoch [253 / 2000]: Train Loss=5.7510, Val Cor=0.5255, Time=0.3014 sec\n",
      "Epoch [254 / 2000]: Train Loss=5.7409, Val Cor=0.5274, Time=0.3206 sec\n",
      "Epoch [255 / 2000]: Train Loss=5.8714, Val Cor=0.3286, Time=0.3096 sec\n",
      "Epoch [256 / 2000]: Train Loss=6.1792, Val Cor=0.5252, Time=0.3173 sec\n",
      "Epoch [257 / 2000]: Train Loss=5.8019, Val Cor=0.5264, Time=0.2993 sec\n",
      "Epoch [258 / 2000]: Train Loss=5.9137, Val Cor=0.5277, Time=0.2986 sec\n",
      "Epoch [259 / 2000]: Train Loss=5.8411, Val Cor=0.5283, Time=0.2918 sec\n",
      "Epoch [260 / 2000]: Train Loss=5.7041, Val Cor=0.5284, Time=0.2985 sec\n",
      "Epoch [261 / 2000]: Train Loss=5.8578, Val Cor=0.5321, Time=0.2979 sec\n",
      "Epoch [262 / 2000]: Train Loss=5.7807, Val Cor=0.5316, Time=0.3168 sec\n",
      "Epoch [263 / 2000]: Train Loss=5.9919, Val Cor=0.5297, Time=0.3048 sec\n",
      "Epoch [264 / 2000]: Train Loss=6.1590, Val Cor=0.5289, Time=0.3043 sec\n",
      "Epoch [265 / 2000]: Train Loss=5.9215, Val Cor=0.5288, Time=0.3133 sec\n",
      "Epoch [266 / 2000]: Train Loss=6.0922, Val Cor=0.5241, Time=0.3086 sec\n",
      "Epoch [267 / 2000]: Train Loss=5.7485, Val Cor=0.4911, Time=0.2906 sec\n",
      "Epoch [268 / 2000]: Train Loss=6.1357, Val Cor=0.3918, Time=0.3211 sec\n",
      "Epoch [269 / 2000]: Train Loss=5.9624, Val Cor=0.5240, Time=0.3163 sec\n",
      "Epoch [270 / 2000]: Train Loss=5.7576, Val Cor=0.5262, Time=0.3080 sec\n",
      "Epoch [271 / 2000]: Train Loss=5.9445, Val Cor=0.5233, Time=0.3058 sec\n",
      "Epoch [272 / 2000]: Train Loss=5.6338, Val Cor=0.5250, Time=0.3017 sec\n",
      "Epoch [273 / 2000]: Train Loss=6.0763, Val Cor=0.5255, Time=0.3164 sec\n",
      "Epoch [274 / 2000]: Train Loss=5.8222, Val Cor=0.5276, Time=0.3079 sec\n",
      "Epoch [275 / 2000]: Train Loss=5.8044, Val Cor=0.5334, Time=0.3235 sec\n",
      "Epoch [276 / 2000]: Train Loss=5.9540, Val Cor=-0.5068, Time=0.3091 sec\n",
      "Epoch [277 / 2000]: Train Loss=5.9794, Val Cor=0.5297, Time=0.3082 sec\n",
      "Epoch [278 / 2000]: Train Loss=5.9097, Val Cor=0.5273, Time=0.3120 sec\n",
      "Epoch [279 / 2000]: Train Loss=5.8416, Val Cor=0.4823, Time=0.3080 sec\n",
      "Epoch [280 / 2000]: Train Loss=5.7748, Val Cor=0.5223, Time=0.3181 sec\n",
      "Epoch [281 / 2000]: Train Loss=5.8375, Val Cor=0.5301, Time=0.3145 sec\n",
      "Epoch [282 / 2000]: Train Loss=5.8296, Val Cor=0.5297, Time=0.3068 sec\n",
      "Epoch [283 / 2000]: Train Loss=5.7642, Val Cor=0.5171, Time=0.3140 sec\n",
      "Epoch [284 / 2000]: Train Loss=5.8988, Val Cor=0.5333, Time=0.3032 sec\n",
      "Epoch [285 / 2000]: Train Loss=5.7998, Val Cor=0.5317, Time=0.3355 sec\n",
      "Epoch [286 / 2000]: Train Loss=5.7321, Val Cor=0.5336, Time=0.3047 sec\n",
      "Epoch [287 / 2000]: Train Loss=5.7018, Val Cor=0.5344, Time=0.3135 sec\n",
      "Epoch [288 / 2000]: Train Loss=5.7465, Val Cor=0.5325, Time=0.2963 sec\n",
      "Epoch [289 / 2000]: Train Loss=5.9445, Val Cor=0.5303, Time=0.2909 sec\n",
      "Epoch [290 / 2000]: Train Loss=5.5764, Val Cor=0.5299, Time=0.2922 sec\n",
      "Epoch [291 / 2000]: Train Loss=5.8304, Val Cor=0.5306, Time=0.2967 sec\n",
      "Epoch [292 / 2000]: Train Loss=5.8031, Val Cor=0.5317, Time=0.2968 sec\n",
      "Epoch [293 / 2000]: Train Loss=5.8221, Val Cor=0.5311, Time=0.2977 sec\n",
      "Epoch [294 / 2000]: Train Loss=5.9449, Val Cor=0.5274, Time=0.2989 sec\n",
      "Epoch [295 / 2000]: Train Loss=5.7197, Val Cor=0.5304, Time=0.2925 sec\n",
      "Epoch [296 / 2000]: Train Loss=5.6779, Val Cor=0.5167, Time=0.2980 sec\n",
      "Epoch [297 / 2000]: Train Loss=6.2263, Val Cor=0.5337, Time=0.2927 sec\n",
      "Epoch [298 / 2000]: Train Loss=5.7855, Val Cor=0.5228, Time=0.2811 sec\n",
      "Epoch [299 / 2000]: Train Loss=5.9001, Val Cor=0.4853, Time=0.3075 sec\n",
      "Epoch [300 / 2000]: Train Loss=6.0332, Val Cor=0.0759, Time=0.3031 sec\n",
      "Epoch [301 / 2000]: Train Loss=6.2438, Val Cor=0.0789, Time=0.2933 sec\n",
      "Epoch [302 / 2000]: Train Loss=6.0676, Val Cor=0.5294, Time=0.3143 sec\n",
      "Epoch [303 / 2000]: Train Loss=5.6693, Val Cor=0.5313, Time=0.3057 sec\n",
      "Epoch [304 / 2000]: Train Loss=6.0150, Val Cor=0.5335, Time=0.3044 sec\n",
      "Epoch [305 / 2000]: Train Loss=5.8730, Val Cor=0.3629, Time=0.3051 sec\n",
      "Epoch [306 / 2000]: Train Loss=6.4582, Val Cor=0.1881, Time=0.3029 sec\n",
      "Epoch [307 / 2000]: Train Loss=6.2414, Val Cor=0.4161, Time=0.3107 sec\n",
      "Epoch [308 / 2000]: Train Loss=5.9247, Val Cor=0.4307, Time=0.3019 sec\n",
      "Epoch [309 / 2000]: Train Loss=6.5368, Val Cor=0.5341, Time=0.3021 sec\n",
      "Epoch [310 / 2000]: Train Loss=6.0779, Val Cor=0.5300, Time=0.3106 sec\n",
      "Epoch [311 / 2000]: Train Loss=5.8330, Val Cor=0.5355, Time=0.3090 sec\n",
      "Epoch [312 / 2000]: Train Loss=5.8210, Val Cor=0.5032, Time=0.2884 sec\n",
      "Epoch [313 / 2000]: Train Loss=5.6600, Val Cor=0.5350, Time=0.2734 sec\n",
      "Epoch [314 / 2000]: Train Loss=5.5723, Val Cor=0.5355, Time=0.2729 sec\n",
      "Epoch [315 / 2000]: Train Loss=6.0167, Val Cor=0.5274, Time=0.2728 sec\n",
      "Epoch [316 / 2000]: Train Loss=5.8809, Val Cor=0.4948, Time=0.2728 sec\n",
      "Epoch [317 / 2000]: Train Loss=5.7950, Val Cor=0.5339, Time=0.2728 sec\n",
      "Epoch [318 / 2000]: Train Loss=5.8141, Val Cor=0.5249, Time=0.2723 sec\n",
      "Epoch [319 / 2000]: Train Loss=6.0463, Val Cor=0.5265, Time=0.2793 sec\n",
      "Epoch [320 / 2000]: Train Loss=5.6733, Val Cor=0.5385, Time=0.2733 sec\n",
      "Epoch [321 / 2000]: Train Loss=5.7239, Val Cor=0.5330, Time=0.2721 sec\n",
      "Epoch [322 / 2000]: Train Loss=5.6572, Val Cor=0.5350, Time=0.2721 sec\n",
      "Epoch [323 / 2000]: Train Loss=5.7029, Val Cor=0.5362, Time=0.2720 sec\n",
      "Epoch [324 / 2000]: Train Loss=5.7561, Val Cor=0.5369, Time=0.2721 sec\n",
      "Epoch [325 / 2000]: Train Loss=5.6725, Val Cor=0.5119, Time=0.2721 sec\n",
      "Epoch [326 / 2000]: Train Loss=5.6530, Val Cor=0.5367, Time=0.2721 sec\n",
      "Epoch [327 / 2000]: Train Loss=5.6175, Val Cor=0.4896, Time=0.2721 sec\n",
      "Epoch [328 / 2000]: Train Loss=6.4695, Val Cor=-0.5068, Time=0.2723 sec\n",
      "Epoch [329 / 2000]: Train Loss=6.1178, Val Cor=-0.5154, Time=0.2723 sec\n",
      "Epoch [330 / 2000]: Train Loss=6.3386, Val Cor=-0.1567, Time=0.2721 sec\n",
      "Epoch [331 / 2000]: Train Loss=5.7578, Val Cor=0.5348, Time=0.2720 sec\n",
      "Epoch [332 / 2000]: Train Loss=5.6280, Val Cor=0.5015, Time=0.2721 sec\n",
      "Epoch [333 / 2000]: Train Loss=5.8916, Val Cor=0.5373, Time=0.2720 sec\n",
      "Epoch [334 / 2000]: Train Loss=5.8263, Val Cor=0.5353, Time=0.2721 sec\n",
      "Epoch [335 / 2000]: Train Loss=6.3122, Val Cor=0.3877, Time=0.2721 sec\n",
      "Epoch [336 / 2000]: Train Loss=6.4292, Val Cor=0.3983, Time=0.2727 sec\n",
      "Epoch [337 / 2000]: Train Loss=5.8423, Val Cor=0.5326, Time=0.2721 sec\n",
      "Epoch [338 / 2000]: Train Loss=6.0710, Val Cor=0.5330, Time=0.2725 sec\n",
      "Epoch [339 / 2000]: Train Loss=6.0700, Val Cor=0.5334, Time=0.2720 sec\n",
      "Epoch [340 / 2000]: Train Loss=5.8144, Val Cor=0.5335, Time=0.2721 sec\n",
      "Epoch [341 / 2000]: Train Loss=5.7945, Val Cor=0.5194, Time=0.2721 sec\n",
      "Epoch [342 / 2000]: Train Loss=5.6407, Val Cor=0.5080, Time=0.2721 sec\n",
      "Epoch [343 / 2000]: Train Loss=5.9796, Val Cor=0.4852, Time=0.2722 sec\n",
      "Epoch [344 / 2000]: Train Loss=5.8762, Val Cor=0.4902, Time=0.2727 sec\n",
      "Epoch [345 / 2000]: Train Loss=6.1300, Val Cor=0.5337, Time=0.2720 sec\n",
      "Epoch [346 / 2000]: Train Loss=5.7961, Val Cor=0.5272, Time=0.2722 sec\n",
      "Epoch [347 / 2000]: Train Loss=5.7932, Val Cor=0.5332, Time=0.2727 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [348 / 2000]: Train Loss=5.7245, Val Cor=0.5387, Time=0.2722 sec\n",
      "Epoch [349 / 2000]: Train Loss=5.6283, Val Cor=0.5310, Time=0.2720 sec\n",
      "Epoch [350 / 2000]: Train Loss=5.5754, Val Cor=0.5371, Time=0.2721 sec\n",
      "Epoch [351 / 2000]: Train Loss=5.4789, Val Cor=0.5144, Time=0.2722 sec\n",
      "Epoch [352 / 2000]: Train Loss=5.8913, Val Cor=0.4919, Time=0.2721 sec\n",
      "Epoch [353 / 2000]: Train Loss=5.8560, Val Cor=0.5350, Time=0.2722 sec\n",
      "Epoch [354 / 2000]: Train Loss=5.6651, Val Cor=0.5389, Time=0.2721 sec\n",
      "Epoch [355 / 2000]: Train Loss=5.8417, Val Cor=0.5383, Time=0.2719 sec\n",
      "Epoch [356 / 2000]: Train Loss=6.1205, Val Cor=-0.4375, Time=0.2721 sec\n",
      "Epoch [357 / 2000]: Train Loss=6.0605, Val Cor=-0.2893, Time=0.2775 sec\n",
      "Epoch [358 / 2000]: Train Loss=6.0399, Val Cor=0.5209, Time=0.2723 sec\n",
      "Epoch [359 / 2000]: Train Loss=5.9397, Val Cor=0.4620, Time=0.2721 sec\n",
      "Epoch [360 / 2000]: Train Loss=5.8899, Val Cor=0.5327, Time=0.2720 sec\n",
      "Epoch [361 / 2000]: Train Loss=5.9443, Val Cor=0.4214, Time=0.2721 sec\n",
      "Epoch [362 / 2000]: Train Loss=6.0099, Val Cor=0.4691, Time=0.2721 sec\n",
      "Epoch [363 / 2000]: Train Loss=5.6815, Val Cor=0.5368, Time=0.2720 sec\n",
      "Epoch [364 / 2000]: Train Loss=5.7536, Val Cor=0.4998, Time=0.2721 sec\n",
      "Epoch [365 / 2000]: Train Loss=5.8627, Val Cor=0.5372, Time=0.2720 sec\n",
      "Epoch [366 / 2000]: Train Loss=5.5970, Val Cor=0.5302, Time=0.2720 sec\n",
      "Epoch [367 / 2000]: Train Loss=5.6484, Val Cor=0.5384, Time=0.2721 sec\n",
      "Epoch [368 / 2000]: Train Loss=5.6282, Val Cor=0.5319, Time=0.2720 sec\n",
      "Epoch [369 / 2000]: Train Loss=5.8238, Val Cor=0.5399, Time=0.2721 sec\n",
      "Epoch [370 / 2000]: Train Loss=5.6658, Val Cor=0.5417, Time=0.2720 sec\n",
      "Epoch [371 / 2000]: Train Loss=5.5339, Val Cor=0.5382, Time=0.2720 sec\n",
      "Epoch [372 / 2000]: Train Loss=5.7470, Val Cor=0.4311, Time=0.2720 sec\n",
      "Epoch [373 / 2000]: Train Loss=5.7735, Val Cor=0.4415, Time=0.2720 sec\n",
      "Epoch [374 / 2000]: Train Loss=5.7469, Val Cor=0.5433, Time=0.2720 sec\n",
      "Epoch [375 / 2000]: Train Loss=5.5260, Val Cor=0.5335, Time=0.2719 sec\n",
      "Epoch [376 / 2000]: Train Loss=5.8716, Val Cor=0.5423, Time=0.2721 sec\n",
      "Epoch [377 / 2000]: Train Loss=5.6482, Val Cor=0.5383, Time=0.2720 sec\n",
      "Epoch [378 / 2000]: Train Loss=5.9292, Val Cor=0.5240, Time=0.2721 sec\n",
      "Epoch [379 / 2000]: Train Loss=5.7601, Val Cor=0.5295, Time=0.2720 sec\n",
      "Epoch [380 / 2000]: Train Loss=5.7355, Val Cor=0.5392, Time=0.2720 sec\n",
      "Epoch [381 / 2000]: Train Loss=5.5312, Val Cor=0.4909, Time=0.2716 sec\n",
      "Epoch [382 / 2000]: Train Loss=5.5460, Val Cor=0.5272, Time=0.2720 sec\n",
      "Epoch [383 / 2000]: Train Loss=5.5290, Val Cor=0.5136, Time=0.2720 sec\n",
      "Epoch [384 / 2000]: Train Loss=5.6178, Val Cor=0.4809, Time=0.2721 sec\n",
      "Epoch [385 / 2000]: Train Loss=5.7333, Val Cor=0.5315, Time=0.2721 sec\n",
      "Epoch [386 / 2000]: Train Loss=5.5810, Val Cor=0.5484, Time=0.2720 sec\n",
      "Epoch [387 / 2000]: Train Loss=5.7388, Val Cor=0.5180, Time=0.2719 sec\n",
      "Epoch [388 / 2000]: Train Loss=5.6719, Val Cor=0.5488, Time=0.2720 sec\n",
      "Epoch [389 / 2000]: Train Loss=5.5906, Val Cor=0.4380, Time=0.2719 sec\n",
      "Epoch [390 / 2000]: Train Loss=5.7821, Val Cor=0.5478, Time=0.2720 sec\n",
      "Epoch [391 / 2000]: Train Loss=5.3049, Val Cor=0.5334, Time=0.2720 sec\n",
      "Epoch [392 / 2000]: Train Loss=5.8234, Val Cor=0.5448, Time=0.2720 sec\n",
      "Epoch [393 / 2000]: Train Loss=5.6920, Val Cor=0.5064, Time=0.2720 sec\n",
      "Epoch [394 / 2000]: Train Loss=5.7497, Val Cor=0.5345, Time=0.2769 sec\n",
      "Epoch [395 / 2000]: Train Loss=5.7069, Val Cor=0.5158, Time=0.2721 sec\n",
      "Epoch [396 / 2000]: Train Loss=6.0829, Val Cor=0.5188, Time=0.2721 sec\n",
      "Epoch [397 / 2000]: Train Loss=5.6898, Val Cor=0.5505, Time=0.2716 sec\n",
      "Epoch [398 / 2000]: Train Loss=5.6009, Val Cor=0.5511, Time=0.2719 sec\n",
      "Epoch [399 / 2000]: Train Loss=5.9633, Val Cor=0.5491, Time=0.2719 sec\n",
      "Epoch [400 / 2000]: Train Loss=5.6564, Val Cor=0.3336, Time=0.2721 sec\n",
      "Epoch [401 / 2000]: Train Loss=5.5454, Val Cor=0.5284, Time=0.2720 sec\n",
      "Epoch [402 / 2000]: Train Loss=5.5148, Val Cor=0.5588, Time=0.2720 sec\n",
      "Epoch [403 / 2000]: Train Loss=5.5217, Val Cor=0.5456, Time=0.2719 sec\n",
      "Epoch [404 / 2000]: Train Loss=5.7338, Val Cor=0.5532, Time=0.2721 sec\n",
      "Epoch [405 / 2000]: Train Loss=5.6583, Val Cor=0.5336, Time=0.2720 sec\n",
      "Epoch [406 / 2000]: Train Loss=5.6109, Val Cor=0.5585, Time=0.2722 sec\n",
      "Epoch [407 / 2000]: Train Loss=5.5612, Val Cor=0.5314, Time=0.2722 sec\n",
      "Epoch [408 / 2000]: Train Loss=5.9733, Val Cor=0.4348, Time=0.2727 sec\n",
      "Epoch [409 / 2000]: Train Loss=5.5936, Val Cor=0.5568, Time=0.2721 sec\n",
      "Epoch [410 / 2000]: Train Loss=5.6430, Val Cor=0.5035, Time=0.2726 sec\n",
      "Epoch [411 / 2000]: Train Loss=5.7192, Val Cor=0.5555, Time=0.2721 sec\n",
      "Epoch [412 / 2000]: Train Loss=5.5212, Val Cor=0.5071, Time=0.2723 sec\n",
      "Epoch [413 / 2000]: Train Loss=5.7695, Val Cor=0.5570, Time=0.2752 sec\n",
      "Epoch [414 / 2000]: Train Loss=5.7251, Val Cor=0.5448, Time=0.2726 sec\n",
      "Epoch [415 / 2000]: Train Loss=5.8503, Val Cor=0.3606, Time=0.2722 sec\n",
      "Epoch [416 / 2000]: Train Loss=5.7638, Val Cor=0.5557, Time=0.2721 sec\n",
      "Epoch [417 / 2000]: Train Loss=5.6410, Val Cor=0.5535, Time=0.2721 sec\n",
      "Epoch [418 / 2000]: Train Loss=5.5237, Val Cor=0.5438, Time=0.2733 sec\n",
      "Epoch [419 / 2000]: Train Loss=5.6811, Val Cor=0.5501, Time=0.2723 sec\n",
      "Epoch [420 / 2000]: Train Loss=5.3858, Val Cor=0.5588, Time=0.2723 sec\n",
      "Epoch [421 / 2000]: Train Loss=5.4754, Val Cor=0.5348, Time=0.2727 sec\n",
      "Epoch [422 / 2000]: Train Loss=5.4740, Val Cor=0.5521, Time=0.2722 sec\n",
      "Epoch [423 / 2000]: Train Loss=5.8837, Val Cor=0.5028, Time=0.2727 sec\n",
      "Epoch [424 / 2000]: Train Loss=5.7807, Val Cor=0.4699, Time=0.2724 sec\n",
      "Epoch [425 / 2000]: Train Loss=5.6973, Val Cor=0.5565, Time=0.2722 sec\n",
      "Epoch [426 / 2000]: Train Loss=5.8515, Val Cor=0.5426, Time=0.2723 sec\n",
      "Epoch [427 / 2000]: Train Loss=5.6033, Val Cor=0.5574, Time=0.2722 sec\n",
      "Epoch [428 / 2000]: Train Loss=6.0101, Val Cor=0.5536, Time=0.2724 sec\n",
      "Epoch [429 / 2000]: Train Loss=5.6921, Val Cor=0.5302, Time=0.2722 sec\n",
      "Epoch [430 / 2000]: Train Loss=5.5433, Val Cor=0.5321, Time=0.2728 sec\n",
      "Epoch [431 / 2000]: Train Loss=5.5989, Val Cor=0.5450, Time=0.2722 sec\n",
      "Epoch [432 / 2000]: Train Loss=5.6474, Val Cor=0.5596, Time=0.2747 sec\n",
      "Epoch [433 / 2000]: Train Loss=5.5461, Val Cor=0.5551, Time=0.2722 sec\n",
      "Epoch [434 / 2000]: Train Loss=5.4179, Val Cor=0.5588, Time=0.2722 sec\n",
      "Epoch [435 / 2000]: Train Loss=5.6376, Val Cor=0.5566, Time=0.2722 sec\n",
      "Epoch [436 / 2000]: Train Loss=5.7891, Val Cor=0.5207, Time=0.2727 sec\n",
      "Epoch [437 / 2000]: Train Loss=5.5567, Val Cor=0.5612, Time=0.2727 sec\n",
      "Epoch [438 / 2000]: Train Loss=5.9251, Val Cor=0.5509, Time=0.2721 sec\n",
      "Epoch [439 / 2000]: Train Loss=5.4581, Val Cor=0.5602, Time=0.2728 sec\n",
      "Epoch [440 / 2000]: Train Loss=5.7329, Val Cor=0.5614, Time=0.2734 sec\n",
      "Epoch [441 / 2000]: Train Loss=5.4626, Val Cor=0.5666, Time=0.2744 sec\n",
      "Epoch [442 / 2000]: Train Loss=5.6288, Val Cor=0.5552, Time=0.2727 sec\n",
      "Epoch [443 / 2000]: Train Loss=5.9482, Val Cor=0.1689, Time=0.2734 sec\n",
      "Epoch [444 / 2000]: Train Loss=6.1088, Val Cor=0.5288, Time=0.2727 sec\n",
      "Epoch [445 / 2000]: Train Loss=5.4686, Val Cor=0.5390, Time=0.2728 sec\n",
      "Epoch [446 / 2000]: Train Loss=5.4234, Val Cor=0.5011, Time=0.2730 sec\n",
      "Epoch [447 / 2000]: Train Loss=5.8580, Val Cor=0.5524, Time=0.2727 sec\n",
      "Epoch [448 / 2000]: Train Loss=5.4133, Val Cor=0.5658, Time=0.2741 sec\n",
      "Epoch [449 / 2000]: Train Loss=5.5659, Val Cor=0.5597, Time=0.2729 sec\n",
      "Epoch [450 / 2000]: Train Loss=5.4710, Val Cor=0.5649, Time=0.2749 sec\n",
      "Epoch [451 / 2000]: Train Loss=5.4480, Val Cor=0.5628, Time=0.2728 sec\n",
      "Epoch [452 / 2000]: Train Loss=5.4690, Val Cor=0.4605, Time=0.2779 sec\n",
      "Epoch [453 / 2000]: Train Loss=5.8472, Val Cor=0.5665, Time=0.2727 sec\n",
      "Epoch [454 / 2000]: Train Loss=5.8259, Val Cor=0.5621, Time=0.2726 sec\n",
      "Epoch [455 / 2000]: Train Loss=5.4782, Val Cor=0.5472, Time=0.2729 sec\n",
      "Epoch [456 / 2000]: Train Loss=5.5925, Val Cor=0.4852, Time=0.2729 sec\n",
      "Epoch [457 / 2000]: Train Loss=5.4128, Val Cor=0.5714, Time=0.2733 sec\n",
      "Epoch [458 / 2000]: Train Loss=5.6268, Val Cor=0.5537, Time=0.2733 sec\n",
      "Epoch [459 / 2000]: Train Loss=5.4467, Val Cor=0.5628, Time=0.2728 sec\n",
      "Epoch [460 / 2000]: Train Loss=5.7441, Val Cor=0.5711, Time=0.2732 sec\n",
      "Epoch [461 / 2000]: Train Loss=5.7612, Val Cor=0.5628, Time=0.2735 sec\n",
      "Epoch [462 / 2000]: Train Loss=5.8380, Val Cor=0.5653, Time=0.2733 sec\n",
      "Epoch [463 / 2000]: Train Loss=5.9801, Val Cor=0.5712, Time=0.2729 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [464 / 2000]: Train Loss=6.0581, Val Cor=0.5698, Time=0.2731 sec\n",
      "Epoch [465 / 2000]: Train Loss=5.6277, Val Cor=0.5279, Time=0.2727 sec\n",
      "Epoch [466 / 2000]: Train Loss=5.7057, Val Cor=0.5495, Time=0.2723 sec\n",
      "Epoch [467 / 2000]: Train Loss=5.6405, Val Cor=0.5710, Time=0.2732 sec\n",
      "Epoch [468 / 2000]: Train Loss=5.9102, Val Cor=0.5652, Time=0.2731 sec\n",
      "Epoch [469 / 2000]: Train Loss=5.7314, Val Cor=0.4948, Time=0.2748 sec\n",
      "Epoch [470 / 2000]: Train Loss=5.8253, Val Cor=0.5663, Time=0.2728 sec\n",
      "Epoch [471 / 2000]: Train Loss=5.4509, Val Cor=0.5647, Time=0.2728 sec\n",
      "Epoch [472 / 2000]: Train Loss=5.6447, Val Cor=0.5681, Time=0.2723 sec\n",
      "Epoch [473 / 2000]: Train Loss=5.5187, Val Cor=0.5699, Time=0.2728 sec\n",
      "Epoch [474 / 2000]: Train Loss=5.5656, Val Cor=0.5705, Time=0.2731 sec\n",
      "Epoch [475 / 2000]: Train Loss=5.4752, Val Cor=0.5640, Time=0.2730 sec\n",
      "Epoch [476 / 2000]: Train Loss=5.6437, Val Cor=0.5701, Time=0.2740 sec\n",
      "Epoch [477 / 2000]: Train Loss=5.5893, Val Cor=0.5746, Time=0.2728 sec\n",
      "Epoch [478 / 2000]: Train Loss=5.7574, Val Cor=0.5658, Time=0.2726 sec\n",
      "Epoch [479 / 2000]: Train Loss=5.6435, Val Cor=0.5695, Time=0.2734 sec\n",
      "Epoch [480 / 2000]: Train Loss=5.4654, Val Cor=0.5802, Time=0.2728 sec\n",
      "Epoch [481 / 2000]: Train Loss=5.5318, Val Cor=0.5663, Time=0.2727 sec\n",
      "Epoch [482 / 2000]: Train Loss=5.8845, Val Cor=0.5669, Time=0.2729 sec\n",
      "Epoch [483 / 2000]: Train Loss=5.6051, Val Cor=0.5689, Time=0.2728 sec\n",
      "Epoch [484 / 2000]: Train Loss=5.6926, Val Cor=0.5771, Time=0.2728 sec\n",
      "Epoch [485 / 2000]: Train Loss=5.5307, Val Cor=0.5766, Time=0.2730 sec\n",
      "Epoch [486 / 2000]: Train Loss=5.5603, Val Cor=0.5334, Time=0.2888 sec\n",
      "Epoch [487 / 2000]: Train Loss=5.4270, Val Cor=0.5741, Time=0.2727 sec\n",
      "Epoch [488 / 2000]: Train Loss=5.5420, Val Cor=0.5529, Time=0.2734 sec\n",
      "Epoch [489 / 2000]: Train Loss=5.4803, Val Cor=0.5158, Time=0.2726 sec\n",
      "Epoch [490 / 2000]: Train Loss=5.5792, Val Cor=0.5717, Time=0.2728 sec\n",
      "Epoch [491 / 2000]: Train Loss=5.5528, Val Cor=0.5772, Time=0.2723 sec\n",
      "Epoch [492 / 2000]: Train Loss=5.7067, Val Cor=0.5717, Time=0.2728 sec\n",
      "Epoch [493 / 2000]: Train Loss=5.4589, Val Cor=0.5750, Time=0.2728 sec\n",
      "Epoch [494 / 2000]: Train Loss=5.3922, Val Cor=0.5723, Time=0.2728 sec\n",
      "Epoch [495 / 2000]: Train Loss=6.1080, Val Cor=0.5740, Time=0.2728 sec\n",
      "Epoch [496 / 2000]: Train Loss=5.4569, Val Cor=0.5644, Time=0.2728 sec\n",
      "Epoch [497 / 2000]: Train Loss=5.4094, Val Cor=0.5833, Time=0.2723 sec\n",
      "Epoch [498 / 2000]: Train Loss=5.6393, Val Cor=0.5706, Time=0.2728 sec\n",
      "Epoch [499 / 2000]: Train Loss=5.6976, Val Cor=0.5699, Time=0.2732 sec\n",
      "Epoch [500 / 2000]: Train Loss=5.3790, Val Cor=0.5784, Time=0.2728 sec\n",
      "Epoch [501 / 2000]: Train Loss=5.6174, Val Cor=0.5784, Time=0.2730 sec\n",
      "Epoch [502 / 2000]: Train Loss=5.7867, Val Cor=0.5749, Time=0.2730 sec\n",
      "Epoch [503 / 2000]: Train Loss=5.3888, Val Cor=0.5763, Time=0.2745 sec\n",
      "Epoch [504 / 2000]: Train Loss=5.6960, Val Cor=0.4532, Time=0.2737 sec\n",
      "Epoch [505 / 2000]: Train Loss=5.8409, Val Cor=0.0692, Time=0.2735 sec\n",
      "Epoch [506 / 2000]: Train Loss=5.6179, Val Cor=0.5683, Time=0.2739 sec\n",
      "Epoch [507 / 2000]: Train Loss=5.8647, Val Cor=0.5641, Time=0.2751 sec\n",
      "Epoch [508 / 2000]: Train Loss=5.4758, Val Cor=0.5855, Time=0.2731 sec\n",
      "Epoch [509 / 2000]: Train Loss=5.5280, Val Cor=0.5651, Time=0.2728 sec\n",
      "Epoch [510 / 2000]: Train Loss=5.7621, Val Cor=0.5768, Time=0.2735 sec\n",
      "Epoch [511 / 2000]: Train Loss=5.4213, Val Cor=0.5604, Time=0.2729 sec\n",
      "Epoch [512 / 2000]: Train Loss=5.3797, Val Cor=0.5803, Time=0.2730 sec\n",
      "Epoch [513 / 2000]: Train Loss=5.8569, Val Cor=0.5833, Time=0.2730 sec\n",
      "Epoch [514 / 2000]: Train Loss=5.5234, Val Cor=0.5539, Time=0.2729 sec\n",
      "Epoch [515 / 2000]: Train Loss=5.5201, Val Cor=0.5380, Time=0.2729 sec\n",
      "Epoch [516 / 2000]: Train Loss=5.4322, Val Cor=0.5486, Time=0.2731 sec\n",
      "Epoch [517 / 2000]: Train Loss=5.4423, Val Cor=0.5644, Time=0.2729 sec\n",
      "Epoch [518 / 2000]: Train Loss=5.3490, Val Cor=0.5140, Time=0.2729 sec\n",
      "Epoch [519 / 2000]: Train Loss=5.6444, Val Cor=0.3497, Time=0.2729 sec\n",
      "Epoch [520 / 2000]: Train Loss=5.8087, Val Cor=0.5747, Time=0.2729 sec\n",
      "Epoch [521 / 2000]: Train Loss=5.4324, Val Cor=0.5787, Time=0.2729 sec\n",
      "Epoch [522 / 2000]: Train Loss=5.4757, Val Cor=0.5869, Time=0.2728 sec\n",
      "Epoch [523 / 2000]: Train Loss=5.3601, Val Cor=0.5774, Time=0.2728 sec\n",
      "Epoch [524 / 2000]: Train Loss=5.2764, Val Cor=0.5882, Time=0.2742 sec\n",
      "Epoch [525 / 2000]: Train Loss=5.3290, Val Cor=0.5887, Time=0.2728 sec\n",
      "Epoch [526 / 2000]: Train Loss=6.6571, Val Cor=0.5791, Time=0.2729 sec\n",
      "Epoch [527 / 2000]: Train Loss=5.7572, Val Cor=0.5669, Time=0.2730 sec\n",
      "Epoch [528 / 2000]: Train Loss=5.2762, Val Cor=0.5783, Time=0.2729 sec\n",
      "Epoch [529 / 2000]: Train Loss=5.3061, Val Cor=0.5800, Time=0.2737 sec\n",
      "Epoch [530 / 2000]: Train Loss=5.5703, Val Cor=0.5876, Time=0.2729 sec\n",
      "Epoch [531 / 2000]: Train Loss=5.4891, Val Cor=0.5880, Time=0.2732 sec\n",
      "Epoch [532 / 2000]: Train Loss=5.5861, Val Cor=0.5719, Time=0.2736 sec\n",
      "Epoch [533 / 2000]: Train Loss=5.7691, Val Cor=0.5882, Time=0.2740 sec\n",
      "Epoch [534 / 2000]: Train Loss=5.7351, Val Cor=0.5864, Time=0.2759 sec\n",
      "Epoch [535 / 2000]: Train Loss=5.4440, Val Cor=0.5857, Time=0.2742 sec\n",
      "Epoch [536 / 2000]: Train Loss=5.3166, Val Cor=0.5878, Time=0.2736 sec\n",
      "Epoch [537 / 2000]: Train Loss=5.7724, Val Cor=0.5894, Time=0.2730 sec\n",
      "Epoch [538 / 2000]: Train Loss=5.9591, Val Cor=0.5779, Time=0.2734 sec\n",
      "Epoch [539 / 2000]: Train Loss=5.6146, Val Cor=0.5845, Time=0.2729 sec\n",
      "Epoch [540 / 2000]: Train Loss=5.5388, Val Cor=0.4894, Time=0.2729 sec\n",
      "Epoch [541 / 2000]: Train Loss=5.3949, Val Cor=0.5647, Time=0.2728 sec\n",
      "Epoch [542 / 2000]: Train Loss=5.4003, Val Cor=0.3902, Time=0.2730 sec\n",
      "Epoch [543 / 2000]: Train Loss=5.7149, Val Cor=0.5815, Time=0.2729 sec\n",
      "Epoch [544 / 2000]: Train Loss=5.3748, Val Cor=0.5895, Time=0.2742 sec\n",
      "Epoch [545 / 2000]: Train Loss=5.3342, Val Cor=0.5693, Time=0.2723 sec\n",
      "Epoch [546 / 2000]: Train Loss=5.4212, Val Cor=0.5765, Time=0.2730 sec\n",
      "Epoch [547 / 2000]: Train Loss=5.7423, Val Cor=0.5813, Time=0.2729 sec\n",
      "Epoch [548 / 2000]: Train Loss=5.6559, Val Cor=0.5870, Time=0.2730 sec\n",
      "Epoch [549 / 2000]: Train Loss=5.3559, Val Cor=0.5800, Time=0.2729 sec\n",
      "Epoch [550 / 2000]: Train Loss=5.3266, Val Cor=0.5806, Time=0.2734 sec\n",
      "Epoch [551 / 2000]: Train Loss=5.4707, Val Cor=0.5845, Time=0.2731 sec\n",
      "Epoch [552 / 2000]: Train Loss=5.4075, Val Cor=0.5886, Time=0.2729 sec\n",
      "Epoch [553 / 2000]: Train Loss=5.5409, Val Cor=0.5677, Time=0.2734 sec\n",
      "Epoch [554 / 2000]: Train Loss=5.5975, Val Cor=0.5815, Time=0.2731 sec\n",
      "Epoch [555 / 2000]: Train Loss=5.3791, Val Cor=0.5548, Time=0.2735 sec\n",
      "Epoch [556 / 2000]: Train Loss=5.2630, Val Cor=0.5958, Time=0.2729 sec\n",
      "Epoch [557 / 2000]: Train Loss=5.7249, Val Cor=0.5784, Time=0.2728 sec\n",
      "Epoch [558 / 2000]: Train Loss=5.3548, Val Cor=0.5811, Time=0.2735 sec\n",
      "Epoch [559 / 2000]: Train Loss=5.2609, Val Cor=0.5987, Time=0.2729 sec\n",
      "Epoch [560 / 2000]: Train Loss=5.4389, Val Cor=0.5774, Time=0.2738 sec\n",
      "Epoch [561 / 2000]: Train Loss=5.4616, Val Cor=0.5948, Time=0.2731 sec\n",
      "Epoch [562 / 2000]: Train Loss=5.2577, Val Cor=0.5973, Time=0.2730 sec\n",
      "Epoch [563 / 2000]: Train Loss=5.3121, Val Cor=0.5927, Time=0.2735 sec\n",
      "Epoch [564 / 2000]: Train Loss=5.4120, Val Cor=0.5262, Time=0.2745 sec\n",
      "Epoch [565 / 2000]: Train Loss=5.9392, Val Cor=-0.2471, Time=0.2736 sec\n",
      "Epoch [566 / 2000]: Train Loss=5.4780, Val Cor=0.5377, Time=0.2736 sec\n",
      "Epoch [567 / 2000]: Train Loss=5.4455, Val Cor=0.5927, Time=0.2736 sec\n",
      "Epoch [568 / 2000]: Train Loss=5.6470, Val Cor=0.5817, Time=0.2736 sec\n",
      "Epoch [569 / 2000]: Train Loss=5.3652, Val Cor=0.5956, Time=0.2735 sec\n",
      "Epoch [570 / 2000]: Train Loss=5.4623, Val Cor=0.5832, Time=0.2730 sec\n",
      "Epoch [571 / 2000]: Train Loss=5.5190, Val Cor=0.5747, Time=0.2731 sec\n",
      "Epoch [572 / 2000]: Train Loss=5.3269, Val Cor=0.5274, Time=0.2730 sec\n",
      "Epoch [573 / 2000]: Train Loss=5.3063, Val Cor=0.5535, Time=0.2729 sec\n",
      "Epoch [574 / 2000]: Train Loss=5.4540, Val Cor=0.4143, Time=0.2728 sec\n",
      "Epoch [575 / 2000]: Train Loss=5.4212, Val Cor=0.5868, Time=0.2730 sec\n",
      "Epoch [576 / 2000]: Train Loss=5.2406, Val Cor=0.5942, Time=0.2729 sec\n",
      "Epoch [577 / 2000]: Train Loss=5.3079, Val Cor=0.5782, Time=0.2729 sec\n",
      "Epoch [578 / 2000]: Train Loss=5.4027, Val Cor=0.6009, Time=0.2725 sec\n",
      "Epoch [579 / 2000]: Train Loss=5.4905, Val Cor=0.5663, Time=0.2729 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [580 / 2000]: Train Loss=5.9220, Val Cor=0.5932, Time=0.2729 sec\n",
      "Epoch [581 / 2000]: Train Loss=5.7634, Val Cor=0.5791, Time=0.2733 sec\n",
      "Epoch [582 / 2000]: Train Loss=5.4715, Val Cor=0.5252, Time=0.2742 sec\n",
      "Epoch [583 / 2000]: Train Loss=5.7112, Val Cor=0.5984, Time=0.2735 sec\n",
      "Epoch [584 / 2000]: Train Loss=5.4730, Val Cor=0.5840, Time=0.2728 sec\n",
      "Epoch [585 / 2000]: Train Loss=5.6531, Val Cor=0.5797, Time=0.2729 sec\n",
      "Epoch [586 / 2000]: Train Loss=5.8191, Val Cor=0.4914, Time=0.2735 sec\n",
      "Epoch [587 / 2000]: Train Loss=5.7553, Val Cor=0.5824, Time=0.2735 sec\n",
      "Epoch [588 / 2000]: Train Loss=5.5726, Val Cor=0.5569, Time=0.2728 sec\n",
      "Epoch [589 / 2000]: Train Loss=5.3669, Val Cor=0.5822, Time=0.2728 sec\n",
      "Epoch [590 / 2000]: Train Loss=5.3623, Val Cor=0.4003, Time=0.2728 sec\n",
      "Epoch [591 / 2000]: Train Loss=5.3792, Val Cor=0.5944, Time=0.2729 sec\n",
      "Epoch [592 / 2000]: Train Loss=5.3581, Val Cor=0.5876, Time=0.2728 sec\n",
      "Epoch [593 / 2000]: Train Loss=5.5470, Val Cor=0.5918, Time=0.1846 sec\n",
      "Epoch [594 / 2000]: Train Loss=5.3610, Val Cor=0.5905, Time=0.1657 sec\n",
      "Epoch [595 / 2000]: Train Loss=5.4000, Val Cor=0.5953, Time=0.1644 sec\n",
      "Epoch [596 / 2000]: Train Loss=5.3066, Val Cor=0.6010, Time=0.1642 sec\n",
      "Epoch [597 / 2000]: Train Loss=5.3376, Val Cor=0.5868, Time=0.1635 sec\n",
      "Epoch [598 / 2000]: Train Loss=5.7058, Val Cor=0.5940, Time=0.1632 sec\n",
      "Epoch [599 / 2000]: Train Loss=5.4635, Val Cor=0.5948, Time=0.1629 sec\n",
      "Epoch [600 / 2000]: Train Loss=5.5304, Val Cor=0.5732, Time=0.1632 sec\n",
      "Epoch [601 / 2000]: Train Loss=5.4016, Val Cor=0.5599, Time=0.1610 sec\n",
      "Epoch [602 / 2000]: Train Loss=5.3681, Val Cor=0.5958, Time=0.1613 sec\n",
      "Epoch [603 / 2000]: Train Loss=5.3281, Val Cor=0.5885, Time=0.1622 sec\n",
      "Epoch [604 / 2000]: Train Loss=5.3947, Val Cor=0.5872, Time=0.1626 sec\n",
      "Epoch [605 / 2000]: Train Loss=5.5964, Val Cor=0.5988, Time=0.1623 sec\n",
      "Epoch [606 / 2000]: Train Loss=5.3853, Val Cor=0.5761, Time=0.1624 sec\n",
      "Epoch [607 / 2000]: Train Loss=5.2979, Val Cor=0.5908, Time=0.1615 sec\n",
      "Epoch [608 / 2000]: Train Loss=5.3934, Val Cor=0.5913, Time=0.1615 sec\n",
      "Epoch [609 / 2000]: Train Loss=5.5641, Val Cor=0.5838, Time=0.1612 sec\n",
      "Epoch [610 / 2000]: Train Loss=5.6875, Val Cor=0.5895, Time=0.1610 sec\n",
      "Epoch [611 / 2000]: Train Loss=5.2406, Val Cor=0.5963, Time=0.1607 sec\n",
      "Epoch [612 / 2000]: Train Loss=5.3804, Val Cor=0.5956, Time=0.1621 sec\n",
      "Epoch [613 / 2000]: Train Loss=5.3425, Val Cor=0.5260, Time=0.1617 sec\n",
      "Epoch [614 / 2000]: Train Loss=5.4809, Val Cor=0.6063, Time=0.1620 sec\n",
      "Epoch [615 / 2000]: Train Loss=5.4745, Val Cor=0.4943, Time=0.1615 sec\n",
      "Epoch [616 / 2000]: Train Loss=5.5076, Val Cor=0.6031, Time=0.1622 sec\n",
      "Epoch [617 / 2000]: Train Loss=5.2127, Val Cor=0.5996, Time=0.1616 sec\n",
      "Epoch [618 / 2000]: Train Loss=5.2391, Val Cor=0.5774, Time=0.1617 sec\n",
      "Epoch [619 / 2000]: Train Loss=5.2667, Val Cor=0.6004, Time=0.1599 sec\n",
      "Epoch [620 / 2000]: Train Loss=5.2351, Val Cor=0.6013, Time=0.1602 sec\n",
      "Epoch [621 / 2000]: Train Loss=5.2295, Val Cor=0.5981, Time=0.1605 sec\n",
      "Epoch [622 / 2000]: Train Loss=5.4812, Val Cor=0.5957, Time=0.1611 sec\n",
      "Epoch [623 / 2000]: Train Loss=5.2913, Val Cor=0.5884, Time=0.1615 sec\n",
      "Epoch [624 / 2000]: Train Loss=5.4839, Val Cor=0.5488, Time=0.1616 sec\n",
      "Epoch [625 / 2000]: Train Loss=5.4290, Val Cor=0.5966, Time=0.1617 sec\n",
      "Epoch [626 / 2000]: Train Loss=5.2549, Val Cor=0.6045, Time=0.1624 sec\n",
      "Epoch [627 / 2000]: Train Loss=5.2390, Val Cor=0.5978, Time=0.1619 sec\n",
      "Epoch [628 / 2000]: Train Loss=5.3976, Val Cor=0.6024, Time=0.1617 sec\n",
      "Epoch [629 / 2000]: Train Loss=5.3483, Val Cor=0.5903, Time=0.1606 sec\n",
      "Epoch [630 / 2000]: Train Loss=5.4476, Val Cor=0.5942, Time=0.1607 sec\n",
      "Epoch [631 / 2000]: Train Loss=5.8057, Val Cor=0.5944, Time=0.1607 sec\n",
      "Epoch [632 / 2000]: Train Loss=5.3818, Val Cor=0.5960, Time=0.1612 sec\n",
      "Epoch [633 / 2000]: Train Loss=5.4415, Val Cor=0.5758, Time=0.1611 sec\n",
      "Epoch [634 / 2000]: Train Loss=5.6325, Val Cor=0.4066, Time=0.1625 sec\n",
      "Epoch [635 / 2000]: Train Loss=5.4169, Val Cor=0.6001, Time=0.1622 sec\n",
      "Epoch [636 / 2000]: Train Loss=5.3826, Val Cor=0.5998, Time=0.1626 sec\n",
      "Epoch [637 / 2000]: Train Loss=5.2571, Val Cor=0.5887, Time=0.1625 sec\n",
      "Epoch [638 / 2000]: Train Loss=5.5082, Val Cor=0.6032, Time=0.1627 sec\n",
      "Epoch [639 / 2000]: Train Loss=5.4489, Val Cor=0.6043, Time=0.1609 sec\n",
      "Epoch [640 / 2000]: Train Loss=5.6316, Val Cor=-0.4872, Time=0.1616 sec\n",
      "Epoch [641 / 2000]: Train Loss=5.5910, Val Cor=0.5936, Time=0.1610 sec\n",
      "Epoch [642 / 2000]: Train Loss=5.4322, Val Cor=0.5975, Time=0.1612 sec\n",
      "Epoch [643 / 2000]: Train Loss=5.5328, Val Cor=0.5940, Time=0.1627 sec\n",
      "Epoch [644 / 2000]: Train Loss=5.2946, Val Cor=0.6054, Time=0.1644 sec\n",
      "Epoch [645 / 2000]: Train Loss=5.0514, Val Cor=0.6032, Time=0.1640 sec\n",
      "Epoch [646 / 2000]: Train Loss=5.3114, Val Cor=0.3736, Time=0.1640 sec\n",
      "Epoch [647 / 2000]: Train Loss=5.3671, Val Cor=0.6101, Time=0.1624 sec\n",
      "Epoch [648 / 2000]: Train Loss=5.3704, Val Cor=0.5987, Time=0.1622 sec\n",
      "Epoch [649 / 2000]: Train Loss=5.5166, Val Cor=0.6094, Time=0.1626 sec\n",
      "Epoch [650 / 2000]: Train Loss=5.3425, Val Cor=0.6038, Time=0.1622 sec\n",
      "Epoch [651 / 2000]: Train Loss=5.2801, Val Cor=0.6088, Time=0.1617 sec\n",
      "Epoch [652 / 2000]: Train Loss=5.3438, Val Cor=0.4518, Time=0.1611 sec\n",
      "Epoch [653 / 2000]: Train Loss=5.4913, Val Cor=0.6097, Time=0.1600 sec\n",
      "Epoch [654 / 2000]: Train Loss=5.4101, Val Cor=0.6047, Time=0.1607 sec\n",
      "Epoch [655 / 2000]: Train Loss=5.2791, Val Cor=-0.0593, Time=0.1606 sec\n",
      "Epoch [656 / 2000]: Train Loss=5.4285, Val Cor=0.5940, Time=0.1611 sec\n",
      "Epoch [657 / 2000]: Train Loss=5.4734, Val Cor=0.5790, Time=0.1617 sec\n",
      "Epoch [658 / 2000]: Train Loss=5.1307, Val Cor=0.6068, Time=0.1619 sec\n",
      "Epoch [659 / 2000]: Train Loss=5.3464, Val Cor=0.6023, Time=0.1618 sec\n",
      "Epoch [660 / 2000]: Train Loss=5.2156, Val Cor=0.6116, Time=0.1619 sec\n",
      "Epoch [661 / 2000]: Train Loss=5.1046, Val Cor=0.6057, Time=0.1611 sec\n",
      "Epoch [662 / 2000]: Train Loss=5.0677, Val Cor=0.6091, Time=0.1605 sec\n",
      "Epoch [663 / 2000]: Train Loss=5.2405, Val Cor=0.5880, Time=0.1602 sec\n",
      "Epoch [664 / 2000]: Train Loss=5.6067, Val Cor=0.6030, Time=0.1607 sec\n",
      "Epoch [665 / 2000]: Train Loss=5.3157, Val Cor=0.5973, Time=0.1605 sec\n",
      "Epoch [666 / 2000]: Train Loss=5.3881, Val Cor=0.6093, Time=0.1609 sec\n",
      "Epoch [667 / 2000]: Train Loss=5.3001, Val Cor=0.5982, Time=0.1613 sec\n",
      "Epoch [668 / 2000]: Train Loss=5.2418, Val Cor=0.6008, Time=0.1613 sec\n",
      "Epoch [669 / 2000]: Train Loss=5.3434, Val Cor=0.6046, Time=0.1621 sec\n",
      "Epoch [670 / 2000]: Train Loss=5.3018, Val Cor=0.6028, Time=0.1621 sec\n",
      "Epoch [671 / 2000]: Train Loss=5.4578, Val Cor=0.6009, Time=0.1615 sec\n",
      "Epoch [672 / 2000]: Train Loss=5.2356, Val Cor=0.6049, Time=0.1609 sec\n",
      "Epoch [673 / 2000]: Train Loss=5.3223, Val Cor=0.6109, Time=0.1606 sec\n",
      "Epoch [674 / 2000]: Train Loss=6.4925, Val Cor=-0.5079, Time=0.1607 sec\n",
      "Epoch [675 / 2000]: Train Loss=8.0467, Val Cor=-0.3861, Time=0.1618 sec\n",
      "Epoch [676 / 2000]: Train Loss=7.1663, Val Cor=-0.4927, Time=0.1628 sec\n",
      "Epoch [677 / 2000]: Train Loss=6.2550, Val Cor=-0.6032, Time=0.1627 sec\n",
      "Epoch [678 / 2000]: Train Loss=5.7967, Val Cor=-0.5644, Time=0.1625 sec\n",
      "Epoch [679 / 2000]: Train Loss=5.4100, Val Cor=0.5048, Time=0.1629 sec\n",
      "Epoch [680 / 2000]: Train Loss=5.3479, Val Cor=0.5286, Time=0.1625 sec\n",
      "Epoch [681 / 2000]: Train Loss=5.3065, Val Cor=0.6012, Time=0.1623 sec\n",
      "Epoch [682 / 2000]: Train Loss=5.3379, Val Cor=0.5622, Time=0.1613 sec\n",
      "Epoch [683 / 2000]: Train Loss=5.3923, Val Cor=0.4187, Time=0.1626 sec\n",
      "Epoch [684 / 2000]: Train Loss=5.7789, Val Cor=-0.2838, Time=0.1634 sec\n",
      "Epoch [685 / 2000]: Train Loss=5.3869, Val Cor=0.5409, Time=0.1628 sec\n",
      "Epoch [686 / 2000]: Train Loss=5.1618, Val Cor=0.6058, Time=0.1635 sec\n",
      "Epoch [687 / 2000]: Train Loss=5.1896, Val Cor=0.5634, Time=0.1640 sec\n",
      "Epoch [688 / 2000]: Train Loss=5.2456, Val Cor=0.5416, Time=0.1625 sec\n",
      "Epoch [689 / 2000]: Train Loss=5.5608, Val Cor=0.0884, Time=0.1623 sec\n",
      "Epoch [690 / 2000]: Train Loss=5.6813, Val Cor=0.5479, Time=0.1627 sec\n",
      "Epoch [691 / 2000]: Train Loss=5.2494, Val Cor=0.5958, Time=0.1625 sec\n",
      "Epoch [692 / 2000]: Train Loss=5.2411, Val Cor=0.5975, Time=0.1613 sec\n",
      "Epoch [693 / 2000]: Train Loss=5.4780, Val Cor=0.6037, Time=0.1602 sec\n",
      "Epoch [694 / 2000]: Train Loss=4.9979, Val Cor=0.5831, Time=0.1607 sec\n",
      "Epoch [695 / 2000]: Train Loss=5.2016, Val Cor=0.6041, Time=0.1608 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [696 / 2000]: Train Loss=5.2730, Val Cor=0.5954, Time=0.1621 sec\n",
      "Epoch [697 / 2000]: Train Loss=5.4321, Val Cor=0.6053, Time=0.1619 sec\n",
      "Epoch [698 / 2000]: Train Loss=5.6353, Val Cor=-0.0956, Time=0.1624 sec\n",
      "Epoch [699 / 2000]: Train Loss=5.3007, Val Cor=0.5753, Time=0.1622 sec\n",
      "Epoch [700 / 2000]: Train Loss=5.3404, Val Cor=0.6045, Time=0.1624 sec\n",
      "Epoch [701 / 2000]: Train Loss=5.3853, Val Cor=0.5518, Time=0.1641 sec\n",
      "Epoch [702 / 2000]: Train Loss=5.1609, Val Cor=-0.3310, Time=0.1607 sec\n",
      "Epoch [703 / 2000]: Train Loss=5.3862, Val Cor=0.5902, Time=0.1604 sec\n",
      "Epoch [704 / 2000]: Train Loss=5.2871, Val Cor=0.6035, Time=0.1604 sec\n",
      "Epoch [705 / 2000]: Train Loss=5.2148, Val Cor=0.6128, Time=0.1607 sec\n",
      "Epoch [706 / 2000]: Train Loss=5.6235, Val Cor=-0.0752, Time=0.1607 sec\n",
      "Epoch [707 / 2000]: Train Loss=5.3559, Val Cor=0.4833, Time=0.1609 sec\n",
      "Epoch [708 / 2000]: Train Loss=5.1633, Val Cor=0.6111, Time=0.1610 sec\n",
      "Epoch [709 / 2000]: Train Loss=5.6717, Val Cor=-0.5113, Time=0.1609 sec\n",
      "Epoch [710 / 2000]: Train Loss=5.6045, Val Cor=-0.0709, Time=0.1630 sec\n",
      "Epoch [711 / 2000]: Train Loss=5.5909, Val Cor=-0.3700, Time=0.1625 sec\n",
      "Epoch [712 / 2000]: Train Loss=5.7304, Val Cor=-0.3621, Time=0.1624 sec\n",
      "Epoch [713 / 2000]: Train Loss=5.6131, Val Cor=-0.2401, Time=0.1629 sec\n",
      "Epoch [714 / 2000]: Train Loss=5.5447, Val Cor=-0.1193, Time=0.1623 sec\n",
      "Epoch [715 / 2000]: Train Loss=5.1908, Val Cor=0.6073, Time=0.1619 sec\n",
      "Epoch [716 / 2000]: Train Loss=5.3665, Val Cor=-0.1131, Time=0.1626 sec\n",
      "Epoch [717 / 2000]: Train Loss=5.1890, Val Cor=-0.3025, Time=0.1628 sec\n",
      "Epoch [718 / 2000]: Train Loss=5.2841, Val Cor=0.6068, Time=0.1628 sec\n",
      "Epoch [719 / 2000]: Train Loss=5.0600, Val Cor=0.6178, Time=0.1614 sec\n",
      "Epoch [720 / 2000]: Train Loss=5.2041, Val Cor=0.6042, Time=0.1614 sec\n",
      "Epoch [721 / 2000]: Train Loss=5.7902, Val Cor=0.2998, Time=0.1611 sec\n",
      "Epoch [722 / 2000]: Train Loss=5.6073, Val Cor=0.5513, Time=0.1625 sec\n",
      "Epoch [723 / 2000]: Train Loss=5.1983, Val Cor=0.5995, Time=0.1625 sec\n",
      "Epoch [724 / 2000]: Train Loss=5.0201, Val Cor=0.5566, Time=0.1623 sec\n",
      "Epoch [725 / 2000]: Train Loss=5.3153, Val Cor=0.6139, Time=0.1627 sec\n",
      "Epoch [726 / 2000]: Train Loss=5.2844, Val Cor=0.6010, Time=0.1629 sec\n",
      "Epoch [727 / 2000]: Train Loss=5.1340, Val Cor=0.5975, Time=0.1624 sec\n",
      "Epoch [728 / 2000]: Train Loss=5.7732, Val Cor=0.5985, Time=0.1616 sec\n",
      "Epoch [729 / 2000]: Train Loss=5.4534, Val Cor=0.3440, Time=0.1606 sec\n",
      "Epoch [730 / 2000]: Train Loss=5.6088, Val Cor=0.6107, Time=0.1608 sec\n",
      "Epoch [731 / 2000]: Train Loss=5.2533, Val Cor=0.3099, Time=0.1604 sec\n",
      "Epoch [732 / 2000]: Train Loss=5.3580, Val Cor=0.6075, Time=0.1610 sec\n",
      "Epoch [733 / 2000]: Train Loss=5.2140, Val Cor=0.6168, Time=0.1616 sec\n",
      "Epoch [734 / 2000]: Train Loss=5.5086, Val Cor=0.5662, Time=0.1623 sec\n",
      "Epoch [735 / 2000]: Train Loss=5.2671, Val Cor=0.6069, Time=0.1616 sec\n",
      "Epoch [736 / 2000]: Train Loss=5.1551, Val Cor=0.6174, Time=0.1625 sec\n",
      "Epoch [737 / 2000]: Train Loss=5.5695, Val Cor=0.6016, Time=0.1618 sec\n",
      "Epoch [738 / 2000]: Train Loss=5.6727, Val Cor=-0.3561, Time=0.1608 sec\n",
      "Epoch [739 / 2000]: Train Loss=5.4801, Val Cor=0.6010, Time=0.1599 sec\n",
      "Epoch [740 / 2000]: Train Loss=5.5163, Val Cor=0.0520, Time=0.1601 sec\n",
      "Epoch [741 / 2000]: Train Loss=5.7225, Val Cor=-0.1469, Time=0.1603 sec\n",
      "Epoch [742 / 2000]: Train Loss=5.4084, Val Cor=0.5931, Time=0.1602 sec\n",
      "Epoch [743 / 2000]: Train Loss=5.2246, Val Cor=0.6212, Time=0.1599 sec\n",
      "Epoch [744 / 2000]: Train Loss=5.2457, Val Cor=0.5934, Time=0.1604 sec\n",
      "Epoch [745 / 2000]: Train Loss=5.3141, Val Cor=0.5985, Time=0.1606 sec\n",
      "Epoch [746 / 2000]: Train Loss=5.2156, Val Cor=0.6139, Time=0.1615 sec\n",
      "Epoch [747 / 2000]: Train Loss=5.2179, Val Cor=0.6159, Time=0.1621 sec\n",
      "Epoch [748 / 2000]: Train Loss=5.2988, Val Cor=0.3201, Time=0.1621 sec\n",
      "Epoch [749 / 2000]: Train Loss=5.2877, Val Cor=0.6122, Time=0.1621 sec\n",
      "Epoch [750 / 2000]: Train Loss=5.2679, Val Cor=0.4536, Time=0.1628 sec\n",
      "Epoch [751 / 2000]: Train Loss=5.4593, Val Cor=-0.1170, Time=0.1621 sec\n",
      "Epoch [752 / 2000]: Train Loss=5.3836, Val Cor=0.6108, Time=0.1621 sec\n",
      "Epoch [753 / 2000]: Train Loss=5.4124, Val Cor=0.5048, Time=0.1616 sec\n",
      "Epoch [754 / 2000]: Train Loss=5.3030, Val Cor=0.6158, Time=0.1615 sec\n",
      "Epoch [755 / 2000]: Train Loss=5.4354, Val Cor=0.5762, Time=0.1610 sec\n",
      "Epoch [756 / 2000]: Train Loss=5.4676, Val Cor=0.6133, Time=0.1617 sec\n",
      "Epoch [757 / 2000]: Train Loss=5.4414, Val Cor=0.6116, Time=0.1613 sec\n",
      "Epoch [758 / 2000]: Train Loss=5.1494, Val Cor=0.6126, Time=0.1616 sec\n",
      "Epoch [759 / 2000]: Train Loss=5.0896, Val Cor=0.6112, Time=0.1615 sec\n",
      "Epoch [760 / 2000]: Train Loss=5.3692, Val Cor=0.4775, Time=0.1632 sec\n",
      "Epoch [761 / 2000]: Train Loss=5.7329, Val Cor=0.6135, Time=0.1635 sec\n",
      "Epoch [762 / 2000]: Train Loss=5.5928, Val Cor=0.6079, Time=0.1641 sec\n",
      "Epoch [763 / 2000]: Train Loss=5.2880, Val Cor=-0.4965, Time=0.1646 sec\n",
      "Epoch [764 / 2000]: Train Loss=5.3638, Val Cor=0.3065, Time=0.1650 sec\n",
      "Epoch [765 / 2000]: Train Loss=5.4138, Val Cor=0.6221, Time=0.1611 sec\n",
      "Epoch [766 / 2000]: Train Loss=5.2094, Val Cor=0.6189, Time=0.1601 sec\n",
      "Epoch [767 / 2000]: Train Loss=5.2693, Val Cor=0.5747, Time=0.1609 sec\n",
      "Epoch [768 / 2000]: Train Loss=5.1359, Val Cor=0.6194, Time=0.1609 sec\n",
      "Epoch [769 / 2000]: Train Loss=5.0603, Val Cor=0.6194, Time=0.1610 sec\n",
      "Epoch [770 / 2000]: Train Loss=5.1358, Val Cor=0.6089, Time=0.1610 sec\n",
      "Epoch [771 / 2000]: Train Loss=5.4424, Val Cor=0.5382, Time=0.1621 sec\n",
      "Epoch [772 / 2000]: Train Loss=5.4588, Val Cor=0.6168, Time=0.1621 sec\n",
      "Epoch [773 / 2000]: Train Loss=5.5518, Val Cor=0.4740, Time=0.1613 sec\n",
      "Epoch [774 / 2000]: Train Loss=5.4527, Val Cor=0.6082, Time=0.1624 sec\n",
      "Epoch [775 / 2000]: Train Loss=5.2263, Val Cor=0.6153, Time=0.1630 sec\n",
      "Epoch [776 / 2000]: Train Loss=5.6563, Val Cor=-0.3345, Time=0.1608 sec\n",
      "Epoch [777 / 2000]: Train Loss=5.4348, Val Cor=0.6113, Time=0.1604 sec\n",
      "Epoch [778 / 2000]: Train Loss=5.3702, Val Cor=0.5678, Time=0.1607 sec\n",
      "Epoch [779 / 2000]: Train Loss=5.1063, Val Cor=0.6128, Time=0.1601 sec\n",
      "Epoch [780 / 2000]: Train Loss=5.3342, Val Cor=0.4222, Time=0.1603 sec\n",
      "Epoch [781 / 2000]: Train Loss=5.1560, Val Cor=0.6119, Time=0.1605 sec\n",
      "Epoch [782 / 2000]: Train Loss=5.1488, Val Cor=0.5582, Time=0.1622 sec\n",
      "Epoch [783 / 2000]: Train Loss=5.4693, Val Cor=0.5841, Time=0.1619 sec\n",
      "Epoch [784 / 2000]: Train Loss=5.2356, Val Cor=0.6249, Time=0.1623 sec\n",
      "Epoch [785 / 2000]: Train Loss=5.0471, Val Cor=0.6166, Time=0.1617 sec\n",
      "Epoch [786 / 2000]: Train Loss=5.2152, Val Cor=0.5743, Time=0.1627 sec\n",
      "Epoch [787 / 2000]: Train Loss=5.0818, Val Cor=0.5954, Time=0.1623 sec\n",
      "Epoch [788 / 2000]: Train Loss=5.2901, Val Cor=-0.1831, Time=0.1621 sec\n",
      "Epoch [789 / 2000]: Train Loss=5.3794, Val Cor=-0.5247, Time=0.1613 sec\n",
      "Epoch [790 / 2000]: Train Loss=5.5240, Val Cor=-0.5730, Time=0.1617 sec\n",
      "Epoch [791 / 2000]: Train Loss=5.4681, Val Cor=0.5411, Time=0.1615 sec\n",
      "Epoch [792 / 2000]: Train Loss=5.4097, Val Cor=0.6188, Time=0.1613 sec\n",
      "Epoch [793 / 2000]: Train Loss=5.8375, Val Cor=-0.5851, Time=0.1610 sec\n",
      "Epoch [794 / 2000]: Train Loss=5.1900, Val Cor=0.6030, Time=0.1617 sec\n",
      "Epoch [795 / 2000]: Train Loss=5.4825, Val Cor=0.4366, Time=0.1624 sec\n",
      "Epoch [796 / 2000]: Train Loss=5.4097, Val Cor=-0.0653, Time=0.1624 sec\n",
      "Epoch [797 / 2000]: Train Loss=5.3220, Val Cor=-0.1654, Time=0.1643 sec\n",
      "Epoch [798 / 2000]: Train Loss=5.4034, Val Cor=0.6022, Time=0.1651 sec\n",
      "Epoch [799 / 2000]: Train Loss=5.3036, Val Cor=0.5903, Time=0.1643 sec\n",
      "Epoch [800 / 2000]: Train Loss=5.1481, Val Cor=0.5839, Time=0.1631 sec\n",
      "Epoch [801 / 2000]: Train Loss=5.3248, Val Cor=0.6090, Time=0.1607 sec\n",
      "Epoch [802 / 2000]: Train Loss=5.3306, Val Cor=0.4426, Time=0.1611 sec\n",
      "Epoch [803 / 2000]: Train Loss=5.7158, Val Cor=0.4030, Time=0.1606 sec\n",
      "Epoch [804 / 2000]: Train Loss=5.3005, Val Cor=0.6130, Time=0.1610 sec\n",
      "Epoch [805 / 2000]: Train Loss=4.9895, Val Cor=0.6222, Time=0.1607 sec\n",
      "Epoch [806 / 2000]: Train Loss=5.0675, Val Cor=0.5684, Time=0.1613 sec\n",
      "Epoch [807 / 2000]: Train Loss=5.1708, Val Cor=0.6198, Time=0.1624 sec\n",
      "Epoch [808 / 2000]: Train Loss=5.2085, Val Cor=0.6061, Time=0.1631 sec\n",
      "Epoch [809 / 2000]: Train Loss=5.1047, Val Cor=0.6134, Time=0.1618 sec\n",
      "Epoch [810 / 2000]: Train Loss=5.1704, Val Cor=0.5877, Time=0.1617 sec\n",
      "Epoch [811 / 2000]: Train Loss=5.1426, Val Cor=0.6069, Time=0.1625 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [812 / 2000]: Train Loss=5.0610, Val Cor=0.6068, Time=0.1634 sec\n",
      "Epoch [813 / 2000]: Train Loss=5.1628, Val Cor=0.6109, Time=0.1627 sec\n",
      "Epoch [814 / 2000]: Train Loss=5.4173, Val Cor=0.6162, Time=0.1619 sec\n",
      "Epoch [815 / 2000]: Train Loss=5.1616, Val Cor=0.5648, Time=0.1616 sec\n",
      "Epoch [816 / 2000]: Train Loss=5.1132, Val Cor=0.6243, Time=0.1603 sec\n",
      "Epoch [817 / 2000]: Train Loss=5.1561, Val Cor=0.5081, Time=0.1604 sec\n",
      "Epoch [818 / 2000]: Train Loss=5.1424, Val Cor=0.6084, Time=0.1613 sec\n",
      "Epoch [819 / 2000]: Train Loss=5.5523, Val Cor=0.2911, Time=0.1609 sec\n",
      "Epoch [820 / 2000]: Train Loss=5.5581, Val Cor=0.5604, Time=0.1614 sec\n",
      "Epoch [821 / 2000]: Train Loss=5.2009, Val Cor=0.6154, Time=0.1623 sec\n",
      "Epoch [822 / 2000]: Train Loss=5.2619, Val Cor=-0.5753, Time=0.1630 sec\n",
      "Epoch [823 / 2000]: Train Loss=5.2576, Val Cor=0.6044, Time=0.1625 sec\n",
      "Epoch [824 / 2000]: Train Loss=5.3447, Val Cor=0.5981, Time=0.1631 sec\n",
      "Epoch [825 / 2000]: Train Loss=5.1148, Val Cor=0.5952, Time=0.1621 sec\n",
      "Epoch [826 / 2000]: Train Loss=5.1650, Val Cor=0.6207, Time=0.1622 sec\n",
      "Epoch [827 / 2000]: Train Loss=5.0849, Val Cor=0.6060, Time=0.1637 sec\n",
      "Epoch [828 / 2000]: Train Loss=5.2308, Val Cor=0.6134, Time=0.1628 sec\n",
      "Epoch [829 / 2000]: Train Loss=5.4509, Val Cor=0.2763, Time=0.1631 sec\n",
      "Epoch [830 / 2000]: Train Loss=5.2233, Val Cor=0.5571, Time=0.1634 sec\n",
      "Epoch [831 / 2000]: Train Loss=5.6176, Val Cor=0.6146, Time=0.1629 sec\n",
      "Epoch [832 / 2000]: Train Loss=5.2187, Val Cor=0.5984, Time=0.1608 sec\n",
      "Epoch [833 / 2000]: Train Loss=5.1909, Val Cor=0.6034, Time=0.1616 sec\n",
      "Epoch [834 / 2000]: Train Loss=5.0420, Val Cor=0.6238, Time=0.1628 sec\n",
      "Epoch [835 / 2000]: Train Loss=5.1056, Val Cor=0.5101, Time=0.1617 sec\n",
      "Epoch [836 / 2000]: Train Loss=5.1077, Val Cor=0.6233, Time=0.1623 sec\n",
      "Epoch [837 / 2000]: Train Loss=5.0450, Val Cor=0.6129, Time=0.1620 sec\n",
      "Epoch [838 / 2000]: Train Loss=5.1578, Val Cor=0.4093, Time=0.1616 sec\n",
      "Epoch [839 / 2000]: Train Loss=5.3930, Val Cor=-0.5206, Time=0.1610 sec\n",
      "Epoch [840 / 2000]: Train Loss=5.3544, Val Cor=0.1487, Time=0.1602 sec\n",
      "Epoch [841 / 2000]: Train Loss=5.1904, Val Cor=0.6072, Time=0.1600 sec\n",
      "Epoch [842 / 2000]: Train Loss=4.9072, Val Cor=0.5585, Time=0.1598 sec\n",
      "Epoch [843 / 2000]: Train Loss=4.9095, Val Cor=0.6218, Time=0.1600 sec\n",
      "Epoch [844 / 2000]: Train Loss=5.1218, Val Cor=0.6173, Time=0.1598 sec\n",
      "Epoch [845 / 2000]: Train Loss=5.0739, Val Cor=0.6113, Time=0.1607 sec\n",
      "Epoch [846 / 2000]: Train Loss=5.0174, Val Cor=0.6253, Time=0.1616 sec\n",
      "Epoch [847 / 2000]: Train Loss=5.6464, Val Cor=0.2466, Time=0.1614 sec\n",
      "Epoch [848 / 2000]: Train Loss=7.9225, Val Cor=-0.2271, Time=0.1617 sec\n",
      "Epoch [849 / 2000]: Train Loss=6.6922, Val Cor=0.0041, Time=0.1615 sec\n",
      "Epoch [850 / 2000]: Train Loss=6.4774, Val Cor=-0.3578, Time=0.1624 sec\n",
      "Epoch [851 / 2000]: Train Loss=5.6293, Val Cor=-0.2995, Time=0.1619 sec\n",
      "Epoch [852 / 2000]: Train Loss=5.7952, Val Cor=0.4225, Time=0.1618 sec\n",
      "Epoch [853 / 2000]: Train Loss=5.5332, Val Cor=0.5777, Time=0.1611 sec\n",
      "Epoch [854 / 2000]: Train Loss=5.5270, Val Cor=0.6023, Time=0.1605 sec\n",
      "Epoch [855 / 2000]: Train Loss=5.3057, Val Cor=0.5991, Time=0.1600 sec\n",
      "Epoch [856 / 2000]: Train Loss=5.3560, Val Cor=0.5808, Time=0.1608 sec\n",
      "Epoch [857 / 2000]: Train Loss=5.4902, Val Cor=0.6064, Time=0.1604 sec\n",
      "Epoch [858 / 2000]: Train Loss=5.2881, Val Cor=0.6084, Time=0.1617 sec\n",
      "Epoch [859 / 2000]: Train Loss=5.1795, Val Cor=0.6053, Time=0.1617 sec\n",
      "Epoch [860 / 2000]: Train Loss=5.3386, Val Cor=0.6106, Time=0.1620 sec\n",
      "Epoch [861 / 2000]: Train Loss=5.2232, Val Cor=0.3841, Time=0.1612 sec\n",
      "Epoch [862 / 2000]: Train Loss=5.0817, Val Cor=0.6134, Time=0.1623 sec\n",
      "Epoch [863 / 2000]: Train Loss=5.2731, Val Cor=0.6121, Time=0.1621 sec\n",
      "Epoch [864 / 2000]: Train Loss=5.6981, Val Cor=0.6168, Time=0.1620 sec\n",
      "Epoch [865 / 2000]: Train Loss=5.4902, Val Cor=-0.4478, Time=0.1609 sec\n",
      "Epoch [866 / 2000]: Train Loss=5.3768, Val Cor=0.6058, Time=0.1604 sec\n",
      "Epoch [867 / 2000]: Train Loss=5.1031, Val Cor=0.6011, Time=0.1601 sec\n",
      "Epoch [868 / 2000]: Train Loss=5.1274, Val Cor=0.6059, Time=0.1606 sec\n",
      "Epoch [869 / 2000]: Train Loss=5.1526, Val Cor=-0.5085, Time=0.1608 sec\n",
      "Epoch [870 / 2000]: Train Loss=4.9250, Val Cor=0.6148, Time=0.1615 sec\n",
      "Epoch [871 / 2000]: Train Loss=4.9588, Val Cor=0.5822, Time=0.1628 sec\n",
      "Epoch [872 / 2000]: Train Loss=5.5335, Val Cor=-0.5542, Time=0.1632 sec\n",
      "Epoch [873 / 2000]: Train Loss=5.4270, Val Cor=0.5885, Time=0.1624 sec\n",
      "Epoch [874 / 2000]: Train Loss=5.0958, Val Cor=-0.3705, Time=0.1630 sec\n",
      "Epoch [875 / 2000]: Train Loss=5.0360, Val Cor=0.6072, Time=0.1692 sec\n",
      "Epoch [876 / 2000]: Train Loss=5.4639, Val Cor=0.6115, Time=0.1643 sec\n",
      "Epoch [877 / 2000]: Train Loss=4.9500, Val Cor=0.6084, Time=0.1638 sec\n",
      "Epoch [878 / 2000]: Train Loss=5.2406, Val Cor=-0.5897, Time=0.1608 sec\n",
      "Epoch [879 / 2000]: Train Loss=5.2937, Val Cor=-0.4636, Time=0.1604 sec\n",
      "Epoch [880 / 2000]: Train Loss=5.2512, Val Cor=0.5584, Time=0.1611 sec\n",
      "Epoch [881 / 2000]: Train Loss=4.8885, Val Cor=0.3607, Time=0.1609 sec\n",
      "Epoch [882 / 2000]: Train Loss=5.0534, Val Cor=0.6152, Time=0.1609 sec\n",
      "Epoch [883 / 2000]: Train Loss=4.9655, Val Cor=-0.0536, Time=0.1618 sec\n",
      "Epoch [884 / 2000]: Train Loss=5.0932, Val Cor=0.5941, Time=0.1624 sec\n",
      "Epoch [885 / 2000]: Train Loss=5.3756, Val Cor=-0.4423, Time=0.1660 sec\n",
      "Epoch [886 / 2000]: Train Loss=5.2353, Val Cor=0.6176, Time=0.1622 sec\n",
      "Epoch [887 / 2000]: Train Loss=5.0985, Val Cor=0.6220, Time=0.1624 sec\n",
      "Epoch [888 / 2000]: Train Loss=4.9102, Val Cor=0.6124, Time=0.1623 sec\n",
      "Epoch [889 / 2000]: Train Loss=5.0356, Val Cor=0.5787, Time=0.1608 sec\n",
      "Epoch [890 / 2000]: Train Loss=4.8516, Val Cor=0.6146, Time=0.1603 sec\n",
      "Epoch [891 / 2000]: Train Loss=4.9228, Val Cor=0.6078, Time=0.1702 sec\n",
      "Epoch [892 / 2000]: Train Loss=5.0508, Val Cor=-0.0619, Time=0.1604 sec\n",
      "Epoch [893 / 2000]: Train Loss=5.1920, Val Cor=0.5770, Time=0.1605 sec\n",
      "Epoch [894 / 2000]: Train Loss=4.8788, Val Cor=0.6305, Time=0.1608 sec\n",
      "Epoch [895 / 2000]: Train Loss=5.2642, Val Cor=0.6231, Time=0.1612 sec\n",
      "Epoch [896 / 2000]: Train Loss=4.9060, Val Cor=0.6227, Time=0.1619 sec\n",
      "Epoch [897 / 2000]: Train Loss=5.1065, Val Cor=0.5487, Time=0.1625 sec\n",
      "Epoch [898 / 2000]: Train Loss=5.3974, Val Cor=0.6233, Time=0.1623 sec\n",
      "Epoch [899 / 2000]: Train Loss=5.1906, Val Cor=0.2884, Time=0.1616 sec\n",
      "Epoch [900 / 2000]: Train Loss=5.2338, Val Cor=0.5709, Time=0.1618 sec\n",
      "Epoch [901 / 2000]: Train Loss=5.2139, Val Cor=0.6202, Time=0.1615 sec\n",
      "Epoch [902 / 2000]: Train Loss=5.0939, Val Cor=0.6040, Time=0.1633 sec\n",
      "Epoch [903 / 2000]: Train Loss=4.8700, Val Cor=0.5895, Time=0.1645 sec\n",
      "Epoch [904 / 2000]: Train Loss=5.1645, Val Cor=-0.5853, Time=0.1637 sec\n",
      "Epoch [905 / 2000]: Train Loss=5.3798, Val Cor=0.6073, Time=0.1638 sec\n",
      "Epoch [906 / 2000]: Train Loss=5.1986, Val Cor=0.2053, Time=0.1632 sec\n",
      "Epoch [907 / 2000]: Train Loss=5.0442, Val Cor=0.6095, Time=0.1666 sec\n",
      "Epoch [908 / 2000]: Train Loss=5.0718, Val Cor=0.6105, Time=0.1612 sec\n",
      "Epoch [909 / 2000]: Train Loss=4.8629, Val Cor=0.6143, Time=0.1605 sec\n",
      "Epoch [910 / 2000]: Train Loss=4.8600, Val Cor=0.4621, Time=0.1611 sec\n",
      "Epoch [911 / 2000]: Train Loss=5.3503, Val Cor=0.5584, Time=0.1611 sec\n",
      "Epoch [912 / 2000]: Train Loss=5.3150, Val Cor=0.2460, Time=0.1610 sec\n",
      "Epoch [913 / 2000]: Train Loss=5.0623, Val Cor=0.5794, Time=0.1614 sec\n",
      "Epoch [914 / 2000]: Train Loss=4.9495, Val Cor=0.5531, Time=0.1627 sec\n",
      "Epoch [915 / 2000]: Train Loss=4.7875, Val Cor=0.6063, Time=0.1628 sec\n",
      "Epoch [916 / 2000]: Train Loss=4.9881, Val Cor=0.6207, Time=0.1623 sec\n",
      "Epoch [917 / 2000]: Train Loss=4.8793, Val Cor=0.3254, Time=0.1624 sec\n",
      "Epoch [918 / 2000]: Train Loss=4.8759, Val Cor=0.6179, Time=0.1629 sec\n",
      "Epoch [919 / 2000]: Train Loss=5.1350, Val Cor=-0.2350, Time=0.1625 sec\n",
      "Epoch [920 / 2000]: Train Loss=5.1835, Val Cor=0.6168, Time=0.1613 sec\n",
      "Epoch [921 / 2000]: Train Loss=5.1084, Val Cor=0.6067, Time=0.1611 sec\n",
      "Epoch [922 / 2000]: Train Loss=5.0370, Val Cor=0.2889, Time=0.1629 sec\n",
      "Epoch [923 / 2000]: Train Loss=5.1346, Val Cor=-0.0892, Time=0.1633 sec\n",
      "Epoch [924 / 2000]: Train Loss=4.9691, Val Cor=0.5276, Time=0.1637 sec\n",
      "Epoch [925 / 2000]: Train Loss=4.9136, Val Cor=0.5955, Time=0.1631 sec\n",
      "Epoch [926 / 2000]: Train Loss=4.8838, Val Cor=0.5748, Time=0.1630 sec\n",
      "Epoch [927 / 2000]: Train Loss=5.3655, Val Cor=0.1020, Time=0.1629 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [928 / 2000]: Train Loss=5.0382, Val Cor=0.3411, Time=0.1636 sec\n",
      "Epoch [929 / 2000]: Train Loss=5.0983, Val Cor=0.1742, Time=0.1668 sec\n",
      "Epoch [930 / 2000]: Train Loss=5.1253, Val Cor=0.5989, Time=0.1629 sec\n",
      "Epoch [931 / 2000]: Train Loss=4.9660, Val Cor=0.6114, Time=0.1615 sec\n",
      "Epoch [932 / 2000]: Train Loss=4.9530, Val Cor=0.5612, Time=0.1610 sec\n",
      "Epoch [933 / 2000]: Train Loss=4.9817, Val Cor=0.3339, Time=0.1614 sec\n",
      "Epoch [934 / 2000]: Train Loss=4.9339, Val Cor=0.5920, Time=0.1619 sec\n",
      "Epoch [935 / 2000]: Train Loss=4.9598, Val Cor=0.6156, Time=0.1616 sec\n",
      "Epoch [936 / 2000]: Train Loss=5.3957, Val Cor=0.5266, Time=0.1622 sec\n",
      "Epoch [937 / 2000]: Train Loss=5.2556, Val Cor=0.3198, Time=0.1622 sec\n",
      "Epoch [938 / 2000]: Train Loss=5.1539, Val Cor=0.5837, Time=0.1633 sec\n",
      "Epoch [939 / 2000]: Train Loss=5.1682, Val Cor=-0.5992, Time=0.1630 sec\n",
      "Epoch [940 / 2000]: Train Loss=5.3414, Val Cor=0.5191, Time=0.1635 sec\n",
      "Epoch [941 / 2000]: Train Loss=5.0079, Val Cor=0.6171, Time=0.1634 sec\n",
      "Epoch [942 / 2000]: Train Loss=5.0782, Val Cor=0.6096, Time=0.1646 sec\n",
      "Epoch [943 / 2000]: Train Loss=5.3026, Val Cor=-0.5949, Time=0.1634 sec\n",
      "Epoch [944 / 2000]: Train Loss=5.2468, Val Cor=0.5947, Time=0.1629 sec\n",
      "Epoch [945 / 2000]: Train Loss=5.0743, Val Cor=0.5924, Time=0.1630 sec\n",
      "Epoch [946 / 2000]: Train Loss=4.8622, Val Cor=0.5677, Time=0.1611 sec\n",
      "Epoch [947 / 2000]: Train Loss=5.0241, Val Cor=0.5766, Time=0.1614 sec\n",
      "Epoch [948 / 2000]: Train Loss=5.0286, Val Cor=0.5508, Time=0.1615 sec\n",
      "Epoch [949 / 2000]: Train Loss=5.2916, Val Cor=0.6055, Time=0.1613 sec\n",
      "Epoch [950 / 2000]: Train Loss=5.4221, Val Cor=-0.4246, Time=0.1611 sec\n",
      "Epoch [951 / 2000]: Train Loss=5.1128, Val Cor=0.5966, Time=0.1621 sec\n",
      "Epoch [952 / 2000]: Train Loss=5.1244, Val Cor=0.4541, Time=0.1629 sec\n",
      "Epoch [953 / 2000]: Train Loss=5.3535, Val Cor=0.6150, Time=0.1627 sec\n",
      "Epoch [954 / 2000]: Train Loss=5.2183, Val Cor=0.4207, Time=0.1650 sec\n",
      "Epoch [955 / 2000]: Train Loss=5.1431, Val Cor=0.5856, Time=0.1606 sec\n",
      "Epoch [956 / 2000]: Train Loss=5.0819, Val Cor=0.6264, Time=0.1607 sec\n",
      "Epoch [957 / 2000]: Train Loss=5.1623, Val Cor=0.6102, Time=0.1604 sec\n",
      "Epoch [958 / 2000]: Train Loss=5.2445, Val Cor=0.5150, Time=0.1611 sec\n",
      "Epoch [959 / 2000]: Train Loss=5.2498, Val Cor=0.6165, Time=0.1606 sec\n",
      "Epoch [960 / 2000]: Train Loss=5.1402, Val Cor=0.6265, Time=0.1621 sec\n",
      "Epoch [961 / 2000]: Train Loss=4.9269, Val Cor=0.6215, Time=0.1619 sec\n",
      "Epoch [962 / 2000]: Train Loss=5.1742, Val Cor=0.5761, Time=0.1633 sec\n",
      "Epoch [963 / 2000]: Train Loss=4.9974, Val Cor=0.6177, Time=0.1632 sec\n",
      "Epoch [964 / 2000]: Train Loss=5.0837, Val Cor=0.6147, Time=0.1626 sec\n",
      "Epoch [965 / 2000]: Train Loss=4.9502, Val Cor=-0.4593, Time=0.1617 sec\n",
      "Epoch [966 / 2000]: Train Loss=5.0905, Val Cor=0.5955, Time=0.1606 sec\n",
      "Epoch [967 / 2000]: Train Loss=5.2908, Val Cor=0.6239, Time=0.1610 sec\n",
      "Epoch [968 / 2000]: Train Loss=5.3516, Val Cor=0.6166, Time=0.1609 sec\n",
      "Epoch [969 / 2000]: Train Loss=4.9926, Val Cor=0.6183, Time=0.1613 sec\n",
      "Epoch [970 / 2000]: Train Loss=5.1580, Val Cor=0.6075, Time=0.1616 sec\n",
      "Epoch [971 / 2000]: Train Loss=5.0389, Val Cor=0.5941, Time=0.1615 sec\n",
      "Epoch [972 / 2000]: Train Loss=4.8794, Val Cor=0.6090, Time=0.1628 sec\n",
      "Epoch [973 / 2000]: Train Loss=5.0725, Val Cor=0.6215, Time=0.1628 sec\n",
      "Epoch [974 / 2000]: Train Loss=5.6273, Val Cor=0.5984, Time=0.1644 sec\n",
      "Epoch [975 / 2000]: Train Loss=5.6892, Val Cor=0.6170, Time=0.1649 sec\n",
      "Epoch [976 / 2000]: Train Loss=5.2041, Val Cor=0.5666, Time=0.1640 sec\n",
      "Epoch [977 / 2000]: Train Loss=5.0902, Val Cor=0.4557, Time=0.1613 sec\n",
      "Epoch [978 / 2000]: Train Loss=4.9873, Val Cor=0.5534, Time=0.1612 sec\n",
      "Epoch [979 / 2000]: Train Loss=5.1747, Val Cor=0.6375, Time=0.1618 sec\n",
      "Epoch [980 / 2000]: Train Loss=5.1935, Val Cor=0.6175, Time=0.1617 sec\n",
      "Epoch [981 / 2000]: Train Loss=5.0243, Val Cor=0.5924, Time=0.1619 sec\n",
      "Epoch [982 / 2000]: Train Loss=5.0368, Val Cor=0.6224, Time=0.1618 sec\n",
      "Epoch [983 / 2000]: Train Loss=4.9795, Val Cor=0.1488, Time=0.1629 sec\n",
      "Epoch [984 / 2000]: Train Loss=5.1425, Val Cor=0.5858, Time=0.1630 sec\n",
      "Epoch [985 / 2000]: Train Loss=5.0327, Val Cor=-0.4506, Time=0.1668 sec\n",
      "Epoch [986 / 2000]: Train Loss=5.1478, Val Cor=0.6090, Time=0.1628 sec\n",
      "Epoch [987 / 2000]: Train Loss=5.1302, Val Cor=0.0766, Time=0.1632 sec\n",
      "Epoch [988 / 2000]: Train Loss=5.1632, Val Cor=0.6144, Time=0.1628 sec\n",
      "Epoch [989 / 2000]: Train Loss=4.8112, Val Cor=0.6087, Time=0.1610 sec\n",
      "Epoch [990 / 2000]: Train Loss=5.1149, Val Cor=-0.3288, Time=0.1614 sec\n",
      "Epoch [991 / 2000]: Train Loss=5.1552, Val Cor=0.6039, Time=0.1615 sec\n",
      "Epoch [992 / 2000]: Train Loss=5.4901, Val Cor=-0.5839, Time=0.1618 sec\n",
      "Epoch [993 / 2000]: Train Loss=5.3333, Val Cor=-0.1365, Time=0.1619 sec\n",
      "Epoch [994 / 2000]: Train Loss=4.9058, Val Cor=0.5880, Time=0.1633 sec\n",
      "Epoch [995 / 2000]: Train Loss=5.0798, Val Cor=0.5514, Time=0.1631 sec\n",
      "Epoch [996 / 2000]: Train Loss=5.0237, Val Cor=0.3329, Time=0.1628 sec\n",
      "Epoch [997 / 2000]: Train Loss=4.9028, Val Cor=0.6194, Time=0.1636 sec\n",
      "Epoch [998 / 2000]: Train Loss=4.8930, Val Cor=0.6264, Time=0.1629 sec\n",
      "Epoch [999 / 2000]: Train Loss=4.9847, Val Cor=0.5917, Time=0.1628 sec\n",
      "Epoch [1000 / 2000]: Train Loss=5.0749, Val Cor=0.6082, Time=0.1631 sec\n",
      "Epoch [1001 / 2000]: Train Loss=4.9306, Val Cor=0.6185, Time=0.1635 sec\n",
      "Epoch [1002 / 2000]: Train Loss=4.8876, Val Cor=0.5792, Time=0.1637 sec\n",
      "Epoch [1003 / 2000]: Train Loss=4.7567, Val Cor=0.5655, Time=0.1619 sec\n",
      "Epoch [1004 / 2000]: Train Loss=5.0841, Val Cor=-0.2361, Time=0.1616 sec\n",
      "Epoch [1005 / 2000]: Train Loss=5.1846, Val Cor=0.5373, Time=0.1612 sec\n",
      "Epoch [1006 / 2000]: Train Loss=5.0072, Val Cor=-0.3269, Time=0.1608 sec\n",
      "Epoch [1007 / 2000]: Train Loss=5.0919, Val Cor=0.3275, Time=0.1611 sec\n",
      "Epoch [1008 / 2000]: Train Loss=5.0625, Val Cor=0.5119, Time=0.1623 sec\n",
      "Epoch [1009 / 2000]: Train Loss=5.0029, Val Cor=0.6295, Time=0.1618 sec\n",
      "Epoch [1010 / 2000]: Train Loss=5.2789, Val Cor=0.2410, Time=0.1628 sec\n",
      "Epoch [1011 / 2000]: Train Loss=5.0185, Val Cor=0.6017, Time=0.1622 sec\n",
      "Epoch [1012 / 2000]: Train Loss=4.8675, Val Cor=0.5969, Time=0.1605 sec\n",
      "Epoch [1013 / 2000]: Train Loss=5.0658, Val Cor=0.6234, Time=0.1601 sec\n",
      "Epoch [1014 / 2000]: Train Loss=5.1676, Val Cor=0.6376, Time=0.1603 sec\n",
      "Epoch [1015 / 2000]: Train Loss=5.8006, Val Cor=0.6124, Time=0.1599 sec\n",
      "Epoch [1016 / 2000]: Train Loss=5.5760, Val Cor=0.6210, Time=0.1610 sec\n",
      "Epoch [1017 / 2000]: Train Loss=5.1580, Val Cor=-0.1322, Time=0.1640 sec\n",
      "Epoch [1018 / 2000]: Train Loss=5.2865, Val Cor=0.6175, Time=0.1610 sec\n",
      "Epoch [1019 / 2000]: Train Loss=5.0849, Val Cor=-0.3720, Time=0.1627 sec\n",
      "Epoch [1020 / 2000]: Train Loss=5.0451, Val Cor=0.6005, Time=0.1628 sec\n",
      "Epoch [1021 / 2000]: Train Loss=5.2522, Val Cor=0.6075, Time=0.1634 sec\n",
      "Epoch [1022 / 2000]: Train Loss=5.0012, Val Cor=0.6141, Time=0.1630 sec\n",
      "Epoch [1023 / 2000]: Train Loss=4.9169, Val Cor=0.6362, Time=0.1614 sec\n",
      "Epoch [1024 / 2000]: Train Loss=5.0457, Val Cor=0.6160, Time=0.1611 sec\n",
      "Epoch [1025 / 2000]: Train Loss=5.3389, Val Cor=-0.2291, Time=0.1608 sec\n",
      "Epoch [1026 / 2000]: Train Loss=5.3684, Val Cor=-0.4346, Time=0.1615 sec\n",
      "Epoch [1027 / 2000]: Train Loss=5.2012, Val Cor=0.4459, Time=0.1622 sec\n",
      "Epoch [1028 / 2000]: Train Loss=5.1097, Val Cor=0.1128, Time=0.1637 sec\n",
      "Epoch [1029 / 2000]: Train Loss=5.1389, Val Cor=0.6007, Time=0.1631 sec\n",
      "Epoch [1030 / 2000]: Train Loss=4.8971, Val Cor=0.6115, Time=0.1622 sec\n",
      "Epoch [1031 / 2000]: Train Loss=5.0451, Val Cor=0.6004, Time=0.1610 sec\n",
      "Epoch [1032 / 2000]: Train Loss=5.0408, Val Cor=0.1273, Time=0.1614 sec\n",
      "Epoch [1033 / 2000]: Train Loss=4.7847, Val Cor=0.6194, Time=0.1616 sec\n",
      "Epoch [1034 / 2000]: Train Loss=4.8491, Val Cor=0.3823, Time=0.1617 sec\n",
      "Epoch [1035 / 2000]: Train Loss=4.9816, Val Cor=0.4513, Time=0.1618 sec\n",
      "Epoch [1036 / 2000]: Train Loss=5.0511, Val Cor=0.5967, Time=0.1633 sec\n",
      "Epoch [1037 / 2000]: Train Loss=5.0508, Val Cor=0.5898, Time=0.1628 sec\n",
      "Epoch [1038 / 2000]: Train Loss=4.8979, Val Cor=0.4889, Time=0.1623 sec\n",
      "Epoch [1039 / 2000]: Train Loss=5.2751, Val Cor=0.6301, Time=0.1633 sec\n",
      "Epoch [1040 / 2000]: Train Loss=5.1404, Val Cor=0.2205, Time=0.1625 sec\n",
      "Epoch [1041 / 2000]: Train Loss=5.1041, Val Cor=0.6013, Time=0.1612 sec\n",
      "Epoch [1042 / 2000]: Train Loss=4.9384, Val Cor=0.4679, Time=0.1604 sec\n",
      "Epoch [1043 / 2000]: Train Loss=5.1842, Val Cor=0.6136, Time=0.1600 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1044 / 2000]: Train Loss=4.9907, Val Cor=0.6130, Time=0.1604 sec\n",
      "Epoch [1045 / 2000]: Train Loss=5.3093, Val Cor=0.6314, Time=0.1612 sec\n",
      "Epoch [1046 / 2000]: Train Loss=5.2666, Val Cor=0.4809, Time=0.1610 sec\n",
      "Epoch [1047 / 2000]: Train Loss=5.5401, Val Cor=0.4371, Time=0.1622 sec\n",
      "Epoch [1048 / 2000]: Train Loss=5.1890, Val Cor=-0.0991, Time=0.1624 sec\n",
      "Epoch [1049 / 2000]: Train Loss=4.9460, Val Cor=0.6021, Time=0.1617 sec\n",
      "Epoch [1050 / 2000]: Train Loss=4.9418, Val Cor=0.6143, Time=0.1625 sec\n",
      "Epoch [1051 / 2000]: Train Loss=5.2387, Val Cor=0.3299, Time=0.1620 sec\n",
      "Epoch [1052 / 2000]: Train Loss=5.1695, Val Cor=0.4671, Time=0.1604 sec\n",
      "Epoch [1053 / 2000]: Train Loss=5.0233, Val Cor=0.6270, Time=0.1599 sec\n",
      "Epoch [1054 / 2000]: Train Loss=5.1613, Val Cor=0.6070, Time=0.1600 sec\n",
      "Epoch [1055 / 2000]: Train Loss=4.9628, Val Cor=0.6018, Time=0.1604 sec\n",
      "Epoch [1056 / 2000]: Train Loss=4.8029, Val Cor=0.6243, Time=0.1610 sec\n",
      "Epoch [1057 / 2000]: Train Loss=5.2589, Val Cor=0.4783, Time=0.1610 sec\n",
      "Epoch [1058 / 2000]: Train Loss=4.9994, Val Cor=0.6106, Time=0.1628 sec\n",
      "Epoch [1059 / 2000]: Train Loss=4.7805, Val Cor=0.6082, Time=0.1630 sec\n",
      "Epoch [1060 / 2000]: Train Loss=4.7555, Val Cor=0.6345, Time=0.1626 sec\n",
      "Epoch [1061 / 2000]: Train Loss=5.1487, Val Cor=0.1928, Time=0.1626 sec\n",
      "Epoch [1062 / 2000]: Train Loss=4.9956, Val Cor=0.6221, Time=0.1635 sec\n",
      "Epoch [1063 / 2000]: Train Loss=4.9940, Val Cor=0.5645, Time=0.1629 sec\n",
      "Epoch [1064 / 2000]: Train Loss=4.8871, Val Cor=-0.2486, Time=0.1615 sec\n",
      "Epoch [1065 / 2000]: Train Loss=4.8290, Val Cor=0.5488, Time=0.1609 sec\n",
      "Epoch [1066 / 2000]: Train Loss=5.0119, Val Cor=0.5427, Time=0.1616 sec\n",
      "Epoch [1067 / 2000]: Train Loss=4.8495, Val Cor=0.6091, Time=0.1624 sec\n",
      "Epoch [1068 / 2000]: Train Loss=5.0863, Val Cor=0.6057, Time=0.1633 sec\n",
      "Epoch [1069 / 2000]: Train Loss=4.8686, Val Cor=0.5877, Time=0.1648 sec\n",
      "Epoch [1070 / 2000]: Train Loss=4.9558, Val Cor=0.6273, Time=0.1631 sec\n",
      "Epoch [1071 / 2000]: Train Loss=5.2925, Val Cor=0.5328, Time=0.1634 sec\n",
      "Epoch [1072 / 2000]: Train Loss=5.1129, Val Cor=-0.1571, Time=0.1628 sec\n",
      "Epoch [1073 / 2000]: Train Loss=5.2259, Val Cor=0.5002, Time=0.1623 sec\n",
      "Epoch [1074 / 2000]: Train Loss=4.9905, Val Cor=0.6180, Time=0.1615 sec\n",
      "Epoch [1075 / 2000]: Train Loss=5.1053, Val Cor=0.5928, Time=0.1607 sec\n",
      "Epoch [1076 / 2000]: Train Loss=4.8853, Val Cor=0.0044, Time=0.1618 sec\n",
      "Epoch [1077 / 2000]: Train Loss=4.7845, Val Cor=0.3949, Time=0.1611 sec\n",
      "Epoch [1078 / 2000]: Train Loss=5.0637, Val Cor=0.5790, Time=0.1628 sec\n",
      "Epoch [1079 / 2000]: Train Loss=4.8833, Val Cor=-0.1294, Time=0.1625 sec\n",
      "Epoch [1080 / 2000]: Train Loss=5.0365, Val Cor=0.4949, Time=0.1630 sec\n",
      "Epoch [1081 / 2000]: Train Loss=5.4390, Val Cor=0.5422, Time=0.1627 sec\n",
      "Epoch [1082 / 2000]: Train Loss=5.2119, Val Cor=0.2562, Time=0.1619 sec\n",
      "Epoch [1083 / 2000]: Train Loss=5.0970, Val Cor=-0.0130, Time=0.1607 sec\n",
      "Epoch [1084 / 2000]: Train Loss=5.3186, Val Cor=0.5824, Time=0.1607 sec\n",
      "Epoch [1085 / 2000]: Train Loss=5.2280, Val Cor=-0.0015, Time=0.1602 sec\n",
      "Epoch [1086 / 2000]: Train Loss=5.1234, Val Cor=0.6104, Time=0.1617 sec\n",
      "Epoch [1087 / 2000]: Train Loss=5.0431, Val Cor=0.5745, Time=0.1613 sec\n",
      "Epoch [1088 / 2000]: Train Loss=5.5841, Val Cor=0.0889, Time=0.1616 sec\n",
      "Epoch [1089 / 2000]: Train Loss=5.2612, Val Cor=0.5963, Time=0.1633 sec\n",
      "Epoch [1090 / 2000]: Train Loss=5.2871, Val Cor=0.0587, Time=0.1633 sec\n",
      "Epoch [1091 / 2000]: Train Loss=5.1511, Val Cor=0.6099, Time=0.1626 sec\n",
      "Epoch [1092 / 2000]: Train Loss=5.0686, Val Cor=0.2504, Time=0.1637 sec\n",
      "Epoch [1093 / 2000]: Train Loss=5.0423, Val Cor=0.5683, Time=0.1632 sec\n",
      "Epoch [1094 / 2000]: Train Loss=5.0158, Val Cor=0.5418, Time=0.1628 sec\n",
      "Epoch [1095 / 2000]: Train Loss=5.1303, Val Cor=0.5302, Time=0.1631 sec\n",
      "Epoch [1096 / 2000]: Train Loss=4.8927, Val Cor=0.6013, Time=0.1612 sec\n",
      "Epoch [1097 / 2000]: Train Loss=5.0781, Val Cor=0.5644, Time=0.1610 sec\n",
      "Epoch [1098 / 2000]: Train Loss=4.9466, Val Cor=0.4559, Time=0.1621 sec\n",
      "Epoch [1099 / 2000]: Train Loss=5.1303, Val Cor=0.6349, Time=0.1620 sec\n",
      "Epoch [1100 / 2000]: Train Loss=5.2926, Val Cor=-0.3986, Time=0.1622 sec\n",
      "Epoch [1101 / 2000]: Train Loss=5.1029, Val Cor=0.3051, Time=0.1623 sec\n",
      "Epoch [1102 / 2000]: Train Loss=4.9629, Val Cor=0.5884, Time=0.1632 sec\n",
      "Epoch [1103 / 2000]: Train Loss=4.9872, Val Cor=0.4491, Time=0.1631 sec\n",
      "Epoch [1104 / 2000]: Train Loss=5.0044, Val Cor=0.5370, Time=0.1641 sec\n",
      "Epoch [1105 / 2000]: Train Loss=4.8991, Val Cor=0.6272, Time=0.1633 sec\n",
      "Epoch [1106 / 2000]: Train Loss=4.9799, Val Cor=-0.4579, Time=0.1613 sec\n",
      "Epoch [1107 / 2000]: Train Loss=4.9983, Val Cor=0.6323, Time=0.1606 sec\n",
      "Epoch [1108 / 2000]: Train Loss=4.8341, Val Cor=0.3660, Time=0.1619 sec\n",
      "Epoch [1109 / 2000]: Train Loss=5.0308, Val Cor=-0.1584, Time=0.1614 sec\n",
      "Epoch [1110 / 2000]: Train Loss=4.7471, Val Cor=-0.2637, Time=0.1634 sec\n",
      "Epoch [1111 / 2000]: Train Loss=5.2081, Val Cor=0.3249, Time=0.1626 sec\n",
      "Epoch [1112 / 2000]: Train Loss=5.1386, Val Cor=0.6206, Time=0.1633 sec\n",
      "Epoch [1113 / 2000]: Train Loss=4.9899, Val Cor=0.2028, Time=0.1632 sec\n",
      "Epoch [1114 / 2000]: Train Loss=4.9043, Val Cor=0.6279, Time=0.1631 sec\n",
      "Epoch [1115 / 2000]: Train Loss=5.0202, Val Cor=0.6316, Time=0.1612 sec\n",
      "Epoch [1116 / 2000]: Train Loss=4.9960, Val Cor=0.5875, Time=0.1607 sec\n",
      "Epoch [1117 / 2000]: Train Loss=4.8077, Val Cor=0.5259, Time=0.1611 sec\n",
      "Epoch [1118 / 2000]: Train Loss=4.9536, Val Cor=0.6178, Time=0.1617 sec\n",
      "Epoch [1119 / 2000]: Train Loss=4.7968, Val Cor=0.5497, Time=0.1622 sec\n",
      "Epoch [1120 / 2000]: Train Loss=5.2105, Val Cor=0.3736, Time=0.1634 sec\n",
      "Epoch [1121 / 2000]: Train Loss=4.9780, Val Cor=0.6345, Time=0.1644 sec\n",
      "Epoch [1122 / 2000]: Train Loss=4.7695, Val Cor=0.6203, Time=0.1637 sec\n",
      "Epoch [1123 / 2000]: Train Loss=4.7962, Val Cor=0.6210, Time=0.1634 sec\n",
      "Epoch [1124 / 2000]: Train Loss=4.9744, Val Cor=0.6047, Time=0.1635 sec\n",
      "Epoch [1125 / 2000]: Train Loss=4.9483, Val Cor=0.5628, Time=0.1635 sec\n",
      "Epoch [1126 / 2000]: Train Loss=4.7994, Val Cor=0.6123, Time=0.1636 sec\n",
      "Epoch [1127 / 2000]: Train Loss=4.7849, Val Cor=0.5844, Time=0.1612 sec\n",
      "Epoch [1128 / 2000]: Train Loss=4.8595, Val Cor=0.5967, Time=0.1610 sec\n",
      "Epoch [1129 / 2000]: Train Loss=4.8481, Val Cor=0.5847, Time=0.1605 sec\n",
      "Epoch [1130 / 2000]: Train Loss=4.9703, Val Cor=0.4266, Time=0.1619 sec\n",
      "Epoch [1131 / 2000]: Train Loss=5.2125, Val Cor=0.5539, Time=0.1613 sec\n",
      "Epoch [1132 / 2000]: Train Loss=5.0462, Val Cor=0.6446, Time=0.1618 sec\n",
      "Epoch [1133 / 2000]: Train Loss=4.8307, Val Cor=-0.3820, Time=0.1614 sec\n",
      "Epoch [1134 / 2000]: Train Loss=4.8603, Val Cor=0.5149, Time=0.1630 sec\n",
      "Epoch [1135 / 2000]: Train Loss=4.9016, Val Cor=0.6131, Time=0.1623 sec\n",
      "Epoch [1136 / 2000]: Train Loss=5.0227, Val Cor=0.6274, Time=0.1622 sec\n",
      "Epoch [1137 / 2000]: Train Loss=4.7869, Val Cor=0.6007, Time=0.1629 sec\n",
      "Epoch [1138 / 2000]: Train Loss=4.8396, Val Cor=0.5890, Time=0.1633 sec\n",
      "Epoch [1139 / 2000]: Train Loss=4.9273, Val Cor=0.6191, Time=0.1621 sec\n",
      "Epoch [1140 / 2000]: Train Loss=4.6483, Val Cor=0.5473, Time=0.1618 sec\n",
      "Epoch [1141 / 2000]: Train Loss=4.7826, Val Cor=0.5295, Time=0.1606 sec\n",
      "Epoch [1142 / 2000]: Train Loss=4.8114, Val Cor=0.5995, Time=0.1607 sec\n",
      "Epoch [1143 / 2000]: Train Loss=5.1950, Val Cor=0.2879, Time=0.1610 sec\n",
      "Epoch [1144 / 2000]: Train Loss=5.2187, Val Cor=0.6135, Time=0.1618 sec\n",
      "Epoch [1145 / 2000]: Train Loss=5.0720, Val Cor=0.6126, Time=0.1633 sec\n",
      "Epoch [1146 / 2000]: Train Loss=5.0025, Val Cor=0.6035, Time=0.1628 sec\n",
      "Epoch [1147 / 2000]: Train Loss=4.8025, Val Cor=0.6164, Time=0.1619 sec\n",
      "Epoch [1148 / 2000]: Train Loss=4.6787, Val Cor=0.6164, Time=0.1624 sec\n",
      "Epoch [1149 / 2000]: Train Loss=4.9591, Val Cor=0.6089, Time=0.1636 sec\n",
      "Epoch [1150 / 2000]: Train Loss=5.0114, Val Cor=0.5734, Time=0.1649 sec\n",
      "Epoch [1151 / 2000]: Train Loss=4.9825, Val Cor=0.6169, Time=0.1653 sec\n",
      "Epoch [1152 / 2000]: Train Loss=5.0173, Val Cor=0.6164, Time=0.1648 sec\n",
      "Epoch [1153 / 2000]: Train Loss=4.8534, Val Cor=0.3010, Time=0.1630 sec\n",
      "Epoch [1154 / 2000]: Train Loss=4.9146, Val Cor=0.6103, Time=0.1630 sec\n",
      "Epoch [1155 / 2000]: Train Loss=4.8033, Val Cor=0.6423, Time=0.1615 sec\n",
      "Epoch [1156 / 2000]: Train Loss=4.9251, Val Cor=0.6031, Time=0.1607 sec\n",
      "Epoch [1157 / 2000]: Train Loss=4.9171, Val Cor=0.4961, Time=0.1602 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1158 / 2000]: Train Loss=5.1732, Val Cor=0.6342, Time=0.1605 sec\n",
      "Epoch [1159 / 2000]: Train Loss=5.2995, Val Cor=0.5831, Time=0.1610 sec\n",
      "Epoch [1160 / 2000]: Train Loss=4.9855, Val Cor=0.6105, Time=0.1615 sec\n",
      "Epoch [1161 / 2000]: Train Loss=5.0302, Val Cor=0.5976, Time=0.1614 sec\n",
      "Epoch [1162 / 2000]: Train Loss=4.7416, Val Cor=0.6009, Time=0.1609 sec\n",
      "Epoch [1163 / 2000]: Train Loss=4.8583, Val Cor=0.5446, Time=0.1626 sec\n",
      "Epoch [1164 / 2000]: Train Loss=5.0729, Val Cor=-0.0273, Time=0.1632 sec\n",
      "Epoch [1165 / 2000]: Train Loss=4.8650, Val Cor=0.5854, Time=0.1627 sec\n",
      "Epoch [1166 / 2000]: Train Loss=4.9829, Val Cor=0.6111, Time=0.1635 sec\n",
      "Epoch [1167 / 2000]: Train Loss=5.0980, Val Cor=0.6049, Time=0.1629 sec\n",
      "Epoch [1168 / 2000]: Train Loss=5.1950, Val Cor=0.5478, Time=0.1613 sec\n",
      "Epoch [1169 / 2000]: Train Loss=4.8679, Val Cor=0.6149, Time=0.1608 sec\n",
      "Epoch [1170 / 2000]: Train Loss=4.7730, Val Cor=0.6026, Time=0.1622 sec\n",
      "Epoch [1171 / 2000]: Train Loss=4.9045, Val Cor=0.6099, Time=0.1620 sec\n",
      "Epoch [1172 / 2000]: Train Loss=4.7334, Val Cor=0.4295, Time=0.1623 sec\n",
      "Epoch [1173 / 2000]: Train Loss=4.8674, Val Cor=0.4932, Time=0.1623 sec\n",
      "Epoch [1174 / 2000]: Train Loss=4.7151, Val Cor=0.5755, Time=0.1634 sec\n",
      "Epoch [1175 / 2000]: Train Loss=4.7468, Val Cor=0.5630, Time=0.1638 sec\n",
      "Epoch [1176 / 2000]: Train Loss=4.6854, Val Cor=-0.2839, Time=0.1643 sec\n",
      "Epoch [1177 / 2000]: Train Loss=5.3007, Val Cor=0.5437, Time=0.1655 sec\n",
      "Epoch [1178 / 2000]: Train Loss=4.9621, Val Cor=0.5133, Time=0.1657 sec\n",
      "Epoch [1179 / 2000]: Train Loss=4.9942, Val Cor=0.6055, Time=0.1616 sec\n",
      "Epoch [1180 / 2000]: Train Loss=4.9108, Val Cor=0.5941, Time=0.1614 sec\n",
      "Epoch [1181 / 2000]: Train Loss=4.8705, Val Cor=0.6051, Time=0.1629 sec\n",
      "Epoch [1182 / 2000]: Train Loss=4.6846, Val Cor=0.5816, Time=0.1648 sec\n",
      "Epoch [1183 / 2000]: Train Loss=4.7866, Val Cor=0.5696, Time=0.1632 sec\n",
      "Epoch [1184 / 2000]: Train Loss=4.6200, Val Cor=0.3314, Time=0.1639 sec\n",
      "Epoch [1185 / 2000]: Train Loss=4.7689, Val Cor=0.5847, Time=0.1630 sec\n",
      "Epoch [1186 / 2000]: Train Loss=4.9757, Val Cor=0.6157, Time=0.1625 sec\n",
      "Epoch [1187 / 2000]: Train Loss=4.8416, Val Cor=0.6243, Time=0.1607 sec\n",
      "Epoch [1188 / 2000]: Train Loss=4.7524, Val Cor=0.5464, Time=0.1609 sec\n",
      "Epoch [1189 / 2000]: Train Loss=5.0635, Val Cor=0.1310, Time=0.1605 sec\n",
      "Epoch [1190 / 2000]: Train Loss=5.1985, Val Cor=0.3939, Time=0.1625 sec\n",
      "Epoch [1191 / 2000]: Train Loss=4.8206, Val Cor=0.5431, Time=0.1620 sec\n",
      "Epoch [1192 / 2000]: Train Loss=4.7120, Val Cor=0.6142, Time=0.1625 sec\n",
      "Epoch [1193 / 2000]: Train Loss=5.2454, Val Cor=0.6024, Time=0.1635 sec\n",
      "Epoch [1194 / 2000]: Train Loss=4.8842, Val Cor=0.0583, Time=0.1635 sec\n",
      "Epoch [1195 / 2000]: Train Loss=4.9038, Val Cor=0.6144, Time=0.1632 sec\n",
      "Epoch [1196 / 2000]: Train Loss=4.7784, Val Cor=-0.5340, Time=0.1657 sec\n",
      "Epoch [1197 / 2000]: Train Loss=4.9915, Val Cor=0.5887, Time=0.1654 sec\n",
      "Epoch [1198 / 2000]: Train Loss=5.0289, Val Cor=0.6167, Time=0.1652 sec\n",
      "Epoch [1199 / 2000]: Train Loss=4.8594, Val Cor=0.5567, Time=0.1634 sec\n",
      "Epoch [1200 / 2000]: Train Loss=5.0483, Val Cor=0.6091, Time=0.1613 sec\n",
      "Epoch [1201 / 2000]: Train Loss=5.3145, Val Cor=0.6093, Time=0.1608 sec\n",
      "Epoch [1202 / 2000]: Train Loss=5.3674, Val Cor=0.6186, Time=0.1612 sec\n",
      "Epoch [1203 / 2000]: Train Loss=5.1193, Val Cor=0.5614, Time=0.1616 sec\n",
      "Epoch [1204 / 2000]: Train Loss=5.1282, Val Cor=0.2504, Time=0.1620 sec\n",
      "Epoch [1205 / 2000]: Train Loss=5.1199, Val Cor=0.0616, Time=0.1633 sec\n",
      "Epoch [1206 / 2000]: Train Loss=4.8491, Val Cor=-0.6267, Time=0.1632 sec\n",
      "Epoch [1207 / 2000]: Train Loss=4.6011, Val Cor=0.5371, Time=0.1618 sec\n",
      "Epoch [1208 / 2000]: Train Loss=4.8953, Val Cor=0.4122, Time=0.1636 sec\n",
      "Epoch [1209 / 2000]: Train Loss=5.0461, Val Cor=0.5843, Time=0.1637 sec\n",
      "Epoch [1210 / 2000]: Train Loss=4.9940, Val Cor=0.6171, Time=0.1634 sec\n",
      "Epoch [1211 / 2000]: Train Loss=4.8263, Val Cor=0.2699, Time=0.1623 sec\n",
      "Epoch [1212 / 2000]: Train Loss=4.7916, Val Cor=0.6158, Time=0.1621 sec\n",
      "Epoch [1213 / 2000]: Train Loss=4.9167, Val Cor=0.6180, Time=0.1615 sec\n",
      "Epoch [1214 / 2000]: Train Loss=4.8140, Val Cor=0.6055, Time=0.1630 sec\n",
      "Epoch [1215 / 2000]: Train Loss=4.7611, Val Cor=0.4408, Time=0.1642 sec\n",
      "Epoch [1216 / 2000]: Train Loss=4.9426, Val Cor=0.4838, Time=0.1621 sec\n",
      "Epoch [1217 / 2000]: Train Loss=4.9589, Val Cor=0.6025, Time=0.1620 sec\n",
      "Epoch [1218 / 2000]: Train Loss=4.9110, Val Cor=0.5516, Time=0.1636 sec\n",
      "Epoch [1219 / 2000]: Train Loss=4.8172, Val Cor=0.6177, Time=0.1629 sec\n",
      "Epoch [1220 / 2000]: Train Loss=4.7616, Val Cor=0.5839, Time=0.1643 sec\n",
      "Epoch [1221 / 2000]: Train Loss=5.1486, Val Cor=0.1950, Time=0.1633 sec\n",
      "Epoch [1222 / 2000]: Train Loss=4.7135, Val Cor=0.5588, Time=0.1624 sec\n",
      "Epoch [1223 / 2000]: Train Loss=4.9104, Val Cor=0.5143, Time=0.1609 sec\n",
      "Epoch [1224 / 2000]: Train Loss=4.9972, Val Cor=0.3751, Time=0.1611 sec\n",
      "Epoch [1225 / 2000]: Train Loss=5.0487, Val Cor=-0.5652, Time=0.1610 sec\n",
      "Epoch [1226 / 2000]: Train Loss=5.0449, Val Cor=0.5686, Time=0.1612 sec\n",
      "Epoch [1227 / 2000]: Train Loss=4.9221, Val Cor=0.3483, Time=0.1619 sec\n",
      "Epoch [1228 / 2000]: Train Loss=4.9118, Val Cor=0.0749, Time=0.1623 sec\n",
      "Epoch [1229 / 2000]: Train Loss=4.8258, Val Cor=0.6451, Time=0.1639 sec\n",
      "Epoch [1230 / 2000]: Train Loss=4.9294, Val Cor=0.2788, Time=0.1640 sec\n",
      "Epoch [1231 / 2000]: Train Loss=4.9077, Val Cor=0.5230, Time=0.1637 sec\n",
      "Epoch [1232 / 2000]: Train Loss=4.7971, Val Cor=0.5967, Time=0.1634 sec\n",
      "Epoch [1233 / 2000]: Train Loss=4.8349, Val Cor=0.3445, Time=0.1652 sec\n",
      "Epoch [1234 / 2000]: Train Loss=4.8494, Val Cor=0.5931, Time=0.1653 sec\n",
      "Epoch [1235 / 2000]: Train Loss=4.8394, Val Cor=0.6319, Time=0.1654 sec\n",
      "Epoch [1236 / 2000]: Train Loss=4.8657, Val Cor=0.6066, Time=0.1633 sec\n",
      "Epoch [1237 / 2000]: Train Loss=5.2007, Val Cor=0.5461, Time=0.1612 sec\n",
      "Epoch [1238 / 2000]: Train Loss=6.0070, Val Cor=-0.1562, Time=0.1616 sec\n",
      "Epoch [1239 / 2000]: Train Loss=5.6950, Val Cor=0.6006, Time=0.1620 sec\n",
      "Epoch [1240 / 2000]: Train Loss=5.1158, Val Cor=0.6172, Time=0.1619 sec\n",
      "Epoch [1241 / 2000]: Train Loss=4.9863, Val Cor=-0.1382, Time=0.1613 sec\n",
      "Epoch [1242 / 2000]: Train Loss=4.9390, Val Cor=0.6244, Time=0.1637 sec\n",
      "Epoch [1243 / 2000]: Train Loss=4.9243, Val Cor=0.6200, Time=0.1632 sec\n",
      "Epoch [1244 / 2000]: Train Loss=4.9361, Val Cor=0.5276, Time=0.1632 sec\n",
      "Epoch [1245 / 2000]: Train Loss=4.9662, Val Cor=0.6185, Time=0.1640 sec\n",
      "Epoch [1246 / 2000]: Train Loss=4.8192, Val Cor=0.5964, Time=0.1640 sec\n",
      "Epoch [1247 / 2000]: Train Loss=4.8121, Val Cor=0.5196, Time=0.1636 sec\n",
      "Epoch [1248 / 2000]: Train Loss=4.8129, Val Cor=0.4784, Time=0.1623 sec\n",
      "Epoch [1249 / 2000]: Train Loss=4.7622, Val Cor=0.2676, Time=0.1615 sec\n",
      "Epoch [1250 / 2000]: Train Loss=5.0645, Val Cor=0.3771, Time=0.1626 sec\n",
      "Epoch [1251 / 2000]: Train Loss=5.1169, Val Cor=0.2500, Time=0.1629 sec\n",
      "Epoch [1252 / 2000]: Train Loss=4.9823, Val Cor=0.5668, Time=0.1644 sec\n",
      "Epoch [1253 / 2000]: Train Loss=4.8877, Val Cor=0.5467, Time=0.1620 sec\n",
      "Epoch [1254 / 2000]: Train Loss=4.9238, Val Cor=0.6120, Time=0.1637 sec\n",
      "Epoch [1255 / 2000]: Train Loss=4.8549, Val Cor=0.4434, Time=0.1631 sec\n",
      "Epoch [1256 / 2000]: Train Loss=5.0903, Val Cor=0.5192, Time=0.1644 sec\n",
      "Epoch [1257 / 2000]: Train Loss=5.4752, Val Cor=0.2145, Time=0.1633 sec\n",
      "Epoch [1258 / 2000]: Train Loss=5.1806, Val Cor=0.5994, Time=0.1616 sec\n",
      "Epoch [1259 / 2000]: Train Loss=5.0841, Val Cor=0.5897, Time=0.1612 sec\n",
      "Epoch [1260 / 2000]: Train Loss=4.7239, Val Cor=0.5680, Time=0.1614 sec\n",
      "Epoch [1261 / 2000]: Train Loss=4.5911, Val Cor=0.4541, Time=0.1619 sec\n",
      "Epoch [1262 / 2000]: Train Loss=4.8268, Val Cor=0.4932, Time=0.1622 sec\n",
      "Epoch [1263 / 2000]: Train Loss=4.8782, Val Cor=0.5985, Time=0.1623 sec\n",
      "Epoch [1264 / 2000]: Train Loss=4.6849, Val Cor=0.6251, Time=0.1639 sec\n",
      "Epoch [1265 / 2000]: Train Loss=4.8610, Val Cor=0.5614, Time=0.1636 sec\n",
      "Epoch [1266 / 2000]: Train Loss=5.0150, Val Cor=0.4913, Time=0.1634 sec\n",
      "Epoch [1267 / 2000]: Train Loss=4.9490, Val Cor=0.5778, Time=0.1648 sec\n",
      "Epoch [1268 / 2000]: Train Loss=4.8407, Val Cor=0.6175, Time=0.1657 sec\n",
      "Epoch [1269 / 2000]: Train Loss=4.7938, Val Cor=0.5964, Time=0.1621 sec\n",
      "Epoch [1270 / 2000]: Train Loss=5.0040, Val Cor=0.6232, Time=0.1623 sec\n",
      "Epoch [1271 / 2000]: Train Loss=4.8989, Val Cor=0.5561, Time=0.1621 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1272 / 2000]: Train Loss=4.7045, Val Cor=0.1083, Time=0.1628 sec\n",
      "Epoch [1273 / 2000]: Train Loss=4.8523, Val Cor=0.4446, Time=0.1625 sec\n",
      "Epoch [1274 / 2000]: Train Loss=4.7079, Val Cor=0.5009, Time=0.1614 sec\n",
      "Epoch [1275 / 2000]: Train Loss=4.9958, Val Cor=-0.1476, Time=0.1625 sec\n",
      "Epoch [1276 / 2000]: Train Loss=4.8420, Val Cor=0.5355, Time=0.1638 sec\n",
      "Epoch [1277 / 2000]: Train Loss=4.8255, Val Cor=0.4441, Time=0.1630 sec\n",
      "Epoch [1278 / 2000]: Train Loss=4.8636, Val Cor=0.6167, Time=0.1633 sec\n",
      "Epoch [1279 / 2000]: Train Loss=4.8338, Val Cor=0.6102, Time=0.1641 sec\n",
      "Epoch [1280 / 2000]: Train Loss=4.6768, Val Cor=0.5949, Time=0.1642 sec\n",
      "Epoch [1281 / 2000]: Train Loss=4.7421, Val Cor=0.6024, Time=0.1634 sec\n",
      "Epoch [1282 / 2000]: Train Loss=4.9353, Val Cor=0.5137, Time=0.1616 sec\n",
      "Epoch [1283 / 2000]: Train Loss=4.9641, Val Cor=0.3733, Time=0.1611 sec\n",
      "Epoch [1284 / 2000]: Train Loss=5.1851, Val Cor=0.5043, Time=0.1616 sec\n",
      "Epoch [1285 / 2000]: Train Loss=4.8512, Val Cor=0.3725, Time=0.1620 sec\n",
      "Epoch [1286 / 2000]: Train Loss=5.1236, Val Cor=0.6044, Time=0.1627 sec\n",
      "Epoch [1287 / 2000]: Train Loss=4.7312, Val Cor=0.3619, Time=0.1625 sec\n",
      "Epoch [1288 / 2000]: Train Loss=4.9958, Val Cor=0.4377, Time=0.1625 sec\n",
      "Epoch [1289 / 2000]: Train Loss=4.8455, Val Cor=0.5856, Time=0.1646 sec\n",
      "Epoch [1290 / 2000]: Train Loss=4.8189, Val Cor=0.5000, Time=0.1645 sec\n",
      "Epoch [1291 / 2000]: Train Loss=4.7294, Val Cor=0.3409, Time=0.1635 sec\n",
      "Epoch [1292 / 2000]: Train Loss=5.2596, Val Cor=0.5643, Time=0.1645 sec\n",
      "Epoch [1293 / 2000]: Train Loss=4.7848, Val Cor=0.5306, Time=0.1637 sec\n",
      "Epoch [1294 / 2000]: Train Loss=4.7764, Val Cor=0.5603, Time=0.1629 sec\n",
      "Epoch [1295 / 2000]: Train Loss=4.7220, Val Cor=0.6066, Time=0.1620 sec\n",
      "Epoch [1296 / 2000]: Train Loss=4.9164, Val Cor=0.6219, Time=0.1620 sec\n",
      "Epoch [1297 / 2000]: Train Loss=5.1650, Val Cor=0.5800, Time=0.1619 sec\n",
      "Epoch [1298 / 2000]: Train Loss=4.9134, Val Cor=0.6270, Time=0.1627 sec\n",
      "Epoch [1299 / 2000]: Train Loss=4.7629, Val Cor=0.6129, Time=0.1624 sec\n",
      "Epoch [1300 / 2000]: Train Loss=4.8199, Val Cor=-0.4618, Time=0.1623 sec\n",
      "Epoch [1301 / 2000]: Train Loss=4.9348, Val Cor=0.6254, Time=0.1641 sec\n",
      "Epoch [1302 / 2000]: Train Loss=4.8930, Val Cor=0.5628, Time=0.1643 sec\n",
      "Epoch [1303 / 2000]: Train Loss=4.8873, Val Cor=-0.0676, Time=0.1635 sec\n",
      "Epoch [1304 / 2000]: Train Loss=4.7944, Val Cor=0.1110, Time=0.1646 sec\n",
      "Epoch [1305 / 2000]: Train Loss=4.9509, Val Cor=0.6227, Time=0.1645 sec\n",
      "Epoch [1306 / 2000]: Train Loss=4.9093, Val Cor=0.5788, Time=0.1642 sec\n",
      "Epoch [1307 / 2000]: Train Loss=4.9510, Val Cor=0.5375, Time=0.1636 sec\n",
      "Epoch [1308 / 2000]: Train Loss=4.8708, Val Cor=-0.1937, Time=0.1633 sec\n",
      "Epoch [1309 / 2000]: Train Loss=4.7349, Val Cor=0.6220, Time=0.1634 sec\n",
      "Epoch [1310 / 2000]: Train Loss=4.7180, Val Cor=0.5616, Time=0.1633 sec\n",
      "Epoch [1311 / 2000]: Train Loss=4.8194, Val Cor=0.6179, Time=0.1612 sec\n",
      "Epoch [1312 / 2000]: Train Loss=4.7986, Val Cor=0.5779, Time=0.1620 sec\n",
      "Epoch [1313 / 2000]: Train Loss=4.9792, Val Cor=0.6069, Time=0.1619 sec\n",
      "Epoch [1314 / 2000]: Train Loss=4.6077, Val Cor=0.5839, Time=0.1636 sec\n",
      "Epoch [1315 / 2000]: Train Loss=4.5517, Val Cor=0.5938, Time=0.1629 sec\n",
      "Epoch [1316 / 2000]: Train Loss=4.9507, Val Cor=0.5944, Time=0.1644 sec\n",
      "Epoch [1317 / 2000]: Train Loss=4.7034, Val Cor=0.5140, Time=0.1642 sec\n",
      "Epoch [1318 / 2000]: Train Loss=4.8258, Val Cor=0.5635, Time=0.1641 sec\n",
      "Epoch [1319 / 2000]: Train Loss=4.8310, Val Cor=0.4267, Time=0.1617 sec\n",
      "Epoch [1320 / 2000]: Train Loss=5.2154, Val Cor=0.5341, Time=0.1610 sec\n",
      "Epoch [1321 / 2000]: Train Loss=4.7734, Val Cor=0.4943, Time=0.1616 sec\n",
      "Epoch [1322 / 2000]: Train Loss=5.0901, Val Cor=0.6179, Time=0.1623 sec\n",
      "Epoch [1323 / 2000]: Train Loss=4.8829, Val Cor=0.6224, Time=0.1625 sec\n",
      "Epoch [1324 / 2000]: Train Loss=4.7888, Val Cor=0.6146, Time=0.1628 sec\n",
      "Epoch [1325 / 2000]: Train Loss=4.6592, Val Cor=0.3073, Time=0.1640 sec\n",
      "Epoch [1326 / 2000]: Train Loss=4.8121, Val Cor=0.6198, Time=0.1637 sec\n",
      "Epoch [1327 / 2000]: Train Loss=4.6139, Val Cor=0.5625, Time=0.1644 sec\n",
      "Epoch [1328 / 2000]: Train Loss=4.8389, Val Cor=0.5388, Time=0.1644 sec\n",
      "Epoch [1329 / 2000]: Train Loss=4.6216, Val Cor=0.5627, Time=0.1634 sec\n",
      "Epoch [1330 / 2000]: Train Loss=4.9419, Val Cor=0.6357, Time=0.1625 sec\n",
      "Epoch [1331 / 2000]: Train Loss=4.9463, Val Cor=0.5808, Time=0.1620 sec\n",
      "Epoch [1332 / 2000]: Train Loss=4.8033, Val Cor=0.6333, Time=0.1625 sec\n",
      "Epoch [1333 / 2000]: Train Loss=4.6330, Val Cor=0.5825, Time=0.1624 sec\n",
      "Epoch [1334 / 2000]: Train Loss=4.7009, Val Cor=0.6214, Time=0.1635 sec\n",
      "Epoch [1335 / 2000]: Train Loss=4.7168, Val Cor=0.6073, Time=0.1631 sec\n",
      "Epoch [1336 / 2000]: Train Loss=4.9522, Val Cor=0.5756, Time=0.1645 sec\n",
      "Epoch [1337 / 2000]: Train Loss=4.8690, Val Cor=0.5548, Time=0.1641 sec\n",
      "Epoch [1338 / 2000]: Train Loss=4.8017, Val Cor=0.3722, Time=0.1641 sec\n",
      "Epoch [1339 / 2000]: Train Loss=4.6328, Val Cor=0.6302, Time=0.1632 sec\n",
      "Epoch [1340 / 2000]: Train Loss=4.7251, Val Cor=0.5679, Time=0.1634 sec\n",
      "Epoch [1341 / 2000]: Train Loss=4.7231, Val Cor=0.5402, Time=0.1635 sec\n",
      "Epoch [1342 / 2000]: Train Loss=4.8072, Val Cor=-0.4390, Time=0.1636 sec\n",
      "Epoch [1343 / 2000]: Train Loss=4.9665, Val Cor=0.6325, Time=0.1614 sec\n",
      "Epoch [1344 / 2000]: Train Loss=4.7175, Val Cor=0.5637, Time=0.1617 sec\n",
      "Epoch [1345 / 2000]: Train Loss=4.8141, Val Cor=0.5507, Time=0.1633 sec\n",
      "Epoch [1346 / 2000]: Train Loss=4.8004, Val Cor=0.6183, Time=0.1631 sec\n",
      "Epoch [1347 / 2000]: Train Loss=4.6624, Val Cor=0.6130, Time=0.1612 sec\n",
      "Epoch [1348 / 2000]: Train Loss=5.1582, Val Cor=0.2899, Time=0.1608 sec\n",
      "Epoch [1349 / 2000]: Train Loss=4.8896, Val Cor=0.5739, Time=0.1612 sec\n",
      "Epoch [1350 / 2000]: Train Loss=5.0203, Val Cor=0.5561, Time=0.1619 sec\n",
      "Epoch [1351 / 2000]: Train Loss=5.0034, Val Cor=-0.2202, Time=0.1619 sec\n",
      "Epoch [1352 / 2000]: Train Loss=4.8170, Val Cor=0.5861, Time=0.1620 sec\n",
      "Epoch [1353 / 2000]: Train Loss=4.6160, Val Cor=0.4491, Time=0.1632 sec\n",
      "Epoch [1354 / 2000]: Train Loss=4.8460, Val Cor=0.1780, Time=0.1642 sec\n",
      "Epoch [1355 / 2000]: Train Loss=5.0420, Val Cor=0.4355, Time=0.1638 sec\n",
      "Epoch [1356 / 2000]: Train Loss=4.8214, Val Cor=0.4892, Time=0.1648 sec\n",
      "Epoch [1357 / 2000]: Train Loss=4.7260, Val Cor=0.3328, Time=0.1640 sec\n",
      "Epoch [1358 / 2000]: Train Loss=4.9387, Val Cor=0.5351, Time=0.1633 sec\n",
      "Epoch [1359 / 2000]: Train Loss=4.8552, Val Cor=0.6268, Time=0.1623 sec\n",
      "Epoch [1360 / 2000]: Train Loss=4.8334, Val Cor=0.5722, Time=0.1620 sec\n",
      "Epoch [1361 / 2000]: Train Loss=4.9284, Val Cor=0.3190, Time=0.1619 sec\n",
      "Epoch [1362 / 2000]: Train Loss=4.8828, Val Cor=0.5888, Time=0.1633 sec\n",
      "Epoch [1363 / 2000]: Train Loss=4.7060, Val Cor=0.4869, Time=0.1644 sec\n",
      "Epoch [1364 / 2000]: Train Loss=4.9111, Val Cor=0.5806, Time=0.1616 sec\n",
      "Epoch [1365 / 2000]: Train Loss=4.7727, Val Cor=0.5058, Time=0.1633 sec\n",
      "Epoch [1366 / 2000]: Train Loss=4.8684, Val Cor=0.5974, Time=0.1638 sec\n",
      "Epoch [1367 / 2000]: Train Loss=4.9488, Val Cor=0.5324, Time=0.1629 sec\n",
      "Epoch [1368 / 2000]: Train Loss=4.6883, Val Cor=0.6117, Time=0.1642 sec\n",
      "Epoch [1369 / 2000]: Train Loss=4.6352, Val Cor=0.6145, Time=0.1633 sec\n",
      "Epoch [1370 / 2000]: Train Loss=4.8712, Val Cor=0.5157, Time=0.1615 sec\n",
      "Epoch [1371 / 2000]: Train Loss=5.1283, Val Cor=0.5553, Time=0.1609 sec\n",
      "Epoch [1372 / 2000]: Train Loss=4.7963, Val Cor=0.2896, Time=0.1615 sec\n",
      "Epoch [1373 / 2000]: Train Loss=4.9712, Val Cor=0.5991, Time=0.1619 sec\n",
      "Epoch [1374 / 2000]: Train Loss=4.8911, Val Cor=0.6132, Time=0.1619 sec\n",
      "Epoch [1375 / 2000]: Train Loss=5.1446, Val Cor=0.3793, Time=0.1629 sec\n",
      "Epoch [1376 / 2000]: Train Loss=5.1076, Val Cor=0.4488, Time=0.1647 sec\n",
      "Epoch [1377 / 2000]: Train Loss=5.2780, Val Cor=0.5858, Time=0.1641 sec\n",
      "Epoch [1378 / 2000]: Train Loss=4.8698, Val Cor=0.6024, Time=0.1637 sec\n",
      "Epoch [1379 / 2000]: Train Loss=4.7522, Val Cor=0.6047, Time=0.1647 sec\n",
      "Epoch [1380 / 2000]: Train Loss=4.8171, Val Cor=0.5963, Time=0.1643 sec\n",
      "Epoch [1381 / 2000]: Train Loss=4.6657, Val Cor=0.5665, Time=0.1629 sec\n",
      "Epoch [1382 / 2000]: Train Loss=4.8428, Val Cor=0.5416, Time=0.1628 sec\n",
      "Epoch [1383 / 2000]: Train Loss=4.7037, Val Cor=0.5850, Time=0.1617 sec\n",
      "Epoch [1384 / 2000]: Train Loss=4.7462, Val Cor=0.5566, Time=0.1622 sec\n",
      "Epoch [1385 / 2000]: Train Loss=4.6830, Val Cor=0.5943, Time=0.1613 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1386 / 2000]: Train Loss=4.8738, Val Cor=0.5170, Time=0.1624 sec\n",
      "Epoch [1387 / 2000]: Train Loss=4.9794, Val Cor=0.4897, Time=0.1622 sec\n",
      "Epoch [1388 / 2000]: Train Loss=5.0940, Val Cor=0.4600, Time=0.1619 sec\n",
      "Epoch [1389 / 2000]: Train Loss=4.8236, Val Cor=0.6228, Time=0.1639 sec\n",
      "Epoch [1390 / 2000]: Train Loss=4.6317, Val Cor=0.5997, Time=0.1637 sec\n",
      "Epoch [1391 / 2000]: Train Loss=4.8012, Val Cor=0.5850, Time=0.1643 sec\n",
      "Epoch [1392 / 2000]: Train Loss=4.8192, Val Cor=0.5880, Time=0.1641 sec\n",
      "Epoch [1393 / 2000]: Train Loss=4.8001, Val Cor=0.5478, Time=0.1616 sec\n",
      "Epoch [1394 / 2000]: Train Loss=4.7614, Val Cor=0.5204, Time=0.1619 sec\n",
      "Epoch [1395 / 2000]: Train Loss=4.8269, Val Cor=0.5110, Time=0.1608 sec\n",
      "Epoch [1396 / 2000]: Train Loss=5.0305, Val Cor=0.5686, Time=0.1623 sec\n",
      "Epoch [1397 / 2000]: Train Loss=4.9147, Val Cor=0.6015, Time=0.1624 sec\n",
      "Epoch [1398 / 2000]: Train Loss=4.8678, Val Cor=0.5317, Time=0.1625 sec\n",
      "Epoch [1399 / 2000]: Train Loss=4.9399, Val Cor=0.2519, Time=0.1644 sec\n",
      "Epoch [1400 / 2000]: Train Loss=5.2968, Val Cor=0.5808, Time=0.1643 sec\n",
      "Epoch [1401 / 2000]: Train Loss=4.6365, Val Cor=0.5259, Time=0.1643 sec\n",
      "Epoch [1402 / 2000]: Train Loss=4.6435, Val Cor=-0.1156, Time=0.1641 sec\n",
      "Epoch [1403 / 2000]: Train Loss=4.8232, Val Cor=0.6278, Time=0.1616 sec\n",
      "Epoch [1404 / 2000]: Train Loss=4.7542, Val Cor=0.5477, Time=0.1617 sec\n",
      "Epoch [1405 / 2000]: Train Loss=5.0242, Val Cor=0.6028, Time=0.1622 sec\n",
      "Epoch [1406 / 2000]: Train Loss=4.8332, Val Cor=0.6002, Time=0.1620 sec\n",
      "Epoch [1407 / 2000]: Train Loss=4.8503, Val Cor=0.2845, Time=0.1640 sec\n",
      "Epoch [1408 / 2000]: Train Loss=5.0272, Val Cor=0.4171, Time=0.1635 sec\n",
      "Epoch [1409 / 2000]: Train Loss=4.7084, Val Cor=0.6038, Time=0.1643 sec\n",
      "Epoch [1410 / 2000]: Train Loss=4.6720, Val Cor=0.4210, Time=0.1640 sec\n",
      "Epoch [1411 / 2000]: Train Loss=4.5725, Val Cor=0.5535, Time=0.1617 sec\n",
      "Epoch [1412 / 2000]: Train Loss=4.6835, Val Cor=0.5899, Time=0.1620 sec\n",
      "Epoch [1413 / 2000]: Train Loss=4.5907, Val Cor=0.6227, Time=0.1624 sec\n",
      "Epoch [1414 / 2000]: Train Loss=4.6754, Val Cor=0.1457, Time=0.1628 sec\n",
      "Epoch [1415 / 2000]: Train Loss=5.8879, Val Cor=0.1556, Time=0.1640 sec\n",
      "Epoch [1416 / 2000]: Train Loss=5.1452, Val Cor=0.6271, Time=0.1641 sec\n",
      "Epoch [1417 / 2000]: Train Loss=5.3320, Val Cor=0.0693, Time=0.1647 sec\n",
      "Epoch [1418 / 2000]: Train Loss=4.9408, Val Cor=0.1312, Time=0.1647 sec\n",
      "Epoch [1419 / 2000]: Train Loss=4.8454, Val Cor=0.0815, Time=0.1638 sec\n",
      "Epoch [1420 / 2000]: Train Loss=5.0935, Val Cor=0.3503, Time=0.1624 sec\n",
      "Epoch [1421 / 2000]: Train Loss=4.6433, Val Cor=-0.0305, Time=0.1622 sec\n",
      "Epoch [1422 / 2000]: Train Loss=4.6691, Val Cor=0.2638, Time=0.1620 sec\n",
      "Epoch [1423 / 2000]: Train Loss=4.7336, Val Cor=0.3144, Time=0.1624 sec\n",
      "Epoch [1424 / 2000]: Train Loss=4.9255, Val Cor=0.4156, Time=0.1627 sec\n",
      "Epoch [1425 / 2000]: Train Loss=4.7864, Val Cor=-0.0856, Time=0.1634 sec\n",
      "Epoch [1426 / 2000]: Train Loss=4.9910, Val Cor=0.5603, Time=0.1653 sec\n",
      "Epoch [1427 / 2000]: Train Loss=4.8022, Val Cor=0.4714, Time=0.1654 sec\n",
      "Epoch [1428 / 2000]: Train Loss=4.7877, Val Cor=0.5867, Time=0.1651 sec\n",
      "Epoch [1429 / 2000]: Train Loss=4.7618, Val Cor=0.5709, Time=0.1652 sec\n",
      "Early stopping triggered.\n",
      "Training RNN model 1:\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1 / 2000]: Train Loss=8.5180, Val Cor=0.0582, Time=0.1673 sec\n",
      "Epoch [2 / 2000]: Train Loss=8.4075, Val Cor=0.2715, Time=0.1660 sec\n",
      "Epoch [3 / 2000]: Train Loss=8.3632, Val Cor=0.3030, Time=0.1665 sec\n",
      "Epoch [4 / 2000]: Train Loss=8.2739, Val Cor=0.3200, Time=0.1661 sec\n",
      "Epoch [5 / 2000]: Train Loss=8.3476, Val Cor=0.3347, Time=0.1666 sec\n",
      "Epoch [6 / 2000]: Train Loss=8.4194, Val Cor=0.3552, Time=0.1661 sec\n",
      "Epoch [7 / 2000]: Train Loss=8.3205, Val Cor=0.3661, Time=0.1670 sec\n",
      "Epoch [8 / 2000]: Train Loss=8.3477, Val Cor=0.3555, Time=0.1669 sec\n",
      "Epoch [9 / 2000]: Train Loss=8.2433, Val Cor=0.3403, Time=0.1669 sec\n",
      "Epoch [10 / 2000]: Train Loss=8.3029, Val Cor=0.3383, Time=0.1651 sec\n",
      "Epoch [11 / 2000]: Train Loss=8.2965, Val Cor=0.3509, Time=0.1644 sec\n",
      "Epoch [12 / 2000]: Train Loss=8.2287, Val Cor=0.3393, Time=0.1653 sec\n",
      "Epoch [13 / 2000]: Train Loss=8.2761, Val Cor=0.3405, Time=0.1650 sec\n",
      "Epoch [14 / 2000]: Train Loss=8.2339, Val Cor=0.3461, Time=0.1661 sec\n",
      "Epoch [15 / 2000]: Train Loss=8.2402, Val Cor=0.3384, Time=0.1661 sec\n",
      "Epoch [16 / 2000]: Train Loss=8.0612, Val Cor=0.3517, Time=0.1650 sec\n",
      "Epoch [17 / 2000]: Train Loss=8.0441, Val Cor=0.3529, Time=0.1644 sec\n",
      "Epoch [18 / 2000]: Train Loss=8.0592, Val Cor=0.3515, Time=0.1633 sec\n",
      "Epoch [19 / 2000]: Train Loss=8.2855, Val Cor=0.3540, Time=0.1629 sec\n",
      "Epoch [20 / 2000]: Train Loss=8.1158, Val Cor=0.3616, Time=0.1620 sec\n",
      "Epoch [21 / 2000]: Train Loss=7.9518, Val Cor=0.3649, Time=0.1634 sec\n",
      "Epoch [22 / 2000]: Train Loss=8.1061, Val Cor=0.3741, Time=0.1626 sec\n",
      "Epoch [23 / 2000]: Train Loss=7.8410, Val Cor=0.3770, Time=0.1627 sec\n",
      "Epoch [24 / 2000]: Train Loss=7.9886, Val Cor=0.3836, Time=0.1623 sec\n",
      "Epoch [25 / 2000]: Train Loss=7.8363, Val Cor=0.3849, Time=0.1631 sec\n",
      "Epoch [26 / 2000]: Train Loss=7.8203, Val Cor=0.3851, Time=0.1625 sec\n",
      "Epoch [27 / 2000]: Train Loss=7.5862, Val Cor=0.3948, Time=0.1628 sec\n",
      "Epoch [28 / 2000]: Train Loss=7.7773, Val Cor=0.3955, Time=0.1639 sec\n",
      "Epoch [29 / 2000]: Train Loss=7.6260, Val Cor=0.4007, Time=0.1646 sec\n",
      "Epoch [30 / 2000]: Train Loss=7.6170, Val Cor=0.4014, Time=0.1642 sec\n",
      "Epoch [31 / 2000]: Train Loss=7.7361, Val Cor=0.3995, Time=0.1641 sec\n",
      "Epoch [32 / 2000]: Train Loss=7.5389, Val Cor=0.4065, Time=0.1648 sec\n",
      "Epoch [33 / 2000]: Train Loss=7.4042, Val Cor=0.4093, Time=0.1646 sec\n",
      "Epoch [34 / 2000]: Train Loss=7.7220, Val Cor=0.4070, Time=0.1646 sec\n",
      "Epoch [35 / 2000]: Train Loss=7.8298, Val Cor=0.4086, Time=0.1638 sec\n",
      "Epoch [36 / 2000]: Train Loss=7.5214, Val Cor=0.4106, Time=0.1626 sec\n",
      "Epoch [37 / 2000]: Train Loss=7.5946, Val Cor=0.4101, Time=0.1627 sec\n",
      "Epoch [38 / 2000]: Train Loss=7.6634, Val Cor=0.4108, Time=0.1623 sec\n",
      "Epoch [39 / 2000]: Train Loss=7.5671, Val Cor=0.4092, Time=0.1618 sec\n",
      "Epoch [40 / 2000]: Train Loss=7.3971, Val Cor=0.4097, Time=0.1631 sec\n",
      "Epoch [41 / 2000]: Train Loss=7.3424, Val Cor=0.4144, Time=0.1635 sec\n",
      "Epoch [42 / 2000]: Train Loss=7.7644, Val Cor=0.4119, Time=0.1630 sec\n",
      "Epoch [43 / 2000]: Train Loss=7.3353, Val Cor=0.4119, Time=0.1629 sec\n",
      "Epoch [44 / 2000]: Train Loss=7.7139, Val Cor=0.4160, Time=0.1646 sec\n",
      "Epoch [45 / 2000]: Train Loss=7.3612, Val Cor=0.4136, Time=0.1646 sec\n",
      "Epoch [46 / 2000]: Train Loss=7.0828, Val Cor=0.4196, Time=0.1636 sec\n",
      "Epoch [47 / 2000]: Train Loss=7.0888, Val Cor=0.4222, Time=0.1628 sec\n",
      "Epoch [48 / 2000]: Train Loss=7.0007, Val Cor=0.4240, Time=0.1628 sec\n",
      "Epoch [49 / 2000]: Train Loss=7.3043, Val Cor=0.4253, Time=0.1631 sec\n",
      "Epoch [50 / 2000]: Train Loss=7.1395, Val Cor=0.4262, Time=0.1628 sec\n",
      "Epoch [51 / 2000]: Train Loss=7.0362, Val Cor=0.4259, Time=0.1636 sec\n",
      "Epoch [52 / 2000]: Train Loss=7.2481, Val Cor=0.4273, Time=0.1646 sec\n",
      "Epoch [53 / 2000]: Train Loss=7.1492, Val Cor=0.4274, Time=0.1651 sec\n",
      "Epoch [54 / 2000]: Train Loss=7.0484, Val Cor=0.4286, Time=0.1647 sec\n",
      "Epoch [55 / 2000]: Train Loss=7.4253, Val Cor=0.4241, Time=0.1652 sec\n",
      "Epoch [56 / 2000]: Train Loss=7.0506, Val Cor=0.4260, Time=0.1654 sec\n",
      "Epoch [57 / 2000]: Train Loss=7.0441, Val Cor=0.4275, Time=0.1642 sec\n",
      "Epoch [58 / 2000]: Train Loss=6.9650, Val Cor=0.4319, Time=0.1633 sec\n",
      "Epoch [59 / 2000]: Train Loss=7.0634, Val Cor=0.4319, Time=0.1627 sec\n",
      "Epoch [60 / 2000]: Train Loss=7.0787, Val Cor=0.4299, Time=0.1621 sec\n",
      "Epoch [61 / 2000]: Train Loss=6.6658, Val Cor=0.4335, Time=0.1622 sec\n",
      "Epoch [62 / 2000]: Train Loss=6.8474, Val Cor=0.4331, Time=0.1619 sec\n",
      "Epoch [63 / 2000]: Train Loss=7.0590, Val Cor=0.4335, Time=0.1629 sec\n",
      "Epoch [64 / 2000]: Train Loss=6.8681, Val Cor=0.4336, Time=0.1627 sec\n",
      "Epoch [65 / 2000]: Train Loss=7.1825, Val Cor=0.4331, Time=0.1637 sec\n",
      "Epoch [66 / 2000]: Train Loss=7.0615, Val Cor=0.4362, Time=0.1630 sec\n",
      "Epoch [67 / 2000]: Train Loss=6.9049, Val Cor=0.4355, Time=0.1628 sec\n",
      "Epoch [68 / 2000]: Train Loss=7.2275, Val Cor=0.4384, Time=0.1649 sec\n",
      "Epoch [69 / 2000]: Train Loss=6.8696, Val Cor=0.4391, Time=0.1647 sec\n",
      "Epoch [70 / 2000]: Train Loss=6.8973, Val Cor=0.4402, Time=0.1648 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71 / 2000]: Train Loss=7.1235, Val Cor=0.4391, Time=0.1665 sec\n",
      "Epoch [72 / 2000]: Train Loss=6.8554, Val Cor=0.4412, Time=0.1653 sec\n",
      "Epoch [73 / 2000]: Train Loss=6.8699, Val Cor=0.4431, Time=0.1649 sec\n",
      "Epoch [74 / 2000]: Train Loss=7.0659, Val Cor=0.4400, Time=0.1649 sec\n",
      "Epoch [75 / 2000]: Train Loss=6.8439, Val Cor=0.4408, Time=0.1668 sec\n",
      "Epoch [76 / 2000]: Train Loss=6.7585, Val Cor=0.4439, Time=0.1644 sec\n",
      "Epoch [77 / 2000]: Train Loss=6.9735, Val Cor=0.4406, Time=0.1644 sec\n",
      "Epoch [78 / 2000]: Train Loss=6.5643, Val Cor=0.4419, Time=0.1641 sec\n",
      "Epoch [79 / 2000]: Train Loss=6.6042, Val Cor=0.4426, Time=0.1628 sec\n",
      "Epoch [80 / 2000]: Train Loss=6.6844, Val Cor=0.4437, Time=0.1627 sec\n",
      "Epoch [81 / 2000]: Train Loss=6.7795, Val Cor=0.4442, Time=0.1630 sec\n",
      "Epoch [82 / 2000]: Train Loss=6.8850, Val Cor=0.4427, Time=0.1627 sec\n",
      "Epoch [83 / 2000]: Train Loss=6.7357, Val Cor=0.4442, Time=0.1633 sec\n",
      "Epoch [84 / 2000]: Train Loss=6.6070, Val Cor=0.4493, Time=0.1627 sec\n",
      "Epoch [85 / 2000]: Train Loss=6.9704, Val Cor=0.4469, Time=0.1631 sec\n",
      "Epoch [86 / 2000]: Train Loss=6.8417, Val Cor=0.4453, Time=0.1647 sec\n",
      "Epoch [87 / 2000]: Train Loss=6.6374, Val Cor=0.4453, Time=0.1656 sec\n",
      "Epoch [88 / 2000]: Train Loss=6.7148, Val Cor=0.4454, Time=0.1655 sec\n",
      "Epoch [89 / 2000]: Train Loss=6.6999, Val Cor=0.4446, Time=0.1667 sec\n",
      "Epoch [90 / 2000]: Train Loss=6.8487, Val Cor=0.4467, Time=0.1662 sec\n",
      "Epoch [91 / 2000]: Train Loss=6.8379, Val Cor=0.4449, Time=0.1647 sec\n",
      "Epoch [92 / 2000]: Train Loss=6.8118, Val Cor=0.4434, Time=0.1638 sec\n",
      "Epoch [93 / 2000]: Train Loss=6.6377, Val Cor=0.4457, Time=0.1637 sec\n",
      "Epoch [94 / 2000]: Train Loss=6.5888, Val Cor=0.4472, Time=0.1648 sec\n",
      "Epoch [95 / 2000]: Train Loss=6.4487, Val Cor=0.4476, Time=0.1649 sec\n",
      "Epoch [96 / 2000]: Train Loss=6.4711, Val Cor=0.4468, Time=0.1660 sec\n",
      "Epoch [97 / 2000]: Train Loss=6.7427, Val Cor=0.4468, Time=0.1663 sec\n",
      "Epoch [98 / 2000]: Train Loss=6.5930, Val Cor=0.4466, Time=0.1636 sec\n",
      "Epoch [99 / 2000]: Train Loss=6.8652, Val Cor=0.4485, Time=0.1646 sec\n",
      "Epoch [100 / 2000]: Train Loss=6.6151, Val Cor=0.4478, Time=0.1642 sec\n",
      "Epoch [101 / 2000]: Train Loss=7.4045, Val Cor=0.4481, Time=0.1622 sec\n",
      "Epoch [102 / 2000]: Train Loss=6.5858, Val Cor=0.4487, Time=0.1618 sec\n",
      "Epoch [103 / 2000]: Train Loss=6.8466, Val Cor=0.4481, Time=0.1622 sec\n",
      "Epoch [104 / 2000]: Train Loss=6.9900, Val Cor=0.4485, Time=0.1626 sec\n",
      "Epoch [105 / 2000]: Train Loss=6.4318, Val Cor=0.4477, Time=0.1630 sec\n",
      "Epoch [106 / 2000]: Train Loss=6.7024, Val Cor=0.4476, Time=0.1627 sec\n",
      "Epoch [107 / 2000]: Train Loss=6.5450, Val Cor=0.4460, Time=0.1628 sec\n",
      "Epoch [108 / 2000]: Train Loss=7.0030, Val Cor=0.4462, Time=0.1643 sec\n",
      "Epoch [109 / 2000]: Train Loss=6.5013, Val Cor=0.4469, Time=0.1655 sec\n",
      "Epoch [110 / 2000]: Train Loss=6.6216, Val Cor=0.4479, Time=0.1662 sec\n",
      "Epoch [111 / 2000]: Train Loss=6.5323, Val Cor=0.4473, Time=0.1663 sec\n",
      "Epoch [112 / 2000]: Train Loss=6.7789, Val Cor=0.4467, Time=0.1645 sec\n",
      "Epoch [113 / 2000]: Train Loss=6.6254, Val Cor=0.4464, Time=0.1640 sec\n",
      "Epoch [114 / 2000]: Train Loss=6.5309, Val Cor=0.4459, Time=0.1622 sec\n",
      "Epoch [115 / 2000]: Train Loss=6.8149, Val Cor=0.4475, Time=0.1645 sec\n",
      "Epoch [116 / 2000]: Train Loss=6.6717, Val Cor=0.4465, Time=0.1647 sec\n",
      "Epoch [117 / 2000]: Train Loss=6.3316, Val Cor=0.4478, Time=0.1643 sec\n",
      "Epoch [118 / 2000]: Train Loss=6.8650, Val Cor=0.4493, Time=0.1648 sec\n",
      "Epoch [119 / 2000]: Train Loss=6.7476, Val Cor=0.4480, Time=0.1658 sec\n",
      "Epoch [120 / 2000]: Train Loss=6.3717, Val Cor=0.4492, Time=0.1679 sec\n",
      "Epoch [121 / 2000]: Train Loss=6.2289, Val Cor=0.4499, Time=0.1661 sec\n",
      "Epoch [122 / 2000]: Train Loss=6.4651, Val Cor=0.4491, Time=0.1624 sec\n",
      "Epoch [123 / 2000]: Train Loss=6.5856, Val Cor=0.4493, Time=0.1627 sec\n",
      "Epoch [124 / 2000]: Train Loss=6.6031, Val Cor=0.4489, Time=0.1616 sec\n",
      "Epoch [125 / 2000]: Train Loss=6.5766, Val Cor=0.4466, Time=0.1633 sec\n",
      "Epoch [126 / 2000]: Train Loss=6.4445, Val Cor=0.4523, Time=0.1630 sec\n",
      "Epoch [127 / 2000]: Train Loss=6.5327, Val Cor=0.4498, Time=0.1628 sec\n",
      "Epoch [128 / 2000]: Train Loss=6.3456, Val Cor=0.4511, Time=0.1657 sec\n",
      "Epoch [129 / 2000]: Train Loss=6.2374, Val Cor=0.4485, Time=0.1660 sec\n",
      "Epoch [130 / 2000]: Train Loss=6.8560, Val Cor=0.4505, Time=0.1665 sec\n",
      "Epoch [131 / 2000]: Train Loss=6.4854, Val Cor=0.4507, Time=0.1663 sec\n",
      "Epoch [132 / 2000]: Train Loss=6.4186, Val Cor=0.4524, Time=0.1656 sec\n",
      "Epoch [133 / 2000]: Train Loss=6.2865, Val Cor=0.4534, Time=0.1657 sec\n",
      "Epoch [134 / 2000]: Train Loss=6.1331, Val Cor=0.4538, Time=0.1654 sec\n",
      "Epoch [135 / 2000]: Train Loss=6.4287, Val Cor=0.4511, Time=0.1656 sec\n",
      "Epoch [136 / 2000]: Train Loss=6.4162, Val Cor=0.4538, Time=0.1645 sec\n",
      "Epoch [137 / 2000]: Train Loss=6.5053, Val Cor=0.4518, Time=0.1646 sec\n",
      "Epoch [138 / 2000]: Train Loss=6.4133, Val Cor=0.4532, Time=0.1639 sec\n",
      "Epoch [139 / 2000]: Train Loss=6.0873, Val Cor=0.4573, Time=0.1650 sec\n",
      "Epoch [140 / 2000]: Train Loss=6.1505, Val Cor=0.4545, Time=0.1648 sec\n",
      "Epoch [141 / 2000]: Train Loss=6.1088, Val Cor=0.4556, Time=0.1645 sec\n",
      "Epoch [142 / 2000]: Train Loss=6.3860, Val Cor=0.4584, Time=0.1619 sec\n",
      "Epoch [143 / 2000]: Train Loss=6.3143, Val Cor=0.4543, Time=0.1623 sec\n",
      "Epoch [144 / 2000]: Train Loss=6.2942, Val Cor=0.4556, Time=0.1651 sec\n",
      "Epoch [145 / 2000]: Train Loss=6.2318, Val Cor=0.4552, Time=0.1647 sec\n",
      "Epoch [146 / 2000]: Train Loss=6.0055, Val Cor=0.4549, Time=0.1642 sec\n",
      "Epoch [147 / 2000]: Train Loss=6.0869, Val Cor=0.4565, Time=0.1649 sec\n",
      "Epoch [148 / 2000]: Train Loss=6.0124, Val Cor=0.4582, Time=0.1646 sec\n",
      "Epoch [149 / 2000]: Train Loss=6.2479, Val Cor=0.4560, Time=0.1646 sec\n",
      "Epoch [150 / 2000]: Train Loss=6.2685, Val Cor=0.4526, Time=0.1632 sec\n",
      "Epoch [151 / 2000]: Train Loss=6.4510, Val Cor=0.4535, Time=0.1629 sec\n",
      "Epoch [152 / 2000]: Train Loss=6.4287, Val Cor=0.4487, Time=0.1620 sec\n",
      "Epoch [153 / 2000]: Train Loss=6.3490, Val Cor=0.4574, Time=0.1622 sec\n",
      "Epoch [154 / 2000]: Train Loss=6.1602, Val Cor=0.4597, Time=0.1629 sec\n",
      "Epoch [155 / 2000]: Train Loss=5.9983, Val Cor=0.4638, Time=0.1633 sec\n",
      "Epoch [156 / 2000]: Train Loss=6.4443, Val Cor=0.4610, Time=0.1636 sec\n",
      "Epoch [157 / 2000]: Train Loss=6.4926, Val Cor=0.4568, Time=0.1632 sec\n",
      "Epoch [158 / 2000]: Train Loss=6.2047, Val Cor=0.4600, Time=0.1647 sec\n",
      "Epoch [159 / 2000]: Train Loss=6.4385, Val Cor=0.4588, Time=0.1654 sec\n",
      "Epoch [160 / 2000]: Train Loss=6.8225, Val Cor=0.4588, Time=0.1657 sec\n",
      "Epoch [161 / 2000]: Train Loss=6.4632, Val Cor=0.4600, Time=0.1669 sec\n",
      "Epoch [162 / 2000]: Train Loss=6.1582, Val Cor=0.4564, Time=0.1660 sec\n",
      "Epoch [163 / 2000]: Train Loss=6.0502, Val Cor=0.4566, Time=0.1667 sec\n",
      "Epoch [164 / 2000]: Train Loss=6.3793, Val Cor=0.4573, Time=0.1642 sec\n",
      "Epoch [165 / 2000]: Train Loss=6.0929, Val Cor=0.4572, Time=0.1634 sec\n",
      "Epoch [166 / 2000]: Train Loss=6.5644, Val Cor=0.4599, Time=0.1623 sec\n",
      "Epoch [167 / 2000]: Train Loss=6.4408, Val Cor=0.4587, Time=0.1632 sec\n",
      "Epoch [168 / 2000]: Train Loss=6.0114, Val Cor=0.4579, Time=0.1635 sec\n",
      "Epoch [169 / 2000]: Train Loss=6.2413, Val Cor=0.4546, Time=0.1636 sec\n",
      "Epoch [170 / 2000]: Train Loss=6.1547, Val Cor=0.4563, Time=0.1648 sec\n",
      "Epoch [171 / 2000]: Train Loss=6.2133, Val Cor=0.4591, Time=0.1655 sec\n",
      "Epoch [172 / 2000]: Train Loss=6.0639, Val Cor=0.4555, Time=0.1646 sec\n",
      "Epoch [173 / 2000]: Train Loss=6.0575, Val Cor=0.4555, Time=0.1644 sec\n",
      "Epoch [174 / 2000]: Train Loss=6.0270, Val Cor=0.4562, Time=0.1650 sec\n",
      "Epoch [175 / 2000]: Train Loss=6.1269, Val Cor=0.4549, Time=0.1660 sec\n",
      "Epoch [176 / 2000]: Train Loss=6.2869, Val Cor=0.4563, Time=0.1643 sec\n",
      "Epoch [177 / 2000]: Train Loss=6.2411, Val Cor=0.4594, Time=0.1645 sec\n",
      "Epoch [178 / 2000]: Train Loss=6.0981, Val Cor=0.4592, Time=0.1663 sec\n",
      "Epoch [179 / 2000]: Train Loss=6.7330, Val Cor=0.4560, Time=0.1664 sec\n",
      "Epoch [180 / 2000]: Train Loss=6.2659, Val Cor=0.4589, Time=0.1659 sec\n",
      "Epoch [181 / 2000]: Train Loss=6.0475, Val Cor=0.4529, Time=0.1650 sec\n",
      "Epoch [182 / 2000]: Train Loss=6.0830, Val Cor=0.4590, Time=0.1638 sec\n",
      "Epoch [183 / 2000]: Train Loss=6.4132, Val Cor=0.4616, Time=0.1651 sec\n",
      "Epoch [184 / 2000]: Train Loss=6.2750, Val Cor=0.4609, Time=0.1650 sec\n",
      "Epoch [185 / 2000]: Train Loss=6.2217, Val Cor=0.4589, Time=0.1647 sec\n",
      "Epoch [186 / 2000]: Train Loss=5.8944, Val Cor=0.4555, Time=0.1630 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [187 / 2000]: Train Loss=6.5151, Val Cor=0.4576, Time=0.1649 sec\n",
      "Epoch [188 / 2000]: Train Loss=6.1392, Val Cor=0.4637, Time=0.1639 sec\n",
      "Epoch [189 / 2000]: Train Loss=6.1590, Val Cor=0.4547, Time=0.1650 sec\n",
      "Epoch [190 / 2000]: Train Loss=6.0259, Val Cor=0.4640, Time=0.1640 sec\n",
      "Epoch [191 / 2000]: Train Loss=6.1177, Val Cor=0.4639, Time=0.1645 sec\n",
      "Epoch [192 / 2000]: Train Loss=6.2145, Val Cor=0.4645, Time=0.1624 sec\n",
      "Epoch [193 / 2000]: Train Loss=6.2261, Val Cor=0.4668, Time=0.1623 sec\n",
      "Epoch [194 / 2000]: Train Loss=6.4018, Val Cor=0.4572, Time=0.1621 sec\n",
      "Epoch [195 / 2000]: Train Loss=6.3192, Val Cor=0.4629, Time=0.1621 sec\n",
      "Epoch [196 / 2000]: Train Loss=6.3127, Val Cor=0.4559, Time=0.1630 sec\n",
      "Epoch [197 / 2000]: Train Loss=6.2515, Val Cor=0.4651, Time=0.1633 sec\n",
      "Epoch [198 / 2000]: Train Loss=5.9873, Val Cor=0.4661, Time=0.1630 sec\n",
      "Epoch [199 / 2000]: Train Loss=5.9085, Val Cor=0.4642, Time=0.1654 sec\n",
      "Epoch [200 / 2000]: Train Loss=6.4956, Val Cor=0.4665, Time=0.1645 sec\n",
      "Epoch [201 / 2000]: Train Loss=6.1383, Val Cor=0.4695, Time=0.1648 sec\n",
      "Epoch [202 / 2000]: Train Loss=6.0793, Val Cor=0.4620, Time=0.1642 sec\n",
      "Epoch [203 / 2000]: Train Loss=6.0249, Val Cor=0.4606, Time=0.1665 sec\n",
      "Epoch [204 / 2000]: Train Loss=6.3640, Val Cor=0.4601, Time=0.1665 sec\n",
      "Epoch [205 / 2000]: Train Loss=6.1947, Val Cor=0.4594, Time=0.1665 sec\n",
      "Epoch [206 / 2000]: Train Loss=6.3409, Val Cor=0.4680, Time=0.1654 sec\n",
      "Epoch [207 / 2000]: Train Loss=6.4865, Val Cor=0.4617, Time=0.1649 sec\n",
      "Epoch [208 / 2000]: Train Loss=6.2439, Val Cor=0.4702, Time=0.1619 sec\n",
      "Epoch [209 / 2000]: Train Loss=6.1950, Val Cor=0.4613, Time=0.1618 sec\n",
      "Epoch [210 / 2000]: Train Loss=5.9222, Val Cor=0.4665, Time=0.1625 sec\n",
      "Epoch [211 / 2000]: Train Loss=5.8399, Val Cor=0.4594, Time=0.1630 sec\n",
      "Epoch [212 / 2000]: Train Loss=6.4358, Val Cor=0.4672, Time=0.1634 sec\n",
      "Epoch [213 / 2000]: Train Loss=6.1147, Val Cor=0.4666, Time=0.1631 sec\n",
      "Epoch [214 / 2000]: Train Loss=6.1025, Val Cor=0.4648, Time=0.1646 sec\n",
      "Epoch [215 / 2000]: Train Loss=6.4270, Val Cor=0.4596, Time=0.1693 sec\n",
      "Epoch [216 / 2000]: Train Loss=6.2423, Val Cor=0.4638, Time=0.1638 sec\n",
      "Epoch [217 / 2000]: Train Loss=6.2524, Val Cor=0.4653, Time=0.1638 sec\n",
      "Epoch [218 / 2000]: Train Loss=6.0822, Val Cor=0.4623, Time=0.1644 sec\n",
      "Epoch [219 / 2000]: Train Loss=6.1647, Val Cor=0.4605, Time=0.1648 sec\n",
      "Epoch [220 / 2000]: Train Loss=5.9563, Val Cor=0.4630, Time=0.1634 sec\n",
      "Epoch [221 / 2000]: Train Loss=6.3404, Val Cor=0.4625, Time=0.1630 sec\n",
      "Epoch [222 / 2000]: Train Loss=5.9396, Val Cor=0.4744, Time=0.1629 sec\n",
      "Epoch [223 / 2000]: Train Loss=5.8345, Val Cor=0.4686, Time=0.1635 sec\n",
      "Epoch [224 / 2000]: Train Loss=5.8546, Val Cor=0.4617, Time=0.1624 sec\n",
      "Epoch [225 / 2000]: Train Loss=5.9607, Val Cor=0.4797, Time=0.1628 sec\n",
      "Epoch [226 / 2000]: Train Loss=6.1397, Val Cor=0.4632, Time=0.1627 sec\n",
      "Epoch [227 / 2000]: Train Loss=6.0920, Val Cor=0.4776, Time=0.1627 sec\n",
      "Epoch [228 / 2000]: Train Loss=5.8997, Val Cor=0.4731, Time=0.1625 sec\n",
      "Epoch [229 / 2000]: Train Loss=5.7565, Val Cor=0.4772, Time=0.1642 sec\n",
      "Epoch [230 / 2000]: Train Loss=5.9683, Val Cor=0.4618, Time=0.1646 sec\n",
      "Epoch [231 / 2000]: Train Loss=6.5390, Val Cor=0.4676, Time=0.1643 sec\n",
      "Epoch [232 / 2000]: Train Loss=6.1124, Val Cor=0.4633, Time=0.1646 sec\n",
      "Epoch [233 / 2000]: Train Loss=6.2584, Val Cor=0.4659, Time=0.1647 sec\n",
      "Epoch [234 / 2000]: Train Loss=6.0058, Val Cor=0.4692, Time=0.1624 sec\n",
      "Epoch [235 / 2000]: Train Loss=6.0078, Val Cor=0.4623, Time=0.1615 sec\n",
      "Epoch [236 / 2000]: Train Loss=6.2766, Val Cor=0.4758, Time=0.1620 sec\n",
      "Epoch [237 / 2000]: Train Loss=6.3161, Val Cor=0.4619, Time=0.1630 sec\n",
      "Epoch [238 / 2000]: Train Loss=5.9454, Val Cor=0.4773, Time=0.1628 sec\n",
      "Epoch [239 / 2000]: Train Loss=5.8822, Val Cor=0.4671, Time=0.1636 sec\n",
      "Epoch [240 / 2000]: Train Loss=6.3885, Val Cor=0.4627, Time=0.1647 sec\n",
      "Epoch [241 / 2000]: Train Loss=6.0062, Val Cor=0.4630, Time=0.1646 sec\n",
      "Epoch [242 / 2000]: Train Loss=6.0325, Val Cor=0.4750, Time=0.1648 sec\n",
      "Epoch [243 / 2000]: Train Loss=5.7909, Val Cor=0.4776, Time=0.1661 sec\n",
      "Epoch [244 / 2000]: Train Loss=5.9095, Val Cor=0.4667, Time=0.1661 sec\n",
      "Epoch [245 / 2000]: Train Loss=5.9894, Val Cor=0.4773, Time=0.1648 sec\n",
      "Epoch [246 / 2000]: Train Loss=5.8466, Val Cor=0.4665, Time=0.1623 sec\n",
      "Epoch [247 / 2000]: Train Loss=5.9002, Val Cor=0.4652, Time=0.1624 sec\n",
      "Epoch [248 / 2000]: Train Loss=5.9467, Val Cor=0.4706, Time=0.1625 sec\n",
      "Epoch [249 / 2000]: Train Loss=5.9661, Val Cor=0.4770, Time=0.1631 sec\n",
      "Epoch [250 / 2000]: Train Loss=5.7893, Val Cor=0.4699, Time=0.1625 sec\n",
      "Epoch [251 / 2000]: Train Loss=5.8662, Val Cor=0.4822, Time=0.1646 sec\n",
      "Epoch [252 / 2000]: Train Loss=5.9572, Val Cor=0.4675, Time=0.1647 sec\n",
      "Epoch [253 / 2000]: Train Loss=5.7813, Val Cor=0.4746, Time=0.1640 sec\n",
      "Epoch [254 / 2000]: Train Loss=6.2117, Val Cor=0.4782, Time=0.1646 sec\n",
      "Epoch [255 / 2000]: Train Loss=6.1226, Val Cor=0.4778, Time=0.1644 sec\n",
      "Epoch [256 / 2000]: Train Loss=5.8665, Val Cor=0.4674, Time=0.1624 sec\n",
      "Epoch [257 / 2000]: Train Loss=5.6795, Val Cor=0.4663, Time=0.1620 sec\n",
      "Epoch [258 / 2000]: Train Loss=5.7823, Val Cor=0.4756, Time=0.1618 sec\n",
      "Epoch [259 / 2000]: Train Loss=5.8696, Val Cor=0.4811, Time=0.1623 sec\n",
      "Epoch [260 / 2000]: Train Loss=6.1123, Val Cor=0.4697, Time=0.1627 sec\n",
      "Epoch [261 / 2000]: Train Loss=5.9914, Val Cor=0.4715, Time=0.1631 sec\n",
      "Epoch [262 / 2000]: Train Loss=6.0344, Val Cor=0.4685, Time=0.1631 sec\n",
      "Epoch [263 / 2000]: Train Loss=6.2865, Val Cor=0.4813, Time=0.1651 sec\n",
      "Epoch [264 / 2000]: Train Loss=5.9889, Val Cor=0.4727, Time=0.1641 sec\n",
      "Epoch [265 / 2000]: Train Loss=5.9081, Val Cor=0.4830, Time=0.1644 sec\n",
      "Epoch [266 / 2000]: Train Loss=5.8652, Val Cor=0.4804, Time=0.1645 sec\n",
      "Epoch [267 / 2000]: Train Loss=6.1020, Val Cor=0.4804, Time=0.1645 sec\n",
      "Epoch [268 / 2000]: Train Loss=5.7905, Val Cor=0.4711, Time=0.1644 sec\n",
      "Epoch [269 / 2000]: Train Loss=6.1391, Val Cor=0.4789, Time=0.1623 sec\n",
      "Epoch [270 / 2000]: Train Loss=5.9810, Val Cor=0.4738, Time=0.1621 sec\n",
      "Epoch [271 / 2000]: Train Loss=6.0951, Val Cor=0.4708, Time=0.1634 sec\n",
      "Epoch [272 / 2000]: Train Loss=6.1298, Val Cor=0.4803, Time=0.1635 sec\n",
      "Epoch [273 / 2000]: Train Loss=6.2078, Val Cor=0.4776, Time=0.1633 sec\n",
      "Epoch [274 / 2000]: Train Loss=6.2530, Val Cor=0.4651, Time=0.1649 sec\n",
      "Epoch [275 / 2000]: Train Loss=6.3414, Val Cor=0.4744, Time=0.1654 sec\n",
      "Epoch [276 / 2000]: Train Loss=5.8171, Val Cor=0.4746, Time=0.1645 sec\n",
      "Epoch [277 / 2000]: Train Loss=5.9182, Val Cor=0.4679, Time=0.1718 sec\n",
      "Epoch [278 / 2000]: Train Loss=5.9702, Val Cor=0.4738, Time=0.1673 sec\n",
      "Epoch [279 / 2000]: Train Loss=5.9123, Val Cor=0.4759, Time=0.1668 sec\n",
      "Epoch [280 / 2000]: Train Loss=5.9656, Val Cor=0.4726, Time=0.1654 sec\n",
      "Epoch [281 / 2000]: Train Loss=6.0052, Val Cor=0.4563, Time=0.1651 sec\n",
      "Epoch [282 / 2000]: Train Loss=6.0512, Val Cor=0.4739, Time=0.1643 sec\n",
      "Epoch [283 / 2000]: Train Loss=5.8945, Val Cor=0.4826, Time=0.1653 sec\n",
      "Epoch [284 / 2000]: Train Loss=5.8226, Val Cor=0.4821, Time=0.1628 sec\n",
      "Epoch [285 / 2000]: Train Loss=5.8012, Val Cor=0.4832, Time=0.1624 sec\n",
      "Epoch [286 / 2000]: Train Loss=6.2169, Val Cor=0.4779, Time=0.1622 sec\n",
      "Epoch [287 / 2000]: Train Loss=5.9804, Val Cor=0.4845, Time=0.1646 sec\n",
      "Epoch [288 / 2000]: Train Loss=5.7896, Val Cor=0.4787, Time=0.1645 sec\n",
      "Epoch [289 / 2000]: Train Loss=5.9210, Val Cor=0.4722, Time=0.1639 sec\n",
      "Epoch [290 / 2000]: Train Loss=6.2333, Val Cor=0.4660, Time=0.1632 sec\n",
      "Epoch [291 / 2000]: Train Loss=6.0556, Val Cor=0.4724, Time=0.1647 sec\n",
      "Epoch [292 / 2000]: Train Loss=6.1076, Val Cor=0.4612, Time=0.1641 sec\n",
      "Epoch [293 / 2000]: Train Loss=6.2178, Val Cor=0.4746, Time=0.1627 sec\n",
      "Epoch [294 / 2000]: Train Loss=5.9474, Val Cor=0.4786, Time=0.1618 sec\n",
      "Epoch [295 / 2000]: Train Loss=6.0303, Val Cor=0.4801, Time=0.1630 sec\n",
      "Epoch [296 / 2000]: Train Loss=5.6989, Val Cor=0.4752, Time=0.1632 sec\n",
      "Epoch [297 / 2000]: Train Loss=5.9485, Val Cor=0.4839, Time=0.1634 sec\n",
      "Epoch [298 / 2000]: Train Loss=5.9306, Val Cor=0.4694, Time=0.1634 sec\n",
      "Epoch [299 / 2000]: Train Loss=6.0072, Val Cor=0.4713, Time=0.1650 sec\n",
      "Epoch [300 / 2000]: Train Loss=5.6538, Val Cor=0.4869, Time=0.1649 sec\n",
      "Epoch [301 / 2000]: Train Loss=5.9270, Val Cor=0.4748, Time=0.1652 sec\n",
      "Epoch [302 / 2000]: Train Loss=5.8142, Val Cor=0.4750, Time=0.1654 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [303 / 2000]: Train Loss=5.6560, Val Cor=0.4769, Time=0.1652 sec\n",
      "Epoch [304 / 2000]: Train Loss=5.8491, Val Cor=0.4754, Time=0.1659 sec\n",
      "Epoch [305 / 2000]: Train Loss=5.7211, Val Cor=0.4866, Time=0.1666 sec\n",
      "Epoch [306 / 2000]: Train Loss=5.7884, Val Cor=0.4897, Time=0.1659 sec\n",
      "Epoch [307 / 2000]: Train Loss=5.8858, Val Cor=0.4884, Time=0.1655 sec\n",
      "Epoch [308 / 2000]: Train Loss=6.0762, Val Cor=0.4681, Time=0.1642 sec\n",
      "Epoch [309 / 2000]: Train Loss=5.9563, Val Cor=0.4886, Time=0.1643 sec\n",
      "Epoch [310 / 2000]: Train Loss=6.0161, Val Cor=0.4826, Time=0.1644 sec\n",
      "Epoch [311 / 2000]: Train Loss=6.0772, Val Cor=0.4820, Time=0.1637 sec\n",
      "Epoch [312 / 2000]: Train Loss=5.8357, Val Cor=0.4815, Time=0.1628 sec\n",
      "Epoch [313 / 2000]: Train Loss=5.9921, Val Cor=0.4787, Time=0.1650 sec\n",
      "Epoch [314 / 2000]: Train Loss=5.9341, Val Cor=0.4781, Time=0.1642 sec\n",
      "Epoch [315 / 2000]: Train Loss=5.8606, Val Cor=0.4757, Time=0.1648 sec\n",
      "Epoch [316 / 2000]: Train Loss=5.9114, Val Cor=0.4857, Time=0.1643 sec\n",
      "Epoch [317 / 2000]: Train Loss=5.9894, Val Cor=0.4862, Time=0.1630 sec\n",
      "Epoch [318 / 2000]: Train Loss=5.9041, Val Cor=0.4897, Time=0.1623 sec\n",
      "Epoch [319 / 2000]: Train Loss=5.7998, Val Cor=0.4868, Time=0.1640 sec\n",
      "Epoch [320 / 2000]: Train Loss=5.8289, Val Cor=0.4805, Time=0.1633 sec\n",
      "Epoch [321 / 2000]: Train Loss=5.8599, Val Cor=0.4800, Time=0.1637 sec\n",
      "Epoch [322 / 2000]: Train Loss=6.0239, Val Cor=0.4849, Time=0.1633 sec\n",
      "Epoch [323 / 2000]: Train Loss=5.8414, Val Cor=0.4889, Time=0.1641 sec\n",
      "Epoch [324 / 2000]: Train Loss=5.8703, Val Cor=0.4859, Time=0.1658 sec\n",
      "Epoch [325 / 2000]: Train Loss=5.7873, Val Cor=0.4877, Time=0.1671 sec\n",
      "Epoch [326 / 2000]: Train Loss=6.2504, Val Cor=0.3185, Time=0.1660 sec\n",
      "Epoch [327 / 2000]: Train Loss=6.3521, Val Cor=0.2726, Time=0.1657 sec\n",
      "Epoch [328 / 2000]: Train Loss=5.9296, Val Cor=0.4676, Time=0.1649 sec\n",
      "Epoch [329 / 2000]: Train Loss=5.9018, Val Cor=0.4821, Time=0.1652 sec\n",
      "Epoch [330 / 2000]: Train Loss=5.7740, Val Cor=0.4888, Time=0.1643 sec\n",
      "Epoch [331 / 2000]: Train Loss=5.8780, Val Cor=0.4612, Time=0.1640 sec\n",
      "Epoch [332 / 2000]: Train Loss=5.9154, Val Cor=0.4739, Time=0.1628 sec\n",
      "Epoch [333 / 2000]: Train Loss=5.9853, Val Cor=0.4852, Time=0.1621 sec\n",
      "Epoch [334 / 2000]: Train Loss=6.0026, Val Cor=0.4581, Time=0.1628 sec\n",
      "Epoch [335 / 2000]: Train Loss=6.2144, Val Cor=-0.1806, Time=0.1633 sec\n",
      "Epoch [336 / 2000]: Train Loss=5.9436, Val Cor=0.4815, Time=0.1634 sec\n",
      "Epoch [337 / 2000]: Train Loss=5.9527, Val Cor=0.4834, Time=0.1631 sec\n",
      "Epoch [338 / 2000]: Train Loss=5.7648, Val Cor=0.4923, Time=0.1634 sec\n",
      "Epoch [339 / 2000]: Train Loss=6.0283, Val Cor=0.4885, Time=0.1648 sec\n",
      "Epoch [340 / 2000]: Train Loss=6.0659, Val Cor=0.4910, Time=0.1678 sec\n",
      "Epoch [341 / 2000]: Train Loss=5.9483, Val Cor=0.4797, Time=0.1660 sec\n",
      "Epoch [342 / 2000]: Train Loss=5.8097, Val Cor=0.4832, Time=0.1652 sec\n",
      "Epoch [343 / 2000]: Train Loss=5.8173, Val Cor=0.4789, Time=0.1662 sec\n",
      "Epoch [344 / 2000]: Train Loss=6.1209, Val Cor=0.4776, Time=0.1647 sec\n",
      "Epoch [345 / 2000]: Train Loss=5.9065, Val Cor=0.4972, Time=0.1645 sec\n",
      "Epoch [346 / 2000]: Train Loss=5.7804, Val Cor=0.4882, Time=0.1639 sec\n",
      "Epoch [347 / 2000]: Train Loss=6.2852, Val Cor=0.4694, Time=0.1627 sec\n",
      "Epoch [348 / 2000]: Train Loss=6.0940, Val Cor=0.4843, Time=0.1620 sec\n",
      "Epoch [349 / 2000]: Train Loss=5.8496, Val Cor=0.4870, Time=0.1619 sec\n",
      "Epoch [350 / 2000]: Train Loss=5.7298, Val Cor=0.4760, Time=0.1624 sec\n",
      "Epoch [351 / 2000]: Train Loss=5.7489, Val Cor=0.4871, Time=0.1631 sec\n",
      "Epoch [352 / 2000]: Train Loss=5.6113, Val Cor=0.4850, Time=0.1627 sec\n",
      "Epoch [353 / 2000]: Train Loss=5.8277, Val Cor=0.4835, Time=0.1646 sec\n",
      "Epoch [354 / 2000]: Train Loss=5.6127, Val Cor=0.4852, Time=0.1640 sec\n",
      "Epoch [355 / 2000]: Train Loss=5.6151, Val Cor=0.4925, Time=0.1646 sec\n",
      "Epoch [356 / 2000]: Train Loss=5.4582, Val Cor=0.4943, Time=0.1644 sec\n",
      "Epoch [357 / 2000]: Train Loss=5.7169, Val Cor=0.4638, Time=0.1636 sec\n",
      "Epoch [358 / 2000]: Train Loss=5.8292, Val Cor=0.4882, Time=0.1635 sec\n",
      "Epoch [359 / 2000]: Train Loss=5.5992, Val Cor=0.4935, Time=0.1643 sec\n",
      "Epoch [360 / 2000]: Train Loss=5.7360, Val Cor=0.4738, Time=0.1646 sec\n",
      "Epoch [361 / 2000]: Train Loss=5.8130, Val Cor=0.4828, Time=0.1645 sec\n",
      "Epoch [362 / 2000]: Train Loss=5.9833, Val Cor=0.4746, Time=0.1661 sec\n",
      "Epoch [363 / 2000]: Train Loss=5.9816, Val Cor=0.4964, Time=0.1666 sec\n",
      "Epoch [364 / 2000]: Train Loss=5.9697, Val Cor=0.4822, Time=0.1655 sec\n",
      "Epoch [365 / 2000]: Train Loss=5.7668, Val Cor=0.4980, Time=0.1650 sec\n",
      "Epoch [366 / 2000]: Train Loss=5.9328, Val Cor=0.4900, Time=0.1630 sec\n",
      "Epoch [367 / 2000]: Train Loss=5.5373, Val Cor=0.4905, Time=0.1647 sec\n",
      "Epoch [368 / 2000]: Train Loss=5.5993, Val Cor=0.4918, Time=0.1645 sec\n",
      "Epoch [369 / 2000]: Train Loss=5.7554, Val Cor=0.4969, Time=0.1644 sec\n",
      "Epoch [370 / 2000]: Train Loss=5.8624, Val Cor=0.4644, Time=0.1622 sec\n",
      "Epoch [371 / 2000]: Train Loss=5.6557, Val Cor=0.4657, Time=0.1621 sec\n",
      "Epoch [372 / 2000]: Train Loss=5.9920, Val Cor=0.4915, Time=0.1617 sec\n",
      "Epoch [373 / 2000]: Train Loss=5.8284, Val Cor=0.4852, Time=0.1624 sec\n",
      "Epoch [374 / 2000]: Train Loss=5.8544, Val Cor=0.4909, Time=0.1623 sec\n",
      "Epoch [375 / 2000]: Train Loss=5.7387, Val Cor=0.4912, Time=0.1628 sec\n",
      "Epoch [376 / 2000]: Train Loss=5.7640, Val Cor=0.4910, Time=0.2841 sec\n",
      "Epoch [377 / 2000]: Train Loss=5.7316, Val Cor=0.4878, Time=0.1661 sec\n",
      "Epoch [378 / 2000]: Train Loss=5.5316, Val Cor=0.4923, Time=0.1681 sec\n",
      "Epoch [379 / 2000]: Train Loss=5.5012, Val Cor=0.4912, Time=0.1670 sec\n",
      "Epoch [380 / 2000]: Train Loss=5.7996, Val Cor=0.4914, Time=0.1669 sec\n",
      "Epoch [381 / 2000]: Train Loss=5.8202, Val Cor=0.4914, Time=0.1670 sec\n",
      "Epoch [382 / 2000]: Train Loss=6.0050, Val Cor=0.4177, Time=0.1665 sec\n",
      "Epoch [383 / 2000]: Train Loss=6.1105, Val Cor=0.4919, Time=0.1648 sec\n",
      "Epoch [384 / 2000]: Train Loss=5.6149, Val Cor=0.4960, Time=0.1645 sec\n",
      "Epoch [385 / 2000]: Train Loss=6.0798, Val Cor=0.4882, Time=0.1648 sec\n",
      "Epoch [386 / 2000]: Train Loss=5.8412, Val Cor=0.4894, Time=0.1627 sec\n",
      "Epoch [387 / 2000]: Train Loss=5.7070, Val Cor=0.4935, Time=0.1628 sec\n",
      "Epoch [388 / 2000]: Train Loss=5.7133, Val Cor=0.4936, Time=0.1630 sec\n",
      "Epoch [389 / 2000]: Train Loss=5.5181, Val Cor=0.4920, Time=0.1641 sec\n",
      "Epoch [390 / 2000]: Train Loss=5.6257, Val Cor=0.4980, Time=0.1643 sec\n",
      "Epoch [391 / 2000]: Train Loss=5.8953, Val Cor=0.4965, Time=0.1646 sec\n",
      "Epoch [392 / 2000]: Train Loss=5.8468, Val Cor=0.5026, Time=0.1645 sec\n",
      "Epoch [393 / 2000]: Train Loss=5.6439, Val Cor=0.4940, Time=0.1634 sec\n",
      "Epoch [394 / 2000]: Train Loss=5.8135, Val Cor=0.5039, Time=0.1620 sec\n",
      "Epoch [395 / 2000]: Train Loss=5.7640, Val Cor=0.4875, Time=0.1619 sec\n",
      "Epoch [396 / 2000]: Train Loss=5.7724, Val Cor=0.4966, Time=0.1615 sec\n",
      "Epoch [397 / 2000]: Train Loss=5.5957, Val Cor=0.4928, Time=0.1628 sec\n",
      "Epoch [398 / 2000]: Train Loss=5.6305, Val Cor=0.4925, Time=0.1632 sec\n",
      "Epoch [399 / 2000]: Train Loss=6.2116, Val Cor=0.5024, Time=0.1648 sec\n",
      "Epoch [400 / 2000]: Train Loss=5.7608, Val Cor=0.4933, Time=0.1659 sec\n",
      "Epoch [401 / 2000]: Train Loss=5.5817, Val Cor=0.4953, Time=0.1684 sec\n",
      "Epoch [402 / 2000]: Train Loss=5.6614, Val Cor=0.4943, Time=0.1648 sec\n",
      "Epoch [403 / 2000]: Train Loss=5.9381, Val Cor=0.0363, Time=0.1647 sec\n",
      "Epoch [404 / 2000]: Train Loss=6.2400, Val Cor=0.4734, Time=0.1629 sec\n",
      "Epoch [405 / 2000]: Train Loss=5.9159, Val Cor=0.4886, Time=0.1617 sec\n",
      "Epoch [406 / 2000]: Train Loss=5.6641, Val Cor=0.4943, Time=0.1633 sec\n",
      "Epoch [407 / 2000]: Train Loss=5.6672, Val Cor=0.4939, Time=0.1626 sec\n",
      "Epoch [408 / 2000]: Train Loss=6.0584, Val Cor=0.2554, Time=0.1649 sec\n",
      "Epoch [409 / 2000]: Train Loss=6.0597, Val Cor=0.4921, Time=0.1648 sec\n",
      "Epoch [410 / 2000]: Train Loss=6.0654, Val Cor=0.4903, Time=0.1640 sec\n",
      "Epoch [411 / 2000]: Train Loss=5.8177, Val Cor=0.4869, Time=0.1648 sec\n",
      "Epoch [412 / 2000]: Train Loss=6.1776, Val Cor=0.4829, Time=0.1653 sec\n",
      "Epoch [413 / 2000]: Train Loss=6.0224, Val Cor=0.4613, Time=0.1643 sec\n",
      "Epoch [414 / 2000]: Train Loss=5.8510, Val Cor=0.4710, Time=0.1625 sec\n",
      "Epoch [415 / 2000]: Train Loss=5.8098, Val Cor=0.4924, Time=0.1616 sec\n",
      "Epoch [416 / 2000]: Train Loss=5.6757, Val Cor=0.4981, Time=0.1618 sec\n",
      "Epoch [417 / 2000]: Train Loss=5.6668, Val Cor=0.5008, Time=0.1626 sec\n",
      "Epoch [418 / 2000]: Train Loss=5.7608, Val Cor=0.4948, Time=0.1635 sec\n",
      "Epoch [419 / 2000]: Train Loss=5.6927, Val Cor=0.5002, Time=0.1627 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [420 / 2000]: Train Loss=5.4712, Val Cor=0.5072, Time=0.1647 sec\n",
      "Epoch [421 / 2000]: Train Loss=5.4877, Val Cor=0.4952, Time=0.1651 sec\n",
      "Epoch [422 / 2000]: Train Loss=5.6338, Val Cor=0.5001, Time=0.1655 sec\n",
      "Epoch [423 / 2000]: Train Loss=5.7738, Val Cor=0.4991, Time=0.1662 sec\n",
      "Epoch [424 / 2000]: Train Loss=5.7017, Val Cor=0.4482, Time=0.1647 sec\n",
      "Epoch [425 / 2000]: Train Loss=5.5899, Val Cor=0.4965, Time=0.1633 sec\n",
      "Epoch [426 / 2000]: Train Loss=5.6125, Val Cor=0.4379, Time=0.1629 sec\n",
      "Epoch [427 / 2000]: Train Loss=5.6694, Val Cor=0.4658, Time=0.1617 sec\n",
      "Epoch [428 / 2000]: Train Loss=5.8348, Val Cor=0.4986, Time=0.1618 sec\n",
      "Epoch [429 / 2000]: Train Loss=5.6210, Val Cor=0.4993, Time=0.1622 sec\n",
      "Epoch [430 / 2000]: Train Loss=5.6520, Val Cor=0.5001, Time=0.1627 sec\n",
      "Epoch [431 / 2000]: Train Loss=5.7840, Val Cor=0.4996, Time=0.1622 sec\n",
      "Epoch [432 / 2000]: Train Loss=5.5785, Val Cor=0.4830, Time=0.1639 sec\n",
      "Epoch [433 / 2000]: Train Loss=5.7777, Val Cor=0.4993, Time=0.1633 sec\n",
      "Epoch [434 / 2000]: Train Loss=5.8239, Val Cor=0.4745, Time=0.1633 sec\n",
      "Epoch [435 / 2000]: Train Loss=5.7878, Val Cor=0.5047, Time=0.1642 sec\n",
      "Epoch [436 / 2000]: Train Loss=6.4006, Val Cor=-0.3036, Time=0.1643 sec\n",
      "Epoch [437 / 2000]: Train Loss=5.9357, Val Cor=0.4955, Time=0.1637 sec\n",
      "Epoch [438 / 2000]: Train Loss=5.7964, Val Cor=0.5028, Time=0.1622 sec\n",
      "Epoch [439 / 2000]: Train Loss=5.6667, Val Cor=0.5026, Time=0.1615 sec\n",
      "Epoch [440 / 2000]: Train Loss=5.4290, Val Cor=0.5070, Time=0.1627 sec\n",
      "Epoch [441 / 2000]: Train Loss=5.6039, Val Cor=0.4970, Time=0.1625 sec\n",
      "Epoch [442 / 2000]: Train Loss=5.6462, Val Cor=0.5139, Time=0.1626 sec\n",
      "Epoch [443 / 2000]: Train Loss=5.7133, Val Cor=0.5010, Time=0.1619 sec\n",
      "Epoch [444 / 2000]: Train Loss=5.6069, Val Cor=0.4001, Time=0.1640 sec\n",
      "Epoch [445 / 2000]: Train Loss=5.3074, Val Cor=0.5081, Time=0.1632 sec\n",
      "Epoch [446 / 2000]: Train Loss=5.4926, Val Cor=0.5088, Time=0.1635 sec\n",
      "Epoch [447 / 2000]: Train Loss=5.4936, Val Cor=0.5080, Time=0.1642 sec\n",
      "Epoch [448 / 2000]: Train Loss=5.6899, Val Cor=0.5062, Time=0.1638 sec\n",
      "Epoch [449 / 2000]: Train Loss=5.5272, Val Cor=0.5141, Time=0.1621 sec\n",
      "Epoch [450 / 2000]: Train Loss=5.5020, Val Cor=0.3927, Time=0.1619 sec\n",
      "Epoch [451 / 2000]: Train Loss=5.5871, Val Cor=0.5093, Time=0.1612 sec\n",
      "Epoch [452 / 2000]: Train Loss=5.4789, Val Cor=0.5169, Time=0.1627 sec\n",
      "Epoch [453 / 2000]: Train Loss=5.6259, Val Cor=0.5066, Time=0.1621 sec\n",
      "Epoch [454 / 2000]: Train Loss=5.5401, Val Cor=0.4246, Time=0.1619 sec\n",
      "Epoch [455 / 2000]: Train Loss=5.7773, Val Cor=-0.4058, Time=0.1611 sec\n",
      "Epoch [456 / 2000]: Train Loss=5.8378, Val Cor=0.5190, Time=0.1645 sec\n",
      "Epoch [457 / 2000]: Train Loss=5.7917, Val Cor=0.5113, Time=0.1643 sec\n",
      "Epoch [458 / 2000]: Train Loss=5.6749, Val Cor=-0.2991, Time=0.1649 sec\n",
      "Epoch [459 / 2000]: Train Loss=5.6855, Val Cor=0.3733, Time=0.1653 sec\n",
      "Epoch [460 / 2000]: Train Loss=5.6106, Val Cor=0.5073, Time=0.1663 sec\n",
      "Epoch [461 / 2000]: Train Loss=5.6847, Val Cor=0.5001, Time=0.1643 sec\n",
      "Epoch [462 / 2000]: Train Loss=5.5366, Val Cor=0.5083, Time=0.1627 sec\n",
      "Epoch [463 / 2000]: Train Loss=5.5395, Val Cor=0.5170, Time=0.1619 sec\n",
      "Epoch [464 / 2000]: Train Loss=5.9285, Val Cor=0.5119, Time=0.1632 sec\n",
      "Epoch [465 / 2000]: Train Loss=5.6014, Val Cor=0.5111, Time=0.1630 sec\n",
      "Epoch [466 / 2000]: Train Loss=5.6686, Val Cor=0.5071, Time=0.1626 sec\n",
      "Epoch [467 / 2000]: Train Loss=5.6740, Val Cor=0.5082, Time=0.1624 sec\n",
      "Epoch [468 / 2000]: Train Loss=5.5185, Val Cor=0.3276, Time=0.1640 sec\n",
      "Epoch [469 / 2000]: Train Loss=5.8878, Val Cor=-0.3843, Time=0.1635 sec\n",
      "Epoch [470 / 2000]: Train Loss=6.0260, Val Cor=-0.3628, Time=0.1645 sec\n",
      "Epoch [471 / 2000]: Train Loss=5.8392, Val Cor=0.5042, Time=0.1638 sec\n",
      "Epoch [472 / 2000]: Train Loss=6.0678, Val Cor=0.5086, Time=0.1625 sec\n",
      "Epoch [473 / 2000]: Train Loss=5.6383, Val Cor=0.0914, Time=0.1619 sec\n",
      "Epoch [474 / 2000]: Train Loss=5.9085, Val Cor=0.5074, Time=0.1627 sec\n",
      "Epoch [475 / 2000]: Train Loss=5.7202, Val Cor=0.5058, Time=0.1629 sec\n",
      "Epoch [476 / 2000]: Train Loss=5.5230, Val Cor=0.4996, Time=0.1645 sec\n",
      "Epoch [477 / 2000]: Train Loss=5.4647, Val Cor=0.5123, Time=0.1620 sec\n",
      "Epoch [478 / 2000]: Train Loss=5.6348, Val Cor=0.5130, Time=0.1624 sec\n",
      "Epoch [479 / 2000]: Train Loss=5.7481, Val Cor=0.5108, Time=0.1636 sec\n",
      "Epoch [480 / 2000]: Train Loss=5.6625, Val Cor=0.5146, Time=0.1641 sec\n",
      "Epoch [481 / 2000]: Train Loss=5.5121, Val Cor=0.4817, Time=0.1634 sec\n",
      "Epoch [482 / 2000]: Train Loss=5.4715, Val Cor=0.5140, Time=0.1643 sec\n",
      "Epoch [483 / 2000]: Train Loss=5.5756, Val Cor=-0.4140, Time=0.1637 sec\n",
      "Epoch [484 / 2000]: Train Loss=5.8011, Val Cor=0.4978, Time=0.1638 sec\n",
      "Epoch [485 / 2000]: Train Loss=5.5912, Val Cor=0.5037, Time=0.1626 sec\n",
      "Epoch [486 / 2000]: Train Loss=5.4934, Val Cor=0.5118, Time=0.1626 sec\n",
      "Epoch [487 / 2000]: Train Loss=5.6765, Val Cor=0.5086, Time=0.1622 sec\n",
      "Epoch [488 / 2000]: Train Loss=5.5159, Val Cor=0.5007, Time=0.1624 sec\n",
      "Epoch [489 / 2000]: Train Loss=5.5247, Val Cor=0.5165, Time=0.1638 sec\n",
      "Epoch [490 / 2000]: Train Loss=5.4386, Val Cor=0.2196, Time=0.1647 sec\n",
      "Epoch [491 / 2000]: Train Loss=5.4125, Val Cor=0.5171, Time=0.1639 sec\n",
      "Epoch [492 / 2000]: Train Loss=5.2795, Val Cor=0.4771, Time=0.1651 sec\n",
      "Epoch [493 / 2000]: Train Loss=5.4033, Val Cor=0.5175, Time=0.1641 sec\n",
      "Epoch [494 / 2000]: Train Loss=5.5117, Val Cor=0.5203, Time=0.1624 sec\n",
      "Epoch [495 / 2000]: Train Loss=5.3146, Val Cor=0.4915, Time=0.1618 sec\n",
      "Epoch [496 / 2000]: Train Loss=5.3173, Val Cor=0.5169, Time=0.1616 sec\n",
      "Epoch [497 / 2000]: Train Loss=5.4862, Val Cor=0.5157, Time=0.1624 sec\n",
      "Epoch [498 / 2000]: Train Loss=5.3265, Val Cor=0.5192, Time=0.1625 sec\n",
      "Epoch [499 / 2000]: Train Loss=5.5247, Val Cor=0.5180, Time=0.1625 sec\n",
      "Epoch [500 / 2000]: Train Loss=5.9323, Val Cor=0.5079, Time=0.1640 sec\n",
      "Epoch [501 / 2000]: Train Loss=5.8259, Val Cor=0.5197, Time=0.1641 sec\n",
      "Epoch [502 / 2000]: Train Loss=5.9571, Val Cor=0.4994, Time=0.1644 sec\n",
      "Epoch [503 / 2000]: Train Loss=5.8042, Val Cor=0.5220, Time=0.1633 sec\n",
      "Epoch [504 / 2000]: Train Loss=5.7514, Val Cor=0.5191, Time=0.1636 sec\n",
      "Epoch [505 / 2000]: Train Loss=5.7467, Val Cor=0.5138, Time=0.1651 sec\n",
      "Epoch [506 / 2000]: Train Loss=5.7306, Val Cor=0.5154, Time=0.1645 sec\n",
      "Epoch [507 / 2000]: Train Loss=5.6014, Val Cor=0.4992, Time=0.1634 sec\n",
      "Epoch [508 / 2000]: Train Loss=5.4455, Val Cor=0.5183, Time=0.1638 sec\n",
      "Epoch [509 / 2000]: Train Loss=5.3341, Val Cor=0.5149, Time=0.1614 sec\n",
      "Epoch [510 / 2000]: Train Loss=5.5748, Val Cor=0.5173, Time=0.1623 sec\n",
      "Epoch [511 / 2000]: Train Loss=5.2787, Val Cor=0.5193, Time=0.1628 sec\n",
      "Epoch [512 / 2000]: Train Loss=5.7336, Val Cor=-0.3628, Time=0.1640 sec\n",
      "Epoch [513 / 2000]: Train Loss=5.6573, Val Cor=-0.3751, Time=0.1644 sec\n",
      "Epoch [514 / 2000]: Train Loss=5.6460, Val Cor=-0.1815, Time=0.1641 sec\n",
      "Epoch [515 / 2000]: Train Loss=5.5744, Val Cor=-0.0344, Time=0.1632 sec\n",
      "Epoch [516 / 2000]: Train Loss=5.5278, Val Cor=0.3841, Time=0.1645 sec\n",
      "Epoch [517 / 2000]: Train Loss=5.5170, Val Cor=-0.0750, Time=0.1637 sec\n",
      "Epoch [518 / 2000]: Train Loss=5.5004, Val Cor=-0.2284, Time=0.1622 sec\n",
      "Epoch [519 / 2000]: Train Loss=5.8379, Val Cor=0.4698, Time=0.1620 sec\n",
      "Epoch [520 / 2000]: Train Loss=5.5263, Val Cor=-0.0440, Time=0.1623 sec\n",
      "Epoch [521 / 2000]: Train Loss=6.3846, Val Cor=-0.3567, Time=0.1627 sec\n",
      "Epoch [522 / 2000]: Train Loss=5.7794, Val Cor=-0.3239, Time=0.1620 sec\n",
      "Epoch [523 / 2000]: Train Loss=5.7699, Val Cor=0.5177, Time=0.1630 sec\n",
      "Epoch [524 / 2000]: Train Loss=5.5797, Val Cor=0.5212, Time=0.1647 sec\n",
      "Epoch [525 / 2000]: Train Loss=5.5599, Val Cor=0.5194, Time=0.1644 sec\n",
      "Epoch [526 / 2000]: Train Loss=5.6979, Val Cor=0.5184, Time=0.1660 sec\n",
      "Epoch [527 / 2000]: Train Loss=5.3954, Val Cor=0.5216, Time=0.1643 sec\n",
      "Epoch [528 / 2000]: Train Loss=5.6184, Val Cor=0.5185, Time=0.1639 sec\n",
      "Epoch [529 / 2000]: Train Loss=5.6902, Val Cor=0.5160, Time=0.1621 sec\n",
      "Epoch [530 / 2000]: Train Loss=5.5951, Val Cor=0.5112, Time=0.1615 sec\n",
      "Epoch [531 / 2000]: Train Loss=5.5630, Val Cor=0.5239, Time=0.1620 sec\n",
      "Epoch [532 / 2000]: Train Loss=5.2643, Val Cor=0.4807, Time=0.1622 sec\n",
      "Epoch [533 / 2000]: Train Loss=5.6677, Val Cor=0.5143, Time=0.1624 sec\n",
      "Epoch [534 / 2000]: Train Loss=5.3370, Val Cor=0.5160, Time=0.1622 sec\n",
      "Epoch [535 / 2000]: Train Loss=5.5149, Val Cor=0.4987, Time=0.1625 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [536 / 2000]: Train Loss=5.5660, Val Cor=0.4952, Time=0.1638 sec\n",
      "Epoch [537 / 2000]: Train Loss=5.6035, Val Cor=0.4865, Time=0.1646 sec\n",
      "Epoch [538 / 2000]: Train Loss=5.4529, Val Cor=0.5209, Time=0.1643 sec\n",
      "Epoch [539 / 2000]: Train Loss=5.5606, Val Cor=0.5227, Time=0.1635 sec\n",
      "Epoch [540 / 2000]: Train Loss=5.4617, Val Cor=0.1786, Time=0.1648 sec\n",
      "Epoch [541 / 2000]: Train Loss=5.6502, Val Cor=0.5080, Time=0.1640 sec\n",
      "Epoch [542 / 2000]: Train Loss=5.4579, Val Cor=0.5232, Time=0.1628 sec\n",
      "Epoch [543 / 2000]: Train Loss=5.3721, Val Cor=0.5303, Time=0.1618 sec\n",
      "Epoch [544 / 2000]: Train Loss=5.8544, Val Cor=-0.3626, Time=0.1625 sec\n",
      "Epoch [545 / 2000]: Train Loss=5.9551, Val Cor=0.0697, Time=0.1629 sec\n",
      "Epoch [546 / 2000]: Train Loss=5.5350, Val Cor=0.5176, Time=0.1623 sec\n",
      "Epoch [547 / 2000]: Train Loss=5.4737, Val Cor=0.5202, Time=0.1625 sec\n",
      "Epoch [548 / 2000]: Train Loss=5.3378, Val Cor=0.5229, Time=0.1625 sec\n",
      "Epoch [549 / 2000]: Train Loss=5.4778, Val Cor=0.4993, Time=0.1625 sec\n",
      "Epoch [550 / 2000]: Train Loss=5.6306, Val Cor=0.4622, Time=0.1644 sec\n",
      "Epoch [551 / 2000]: Train Loss=5.7014, Val Cor=0.4595, Time=0.1644 sec\n",
      "Epoch [552 / 2000]: Train Loss=5.7834, Val Cor=0.5121, Time=0.1643 sec\n",
      "Epoch [553 / 2000]: Train Loss=5.4973, Val Cor=0.5160, Time=0.1646 sec\n",
      "Epoch [554 / 2000]: Train Loss=5.4397, Val Cor=0.5175, Time=0.1652 sec\n",
      "Epoch [555 / 2000]: Train Loss=5.4563, Val Cor=0.5193, Time=0.1641 sec\n",
      "Epoch [556 / 2000]: Train Loss=5.4834, Val Cor=-0.4912, Time=0.1628 sec\n",
      "Epoch [557 / 2000]: Train Loss=6.1167, Val Cor=-0.4940, Time=0.1617 sec\n",
      "Epoch [558 / 2000]: Train Loss=5.6778, Val Cor=0.5263, Time=0.1621 sec\n",
      "Epoch [559 / 2000]: Train Loss=5.9060, Val Cor=0.4635, Time=0.1621 sec\n",
      "Epoch [560 / 2000]: Train Loss=5.9677, Val Cor=0.5105, Time=0.1630 sec\n",
      "Epoch [561 / 2000]: Train Loss=5.5881, Val Cor=0.5198, Time=0.1621 sec\n",
      "Epoch [562 / 2000]: Train Loss=5.8725, Val Cor=0.4784, Time=0.1629 sec\n",
      "Epoch [563 / 2000]: Train Loss=5.7886, Val Cor=0.5309, Time=0.1639 sec\n",
      "Epoch [564 / 2000]: Train Loss=5.3921, Val Cor=0.5213, Time=0.1697 sec\n",
      "Epoch [565 / 2000]: Train Loss=5.2839, Val Cor=0.5307, Time=0.1659 sec\n",
      "Epoch [566 / 2000]: Train Loss=5.4738, Val Cor=0.5125, Time=0.1657 sec\n",
      "Epoch [567 / 2000]: Train Loss=5.4413, Val Cor=0.5248, Time=0.1648 sec\n",
      "Epoch [568 / 2000]: Train Loss=5.4355, Val Cor=0.5354, Time=0.1645 sec\n",
      "Epoch [569 / 2000]: Train Loss=5.5187, Val Cor=0.5160, Time=0.1639 sec\n",
      "Epoch [570 / 2000]: Train Loss=5.6210, Val Cor=0.5345, Time=0.1639 sec\n",
      "Epoch [571 / 2000]: Train Loss=5.3569, Val Cor=0.5197, Time=0.1620 sec\n",
      "Epoch [572 / 2000]: Train Loss=5.6131, Val Cor=0.5167, Time=0.1615 sec\n",
      "Epoch [573 / 2000]: Train Loss=5.3853, Val Cor=0.5281, Time=0.1618 sec\n",
      "Epoch [574 / 2000]: Train Loss=5.5701, Val Cor=0.5378, Time=0.1624 sec\n",
      "Epoch [575 / 2000]: Train Loss=5.4693, Val Cor=0.5236, Time=0.1619 sec\n",
      "Epoch [576 / 2000]: Train Loss=5.4500, Val Cor=0.5280, Time=0.1620 sec\n",
      "Epoch [577 / 2000]: Train Loss=5.7960, Val Cor=-0.5231, Time=0.1625 sec\n",
      "Epoch [578 / 2000]: Train Loss=6.2307, Val Cor=0.4897, Time=0.1639 sec\n",
      "Epoch [579 / 2000]: Train Loss=5.7423, Val Cor=-0.4921, Time=0.1637 sec\n",
      "Epoch [580 / 2000]: Train Loss=5.5428, Val Cor=0.5266, Time=0.1635 sec\n",
      "Epoch [581 / 2000]: Train Loss=5.4215, Val Cor=0.5288, Time=0.1641 sec\n",
      "Epoch [582 / 2000]: Train Loss=5.3963, Val Cor=0.4957, Time=0.1633 sec\n",
      "Epoch [583 / 2000]: Train Loss=6.0204, Val Cor=0.5472, Time=0.1642 sec\n",
      "Epoch [584 / 2000]: Train Loss=5.7478, Val Cor=0.5040, Time=0.1645 sec\n",
      "Epoch [585 / 2000]: Train Loss=5.6578, Val Cor=0.5288, Time=0.1638 sec\n",
      "Epoch [586 / 2000]: Train Loss=5.6994, Val Cor=0.5359, Time=0.1617 sec\n",
      "Epoch [587 / 2000]: Train Loss=5.3242, Val Cor=0.5185, Time=0.1613 sec\n",
      "Epoch [588 / 2000]: Train Loss=5.4297, Val Cor=0.5274, Time=0.1622 sec\n",
      "Epoch [589 / 2000]: Train Loss=5.1789, Val Cor=0.1709, Time=0.1682 sec\n",
      "Epoch [590 / 2000]: Train Loss=5.5226, Val Cor=0.5181, Time=0.1622 sec\n",
      "Epoch [591 / 2000]: Train Loss=5.3907, Val Cor=0.2436, Time=0.1617 sec\n",
      "Epoch [592 / 2000]: Train Loss=5.4407, Val Cor=0.5255, Time=0.1635 sec\n",
      "Epoch [593 / 2000]: Train Loss=5.2759, Val Cor=0.5268, Time=0.1626 sec\n",
      "Epoch [594 / 2000]: Train Loss=5.3248, Val Cor=0.5285, Time=0.1636 sec\n",
      "Epoch [595 / 2000]: Train Loss=5.1476, Val Cor=0.5344, Time=0.1632 sec\n",
      "Epoch [596 / 2000]: Train Loss=5.5566, Val Cor=0.5434, Time=0.1621 sec\n",
      "Epoch [597 / 2000]: Train Loss=5.6163, Val Cor=0.3443, Time=0.1617 sec\n",
      "Epoch [598 / 2000]: Train Loss=5.6247, Val Cor=0.5333, Time=0.1623 sec\n",
      "Epoch [599 / 2000]: Train Loss=5.2878, Val Cor=0.5334, Time=0.1624 sec\n",
      "Epoch [600 / 2000]: Train Loss=5.2799, Val Cor=0.4540, Time=0.1617 sec\n",
      "Epoch [601 / 2000]: Train Loss=5.2341, Val Cor=0.5151, Time=0.1615 sec\n",
      "Epoch [602 / 2000]: Train Loss=5.5640, Val Cor=0.0625, Time=0.1618 sec\n",
      "Epoch [603 / 2000]: Train Loss=5.7346, Val Cor=-0.4995, Time=0.1612 sec\n",
      "Epoch [604 / 2000]: Train Loss=6.0134, Val Cor=-0.3407, Time=0.1635 sec\n",
      "Epoch [605 / 2000]: Train Loss=5.9038, Val Cor=-0.3675, Time=0.1629 sec\n",
      "Epoch [606 / 2000]: Train Loss=5.5370, Val Cor=0.5240, Time=0.1629 sec\n",
      "Epoch [607 / 2000]: Train Loss=5.4931, Val Cor=-0.3344, Time=0.1635 sec\n",
      "Epoch [608 / 2000]: Train Loss=5.7998, Val Cor=-0.3267, Time=0.1636 sec\n",
      "Epoch [609 / 2000]: Train Loss=5.7929, Val Cor=0.5060, Time=0.1634 sec\n",
      "Epoch [610 / 2000]: Train Loss=5.4377, Val Cor=0.3668, Time=0.1617 sec\n",
      "Epoch [611 / 2000]: Train Loss=5.6861, Val Cor=0.5393, Time=0.1617 sec\n",
      "Epoch [612 / 2000]: Train Loss=5.3999, Val Cor=0.5563, Time=0.1624 sec\n",
      "Epoch [613 / 2000]: Train Loss=5.2886, Val Cor=0.5388, Time=0.1622 sec\n",
      "Epoch [614 / 2000]: Train Loss=5.4296, Val Cor=0.5204, Time=0.1622 sec\n",
      "Epoch [615 / 2000]: Train Loss=5.3448, Val Cor=0.4753, Time=0.1623 sec\n",
      "Epoch [616 / 2000]: Train Loss=5.4369, Val Cor=0.5225, Time=0.1635 sec\n",
      "Epoch [617 / 2000]: Train Loss=5.4857, Val Cor=0.5149, Time=0.1631 sec\n",
      "Epoch [618 / 2000]: Train Loss=5.3612, Val Cor=0.2049, Time=0.1633 sec\n",
      "Epoch [619 / 2000]: Train Loss=5.5357, Val Cor=0.5176, Time=0.1646 sec\n",
      "Epoch [620 / 2000]: Train Loss=5.5126, Val Cor=0.5349, Time=0.1653 sec\n",
      "Epoch [621 / 2000]: Train Loss=5.2246, Val Cor=0.5340, Time=0.1618 sec\n",
      "Epoch [622 / 2000]: Train Loss=5.3455, Val Cor=0.5536, Time=0.1612 sec\n",
      "Epoch [623 / 2000]: Train Loss=5.4236, Val Cor=0.5303, Time=0.1619 sec\n",
      "Epoch [624 / 2000]: Train Loss=5.2702, Val Cor=0.5272, Time=0.1619 sec\n",
      "Epoch [625 / 2000]: Train Loss=5.1808, Val Cor=0.5248, Time=0.1619 sec\n",
      "Epoch [626 / 2000]: Train Loss=5.3055, Val Cor=0.5211, Time=0.1636 sec\n",
      "Epoch [627 / 2000]: Train Loss=5.5286, Val Cor=0.4929, Time=0.1627 sec\n",
      "Epoch [628 / 2000]: Train Loss=5.4209, Val Cor=0.5295, Time=0.1624 sec\n",
      "Epoch [629 / 2000]: Train Loss=5.2231, Val Cor=0.5534, Time=0.1635 sec\n",
      "Epoch [630 / 2000]: Train Loss=5.2746, Val Cor=0.5273, Time=0.1635 sec\n",
      "Epoch [631 / 2000]: Train Loss=5.1429, Val Cor=0.5362, Time=0.1611 sec\n",
      "Epoch [632 / 2000]: Train Loss=5.1944, Val Cor=0.4301, Time=0.1616 sec\n",
      "Epoch [633 / 2000]: Train Loss=5.4819, Val Cor=0.5324, Time=0.1619 sec\n",
      "Epoch [634 / 2000]: Train Loss=5.4483, Val Cor=-0.3334, Time=0.1624 sec\n",
      "Epoch [635 / 2000]: Train Loss=5.5135, Val Cor=-0.1603, Time=0.1621 sec\n",
      "Epoch [636 / 2000]: Train Loss=5.3936, Val Cor=0.4864, Time=0.1619 sec\n",
      "Epoch [637 / 2000]: Train Loss=5.1840, Val Cor=0.5161, Time=0.1633 sec\n",
      "Epoch [638 / 2000]: Train Loss=5.4823, Val Cor=0.5022, Time=0.1632 sec\n",
      "Epoch [639 / 2000]: Train Loss=5.4812, Val Cor=-0.3489, Time=0.1637 sec\n",
      "Epoch [640 / 2000]: Train Loss=6.0338, Val Cor=0.5227, Time=0.1641 sec\n",
      "Epoch [641 / 2000]: Train Loss=5.3822, Val Cor=0.5276, Time=0.1640 sec\n",
      "Epoch [642 / 2000]: Train Loss=5.5932, Val Cor=-0.4408, Time=0.1607 sec\n",
      "Epoch [643 / 2000]: Train Loss=5.5752, Val Cor=0.1232, Time=0.1604 sec\n",
      "Epoch [644 / 2000]: Train Loss=5.2306, Val Cor=0.4747, Time=0.1609 sec\n",
      "Epoch [645 / 2000]: Train Loss=5.6917, Val Cor=-0.2973, Time=0.1607 sec\n",
      "Epoch [646 / 2000]: Train Loss=5.5650, Val Cor=0.4252, Time=0.1612 sec\n",
      "Epoch [647 / 2000]: Train Loss=5.3929, Val Cor=0.5095, Time=0.1625 sec\n",
      "Epoch [648 / 2000]: Train Loss=5.4716, Val Cor=0.5414, Time=0.1622 sec\n",
      "Epoch [649 / 2000]: Train Loss=5.4709, Val Cor=0.5338, Time=0.1628 sec\n",
      "Epoch [650 / 2000]: Train Loss=5.2516, Val Cor=-0.4304, Time=0.1623 sec\n",
      "Epoch [651 / 2000]: Train Loss=5.4876, Val Cor=0.5293, Time=0.1608 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [652 / 2000]: Train Loss=5.2209, Val Cor=0.5461, Time=0.1620 sec\n",
      "Epoch [653 / 2000]: Train Loss=5.5374, Val Cor=0.5334, Time=0.1613 sec\n",
      "Epoch [654 / 2000]: Train Loss=5.2880, Val Cor=0.5250, Time=0.1614 sec\n",
      "Epoch [655 / 2000]: Train Loss=5.3391, Val Cor=0.5306, Time=0.1613 sec\n",
      "Epoch [656 / 2000]: Train Loss=5.3649, Val Cor=0.5325, Time=0.1627 sec\n",
      "Epoch [657 / 2000]: Train Loss=5.3637, Val Cor=0.4823, Time=0.1631 sec\n",
      "Epoch [658 / 2000]: Train Loss=5.3759, Val Cor=0.4964, Time=0.1631 sec\n",
      "Epoch [659 / 2000]: Train Loss=5.4372, Val Cor=0.4996, Time=0.1624 sec\n",
      "Epoch [660 / 2000]: Train Loss=6.3661, Val Cor=-0.2619, Time=0.1635 sec\n",
      "Epoch [661 / 2000]: Train Loss=5.9639, Val Cor=-0.3075, Time=0.1631 sec\n",
      "Epoch [662 / 2000]: Train Loss=5.8943, Val Cor=-0.2714, Time=0.1626 sec\n",
      "Epoch [663 / 2000]: Train Loss=5.6855, Val Cor=-0.1498, Time=0.1616 sec\n",
      "Epoch [664 / 2000]: Train Loss=5.2563, Val Cor=0.5351, Time=0.1613 sec\n",
      "Epoch [665 / 2000]: Train Loss=5.3772, Val Cor=0.5169, Time=0.1610 sec\n",
      "Epoch [666 / 2000]: Train Loss=5.4903, Val Cor=0.5365, Time=0.1620 sec\n",
      "Epoch [667 / 2000]: Train Loss=5.4113, Val Cor=0.5410, Time=0.1619 sec\n",
      "Epoch [668 / 2000]: Train Loss=5.4309, Val Cor=0.5104, Time=0.1657 sec\n",
      "Epoch [669 / 2000]: Train Loss=5.4910, Val Cor=-0.2370, Time=0.1615 sec\n",
      "Epoch [670 / 2000]: Train Loss=5.4961, Val Cor=0.3680, Time=0.1634 sec\n",
      "Epoch [671 / 2000]: Train Loss=5.3716, Val Cor=0.4657, Time=0.1646 sec\n",
      "Epoch [672 / 2000]: Train Loss=5.5248, Val Cor=0.0913, Time=0.1655 sec\n",
      "Epoch [673 / 2000]: Train Loss=5.5921, Val Cor=0.4538, Time=0.1624 sec\n",
      "Epoch [674 / 2000]: Train Loss=5.4360, Val Cor=-0.4109, Time=0.1635 sec\n",
      "Epoch [675 / 2000]: Train Loss=5.6115, Val Cor=0.3049, Time=0.1625 sec\n",
      "Epoch [676 / 2000]: Train Loss=5.3850, Val Cor=0.5268, Time=0.1604 sec\n",
      "Epoch [677 / 2000]: Train Loss=5.0912, Val Cor=0.4054, Time=0.1609 sec\n",
      "Epoch [678 / 2000]: Train Loss=5.3490, Val Cor=-0.5206, Time=0.1610 sec\n",
      "Epoch [679 / 2000]: Train Loss=5.2871, Val Cor=-0.2368, Time=0.1616 sec\n",
      "Epoch [680 / 2000]: Train Loss=5.4055, Val Cor=0.5355, Time=0.1614 sec\n",
      "Epoch [681 / 2000]: Train Loss=5.2042, Val Cor=0.5281, Time=0.1628 sec\n",
      "Epoch [682 / 2000]: Train Loss=5.2147, Val Cor=0.5162, Time=0.1629 sec\n",
      "Epoch [683 / 2000]: Train Loss=5.3257, Val Cor=0.4628, Time=0.1624 sec\n",
      "Epoch [684 / 2000]: Train Loss=5.6531, Val Cor=0.5407, Time=0.1633 sec\n",
      "Epoch [685 / 2000]: Train Loss=5.4334, Val Cor=-0.0940, Time=0.1628 sec\n",
      "Epoch [686 / 2000]: Train Loss=5.3391, Val Cor=-0.0976, Time=0.1620 sec\n",
      "Epoch [687 / 2000]: Train Loss=5.2620, Val Cor=0.5315, Time=0.1617 sec\n",
      "Epoch [688 / 2000]: Train Loss=5.2916, Val Cor=-0.0670, Time=0.1628 sec\n",
      "Epoch [689 / 2000]: Train Loss=5.3942, Val Cor=0.3911, Time=0.1617 sec\n",
      "Epoch [690 / 2000]: Train Loss=5.2857, Val Cor=0.5394, Time=0.1621 sec\n",
      "Epoch [691 / 2000]: Train Loss=5.1742, Val Cor=0.5169, Time=0.1623 sec\n",
      "Epoch [692 / 2000]: Train Loss=5.6457, Val Cor=-0.1570, Time=0.1631 sec\n",
      "Epoch [693 / 2000]: Train Loss=5.2201, Val Cor=0.5148, Time=0.1628 sec\n",
      "Epoch [694 / 2000]: Train Loss=5.7536, Val Cor=0.2023, Time=0.1632 sec\n",
      "Epoch [695 / 2000]: Train Loss=5.7980, Val Cor=-0.0031, Time=0.1618 sec\n",
      "Epoch [696 / 2000]: Train Loss=5.4121, Val Cor=0.5436, Time=0.1613 sec\n",
      "Epoch [697 / 2000]: Train Loss=5.4057, Val Cor=0.4188, Time=0.1615 sec\n",
      "Epoch [698 / 2000]: Train Loss=5.3102, Val Cor=0.4999, Time=0.1616 sec\n",
      "Epoch [699 / 2000]: Train Loss=5.1684, Val Cor=0.5512, Time=0.1613 sec\n",
      "Epoch [700 / 2000]: Train Loss=5.4050, Val Cor=0.2903, Time=0.1625 sec\n",
      "Epoch [701 / 2000]: Train Loss=5.3290, Val Cor=-0.1429, Time=0.1625 sec\n",
      "Epoch [702 / 2000]: Train Loss=5.3868, Val Cor=0.3975, Time=0.1631 sec\n",
      "Epoch [703 / 2000]: Train Loss=5.5582, Val Cor=0.5325, Time=0.1622 sec\n",
      "Epoch [704 / 2000]: Train Loss=5.4258, Val Cor=0.5102, Time=0.1615 sec\n",
      "Epoch [705 / 2000]: Train Loss=5.3164, Val Cor=-0.2129, Time=0.1609 sec\n",
      "Epoch [706 / 2000]: Train Loss=5.5441, Val Cor=-0.1313, Time=0.1606 sec\n",
      "Epoch [707 / 2000]: Train Loss=5.5235, Val Cor=-0.2995, Time=0.1605 sec\n",
      "Epoch [708 / 2000]: Train Loss=5.3318, Val Cor=0.1029, Time=0.1612 sec\n",
      "Epoch [709 / 2000]: Train Loss=5.5113, Val Cor=0.5382, Time=0.1612 sec\n",
      "Epoch [710 / 2000]: Train Loss=5.4503, Val Cor=0.5386, Time=0.1620 sec\n",
      "Epoch [711 / 2000]: Train Loss=5.2765, Val Cor=0.4663, Time=0.1618 sec\n",
      "Epoch [712 / 2000]: Train Loss=5.2060, Val Cor=0.5384, Time=0.1618 sec\n",
      "Epoch [713 / 2000]: Train Loss=5.3457, Val Cor=0.5363, Time=0.1617 sec\n",
      "Epoch [714 / 2000]: Train Loss=5.2750, Val Cor=0.5319, Time=0.1633 sec\n",
      "Epoch [715 / 2000]: Train Loss=5.3295, Val Cor=-0.2182, Time=0.1630 sec\n",
      "Epoch [716 / 2000]: Train Loss=5.1944, Val Cor=0.5341, Time=0.1633 sec\n",
      "Epoch [717 / 2000]: Train Loss=5.3790, Val Cor=0.5375, Time=0.1621 sec\n",
      "Epoch [718 / 2000]: Train Loss=5.3094, Val Cor=0.5263, Time=0.1637 sec\n",
      "Epoch [719 / 2000]: Train Loss=5.2192, Val Cor=0.5313, Time=0.1630 sec\n",
      "Epoch [720 / 2000]: Train Loss=5.1978, Val Cor=0.4926, Time=0.1624 sec\n",
      "Epoch [721 / 2000]: Train Loss=5.3997, Val Cor=0.2472, Time=0.1616 sec\n",
      "Epoch [722 / 2000]: Train Loss=5.1325, Val Cor=0.4904, Time=0.1623 sec\n",
      "Epoch [723 / 2000]: Train Loss=5.1695, Val Cor=-0.2766, Time=0.1625 sec\n",
      "Epoch [724 / 2000]: Train Loss=5.2980, Val Cor=0.5292, Time=0.1637 sec\n",
      "Epoch [725 / 2000]: Train Loss=5.3862, Val Cor=-0.1912, Time=0.1627 sec\n",
      "Epoch [726 / 2000]: Train Loss=5.1414, Val Cor=-0.0520, Time=0.1627 sec\n",
      "Epoch [727 / 2000]: Train Loss=5.7905, Val Cor=0.4743, Time=0.1625 sec\n",
      "Epoch [728 / 2000]: Train Loss=5.6250, Val Cor=0.5149, Time=0.1628 sec\n",
      "Epoch [729 / 2000]: Train Loss=5.2346, Val Cor=0.4699, Time=0.1625 sec\n",
      "Epoch [730 / 2000]: Train Loss=5.2946, Val Cor=-0.0209, Time=0.1616 sec\n",
      "Epoch [731 / 2000]: Train Loss=5.3459, Val Cor=0.5331, Time=0.1607 sec\n",
      "Epoch [732 / 2000]: Train Loss=5.2295, Val Cor=0.5400, Time=0.1606 sec\n",
      "Epoch [733 / 2000]: Train Loss=5.7047, Val Cor=-0.1080, Time=0.1612 sec\n",
      "Epoch [734 / 2000]: Train Loss=5.8882, Val Cor=0.3491, Time=0.1615 sec\n",
      "Epoch [735 / 2000]: Train Loss=5.6006, Val Cor=-0.3017, Time=0.1614 sec\n",
      "Epoch [736 / 2000]: Train Loss=5.2887, Val Cor=0.5387, Time=0.1629 sec\n",
      "Epoch [737 / 2000]: Train Loss=5.0947, Val Cor=0.4983, Time=0.1633 sec\n",
      "Epoch [738 / 2000]: Train Loss=5.0657, Val Cor=0.0832, Time=0.1631 sec\n",
      "Epoch [739 / 2000]: Train Loss=5.4494, Val Cor=0.2966, Time=0.1634 sec\n",
      "Epoch [740 / 2000]: Train Loss=5.3478, Val Cor=0.5000, Time=0.1628 sec\n",
      "Epoch [741 / 2000]: Train Loss=5.3839, Val Cor=0.4672, Time=0.1619 sec\n",
      "Epoch [742 / 2000]: Train Loss=5.8978, Val Cor=0.4724, Time=0.1620 sec\n",
      "Epoch [743 / 2000]: Train Loss=5.2495, Val Cor=-0.2726, Time=0.1617 sec\n",
      "Epoch [744 / 2000]: Train Loss=5.6521, Val Cor=0.4003, Time=0.1622 sec\n",
      "Epoch [745 / 2000]: Train Loss=5.3711, Val Cor=0.5283, Time=0.1622 sec\n",
      "Epoch [746 / 2000]: Train Loss=5.4093, Val Cor=0.5277, Time=0.1625 sec\n",
      "Epoch [747 / 2000]: Train Loss=5.3883, Val Cor=0.5580, Time=0.1636 sec\n",
      "Epoch [748 / 2000]: Train Loss=5.2395, Val Cor=0.5357, Time=0.1633 sec\n",
      "Epoch [749 / 2000]: Train Loss=5.4259, Val Cor=0.5347, Time=0.1642 sec\n",
      "Epoch [750 / 2000]: Train Loss=5.1453, Val Cor=0.5355, Time=0.1648 sec\n",
      "Epoch [751 / 2000]: Train Loss=5.1325, Val Cor=0.3565, Time=0.1634 sec\n",
      "Epoch [752 / 2000]: Train Loss=5.1453, Val Cor=-0.0843, Time=0.1632 sec\n",
      "Epoch [753 / 2000]: Train Loss=5.1055, Val Cor=-0.1600, Time=0.1617 sec\n",
      "Epoch [754 / 2000]: Train Loss=5.1154, Val Cor=0.5494, Time=0.1615 sec\n",
      "Epoch [755 / 2000]: Train Loss=4.9893, Val Cor=0.5379, Time=0.1606 sec\n",
      "Epoch [756 / 2000]: Train Loss=5.0998, Val Cor=0.5499, Time=0.1615 sec\n",
      "Epoch [757 / 2000]: Train Loss=5.0779, Val Cor=0.5399, Time=0.1617 sec\n",
      "Epoch [758 / 2000]: Train Loss=5.0799, Val Cor=0.5568, Time=0.1618 sec\n",
      "Epoch [759 / 2000]: Train Loss=4.9388, Val Cor=0.5269, Time=0.1630 sec\n",
      "Epoch [760 / 2000]: Train Loss=5.2841, Val Cor=0.4536, Time=0.1634 sec\n",
      "Epoch [761 / 2000]: Train Loss=5.6938, Val Cor=0.4736, Time=0.1632 sec\n",
      "Epoch [762 / 2000]: Train Loss=5.3334, Val Cor=-0.2389, Time=0.1640 sec\n",
      "Epoch [763 / 2000]: Train Loss=5.2831, Val Cor=-0.3792, Time=0.1619 sec\n",
      "Epoch [764 / 2000]: Train Loss=5.0905, Val Cor=0.5028, Time=0.1620 sec\n",
      "Epoch [765 / 2000]: Train Loss=5.0497, Val Cor=-0.4392, Time=0.1620 sec\n",
      "Epoch [766 / 2000]: Train Loss=5.4306, Val Cor=0.5269, Time=0.1636 sec\n",
      "Epoch [767 / 2000]: Train Loss=5.3011, Val Cor=0.5416, Time=0.1624 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [768 / 2000]: Train Loss=5.3186, Val Cor=0.5360, Time=0.1619 sec\n",
      "Epoch [769 / 2000]: Train Loss=5.2175, Val Cor=0.5028, Time=0.1628 sec\n",
      "Epoch [770 / 2000]: Train Loss=5.0903, Val Cor=0.5227, Time=0.1636 sec\n",
      "Epoch [771 / 2000]: Train Loss=5.1435, Val Cor=-0.0946, Time=0.1632 sec\n",
      "Epoch [772 / 2000]: Train Loss=5.1105, Val Cor=0.5331, Time=0.1645 sec\n",
      "Epoch [773 / 2000]: Train Loss=5.9414, Val Cor=0.5312, Time=0.1636 sec\n",
      "Epoch [774 / 2000]: Train Loss=5.3199, Val Cor=0.2252, Time=0.1629 sec\n",
      "Epoch [775 / 2000]: Train Loss=5.4533, Val Cor=0.4733, Time=0.1615 sec\n",
      "Epoch [776 / 2000]: Train Loss=5.3951, Val Cor=0.5097, Time=0.1612 sec\n",
      "Epoch [777 / 2000]: Train Loss=5.3017, Val Cor=0.5073, Time=0.1613 sec\n",
      "Epoch [778 / 2000]: Train Loss=5.0330, Val Cor=0.5218, Time=0.1614 sec\n",
      "Epoch [779 / 2000]: Train Loss=5.0545, Val Cor=0.5520, Time=0.1627 sec\n",
      "Epoch [780 / 2000]: Train Loss=5.1838, Val Cor=0.5367, Time=0.1630 sec\n",
      "Epoch [781 / 2000]: Train Loss=5.3480, Val Cor=0.5415, Time=0.1629 sec\n",
      "Epoch [782 / 2000]: Train Loss=5.2209, Val Cor=0.4752, Time=0.1634 sec\n",
      "Epoch [783 / 2000]: Train Loss=5.1523, Val Cor=0.4501, Time=0.1636 sec\n",
      "Epoch [784 / 2000]: Train Loss=5.4495, Val Cor=-0.1379, Time=0.1629 sec\n",
      "Epoch [785 / 2000]: Train Loss=5.2441, Val Cor=0.4863, Time=0.1627 sec\n",
      "Epoch [786 / 2000]: Train Loss=5.3025, Val Cor=0.1615, Time=0.1622 sec\n",
      "Epoch [787 / 2000]: Train Loss=5.2169, Val Cor=0.5269, Time=0.1630 sec\n",
      "Epoch [788 / 2000]: Train Loss=5.0411, Val Cor=0.4708, Time=0.1612 sec\n",
      "Epoch [789 / 2000]: Train Loss=5.1091, Val Cor=0.4769, Time=0.1621 sec\n",
      "Epoch [790 / 2000]: Train Loss=5.1690, Val Cor=0.0688, Time=0.1621 sec\n",
      "Epoch [791 / 2000]: Train Loss=5.0681, Val Cor=0.4061, Time=0.1629 sec\n",
      "Epoch [792 / 2000]: Train Loss=4.9005, Val Cor=0.0165, Time=0.1635 sec\n",
      "Epoch [793 / 2000]: Train Loss=5.3162, Val Cor=0.5393, Time=0.1626 sec\n",
      "Epoch [794 / 2000]: Train Loss=5.0823, Val Cor=0.5404, Time=0.1631 sec\n",
      "Epoch [795 / 2000]: Train Loss=5.2921, Val Cor=0.5046, Time=0.1629 sec\n",
      "Epoch [796 / 2000]: Train Loss=5.3051, Val Cor=0.5355, Time=0.1630 sec\n",
      "Epoch [797 / 2000]: Train Loss=4.9555, Val Cor=0.4900, Time=0.1609 sec\n",
      "Epoch [798 / 2000]: Train Loss=5.2021, Val Cor=0.3120, Time=0.1603 sec\n",
      "Epoch [799 / 2000]: Train Loss=4.9757, Val Cor=0.5299, Time=0.1607 sec\n",
      "Epoch [800 / 2000]: Train Loss=6.1087, Val Cor=-0.2429, Time=0.1615 sec\n",
      "Epoch [801 / 2000]: Train Loss=6.5393, Val Cor=0.0762, Time=0.1610 sec\n",
      "Epoch [802 / 2000]: Train Loss=5.7307, Val Cor=-0.0397, Time=0.1616 sec\n",
      "Epoch [803 / 2000]: Train Loss=5.3808, Val Cor=0.2192, Time=0.1609 sec\n",
      "Epoch [804 / 2000]: Train Loss=5.4205, Val Cor=0.4697, Time=0.1632 sec\n",
      "Epoch [805 / 2000]: Train Loss=5.1146, Val Cor=0.2760, Time=0.1631 sec\n",
      "Epoch [806 / 2000]: Train Loss=5.2459, Val Cor=0.3973, Time=0.1635 sec\n",
      "Epoch [807 / 2000]: Train Loss=5.2082, Val Cor=0.3407, Time=0.1632 sec\n",
      "Epoch [808 / 2000]: Train Loss=5.1725, Val Cor=0.5273, Time=0.1633 sec\n",
      "Epoch [809 / 2000]: Train Loss=5.2958, Val Cor=0.5056, Time=0.1628 sec\n",
      "Epoch [810 / 2000]: Train Loss=5.2639, Val Cor=-0.4558, Time=0.1616 sec\n",
      "Epoch [811 / 2000]: Train Loss=5.2865, Val Cor=-0.3970, Time=0.1613 sec\n",
      "Epoch [812 / 2000]: Train Loss=5.0948, Val Cor=0.5352, Time=0.1676 sec\n",
      "Epoch [813 / 2000]: Train Loss=5.3886, Val Cor=0.5151, Time=0.1621 sec\n",
      "Epoch [814 / 2000]: Train Loss=5.0785, Val Cor=0.2190, Time=0.1626 sec\n",
      "Epoch [815 / 2000]: Train Loss=5.2823, Val Cor=0.4861, Time=0.1619 sec\n",
      "Epoch [816 / 2000]: Train Loss=5.2610, Val Cor=0.3121, Time=0.1631 sec\n",
      "Epoch [817 / 2000]: Train Loss=5.0536, Val Cor=0.5402, Time=0.1636 sec\n",
      "Epoch [818 / 2000]: Train Loss=4.9447, Val Cor=0.5230, Time=0.1643 sec\n",
      "Epoch [819 / 2000]: Train Loss=4.9966, Val Cor=-0.0058, Time=0.1631 sec\n",
      "Epoch [820 / 2000]: Train Loss=5.1216, Val Cor=0.5146, Time=0.1634 sec\n",
      "Epoch [821 / 2000]: Train Loss=5.3874, Val Cor=0.4409, Time=0.1628 sec\n",
      "Epoch [822 / 2000]: Train Loss=5.1898, Val Cor=0.0397, Time=0.1635 sec\n",
      "Epoch [823 / 2000]: Train Loss=5.0286, Val Cor=0.4191, Time=0.1626 sec\n",
      "Epoch [824 / 2000]: Train Loss=5.1853, Val Cor=0.5316, Time=0.1617 sec\n",
      "Epoch [825 / 2000]: Train Loss=5.2733, Val Cor=0.5275, Time=0.1610 sec\n",
      "Epoch [826 / 2000]: Train Loss=5.0390, Val Cor=0.5085, Time=0.1614 sec\n",
      "Epoch [827 / 2000]: Train Loss=5.0516, Val Cor=0.0351, Time=0.1613 sec\n",
      "Epoch [828 / 2000]: Train Loss=5.1048, Val Cor=0.5403, Time=0.1614 sec\n",
      "Epoch [829 / 2000]: Train Loss=5.1504, Val Cor=0.5134, Time=0.1626 sec\n",
      "Epoch [830 / 2000]: Train Loss=5.1144, Val Cor=-0.3238, Time=0.1625 sec\n",
      "Epoch [831 / 2000]: Train Loss=5.3273, Val Cor=0.2800, Time=0.1624 sec\n",
      "Epoch [832 / 2000]: Train Loss=4.9974, Val Cor=0.5279, Time=0.1633 sec\n",
      "Epoch [833 / 2000]: Train Loss=5.1250, Val Cor=0.2144, Time=0.1636 sec\n",
      "Epoch [834 / 2000]: Train Loss=5.1010, Val Cor=0.3777, Time=0.1648 sec\n",
      "Epoch [835 / 2000]: Train Loss=5.0116, Val Cor=0.2486, Time=0.1628 sec\n",
      "Epoch [836 / 2000]: Train Loss=4.8871, Val Cor=0.5061, Time=0.1620 sec\n",
      "Epoch [837 / 2000]: Train Loss=5.2572, Val Cor=0.1242, Time=0.1613 sec\n",
      "Epoch [838 / 2000]: Train Loss=5.6921, Val Cor=-0.2345, Time=0.1618 sec\n",
      "Epoch [839 / 2000]: Train Loss=5.1204, Val Cor=0.0940, Time=0.1618 sec\n",
      "Epoch [840 / 2000]: Train Loss=5.4664, Val Cor=0.5360, Time=0.1615 sec\n",
      "Epoch [841 / 2000]: Train Loss=5.4414, Val Cor=0.5567, Time=0.1613 sec\n",
      "Epoch [842 / 2000]: Train Loss=5.2074, Val Cor=0.4839, Time=0.1614 sec\n",
      "Epoch [843 / 2000]: Train Loss=5.2961, Val Cor=0.3069, Time=0.1623 sec\n",
      "Epoch [844 / 2000]: Train Loss=5.3966, Val Cor=0.3885, Time=0.1624 sec\n",
      "Epoch [845 / 2000]: Train Loss=5.6676, Val Cor=0.4173, Time=0.1627 sec\n",
      "Epoch [846 / 2000]: Train Loss=5.4188, Val Cor=0.5002, Time=0.1626 sec\n",
      "Epoch [847 / 2000]: Train Loss=5.2197, Val Cor=0.5194, Time=0.1625 sec\n",
      "Epoch [848 / 2000]: Train Loss=5.1412, Val Cor=0.5306, Time=0.1621 sec\n",
      "Epoch [849 / 2000]: Train Loss=5.2128, Val Cor=0.3897, Time=0.1624 sec\n",
      "Epoch [850 / 2000]: Train Loss=5.0893, Val Cor=0.5327, Time=0.1628 sec\n",
      "Epoch [851 / 2000]: Train Loss=4.9907, Val Cor=0.2433, Time=0.1630 sec\n",
      "Epoch [852 / 2000]: Train Loss=5.0840, Val Cor=0.5492, Time=0.1626 sec\n",
      "Epoch [853 / 2000]: Train Loss=4.8591, Val Cor=-0.0576, Time=0.1632 sec\n",
      "Epoch [854 / 2000]: Train Loss=5.2809, Val Cor=0.2724, Time=0.1642 sec\n",
      "Epoch [855 / 2000]: Train Loss=5.3462, Val Cor=-0.4557, Time=0.1636 sec\n",
      "Epoch [856 / 2000]: Train Loss=5.5438, Val Cor=0.2780, Time=0.1634 sec\n",
      "Epoch [857 / 2000]: Train Loss=5.5512, Val Cor=-0.1245, Time=0.1618 sec\n",
      "Epoch [858 / 2000]: Train Loss=5.1784, Val Cor=0.3485, Time=0.1625 sec\n",
      "Epoch [859 / 2000]: Train Loss=4.9923, Val Cor=0.5586, Time=0.1627 sec\n",
      "Epoch [860 / 2000]: Train Loss=4.9609, Val Cor=0.2234, Time=0.1637 sec\n",
      "Epoch [861 / 2000]: Train Loss=4.9822, Val Cor=0.5084, Time=0.1626 sec\n",
      "Epoch [862 / 2000]: Train Loss=5.4248, Val Cor=0.4697, Time=0.1625 sec\n",
      "Epoch [863 / 2000]: Train Loss=5.2385, Val Cor=0.3782, Time=0.1625 sec\n",
      "Epoch [864 / 2000]: Train Loss=4.8906, Val Cor=-0.4316, Time=0.1627 sec\n",
      "Epoch [865 / 2000]: Train Loss=5.1870, Val Cor=0.2646, Time=0.1623 sec\n",
      "Epoch [866 / 2000]: Train Loss=5.0268, Val Cor=0.5181, Time=0.1613 sec\n",
      "Epoch [867 / 2000]: Train Loss=5.9099, Val Cor=0.1435, Time=0.1607 sec\n",
      "Epoch [868 / 2000]: Train Loss=5.6555, Val Cor=0.5373, Time=0.1602 sec\n",
      "Epoch [869 / 2000]: Train Loss=5.2574, Val Cor=0.4740, Time=0.1602 sec\n",
      "Epoch [870 / 2000]: Train Loss=5.1024, Val Cor=0.2430, Time=0.1605 sec\n",
      "Epoch [871 / 2000]: Train Loss=5.3316, Val Cor=0.4723, Time=0.1609 sec\n",
      "Epoch [872 / 2000]: Train Loss=5.2538, Val Cor=0.5206, Time=0.1614 sec\n",
      "Epoch [873 / 2000]: Train Loss=5.2980, Val Cor=0.1956, Time=0.1612 sec\n",
      "Epoch [874 / 2000]: Train Loss=5.1454, Val Cor=0.3357, Time=0.1670 sec\n",
      "Epoch [875 / 2000]: Train Loss=5.3691, Val Cor=-0.0162, Time=0.1630 sec\n",
      "Epoch [876 / 2000]: Train Loss=5.3961, Val Cor=-0.3757, Time=0.1635 sec\n",
      "Epoch [877 / 2000]: Train Loss=5.0370, Val Cor=-0.1840, Time=0.1633 sec\n",
      "Epoch [878 / 2000]: Train Loss=5.2360, Val Cor=0.4738, Time=0.1627 sec\n",
      "Epoch [879 / 2000]: Train Loss=5.0987, Val Cor=0.5153, Time=0.1612 sec\n",
      "Epoch [880 / 2000]: Train Loss=5.1922, Val Cor=-0.1761, Time=0.1624 sec\n",
      "Epoch [881 / 2000]: Train Loss=5.3775, Val Cor=0.5313, Time=0.1619 sec\n",
      "Epoch [882 / 2000]: Train Loss=5.0180, Val Cor=0.5219, Time=0.1624 sec\n",
      "Epoch [883 / 2000]: Train Loss=5.2302, Val Cor=0.4855, Time=0.1622 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [884 / 2000]: Train Loss=5.4069, Val Cor=0.4891, Time=0.1627 sec\n",
      "Epoch [885 / 2000]: Train Loss=5.0764, Val Cor=0.4862, Time=0.1633 sec\n",
      "Epoch [886 / 2000]: Train Loss=5.3121, Val Cor=0.4902, Time=0.1632 sec\n",
      "Epoch [887 / 2000]: Train Loss=5.0764, Val Cor=-0.0327, Time=0.1631 sec\n",
      "Epoch [888 / 2000]: Train Loss=5.1911, Val Cor=0.5345, Time=0.1634 sec\n",
      "Epoch [889 / 2000]: Train Loss=5.1552, Val Cor=0.5224, Time=0.1627 sec\n",
      "Epoch [890 / 2000]: Train Loss=5.0657, Val Cor=0.5096, Time=0.1635 sec\n",
      "Epoch [891 / 2000]: Train Loss=5.2817, Val Cor=-0.2485, Time=0.1612 sec\n",
      "Epoch [892 / 2000]: Train Loss=5.3998, Val Cor=0.0148, Time=0.1611 sec\n",
      "Epoch [893 / 2000]: Train Loss=5.0876, Val Cor=0.0305, Time=0.1616 sec\n",
      "Epoch [894 / 2000]: Train Loss=5.5061, Val Cor=0.4626, Time=0.1614 sec\n",
      "Epoch [895 / 2000]: Train Loss=5.2369, Val Cor=0.5256, Time=0.1614 sec\n",
      "Epoch [896 / 2000]: Train Loss=4.9470, Val Cor=0.5011, Time=0.1627 sec\n",
      "Epoch [897 / 2000]: Train Loss=4.9442, Val Cor=0.5351, Time=0.1623 sec\n",
      "Epoch [898 / 2000]: Train Loss=5.4121, Val Cor=0.5241, Time=0.1629 sec\n",
      "Epoch [899 / 2000]: Train Loss=5.2235, Val Cor=0.3238, Time=0.1633 sec\n",
      "Epoch [900 / 2000]: Train Loss=4.9308, Val Cor=0.5564, Time=0.1624 sec\n",
      "Epoch [901 / 2000]: Train Loss=4.9679, Val Cor=0.5415, Time=0.1615 sec\n",
      "Epoch [902 / 2000]: Train Loss=5.2434, Val Cor=0.2594, Time=0.1618 sec\n",
      "Epoch [903 / 2000]: Train Loss=5.0813, Val Cor=0.3532, Time=0.1621 sec\n",
      "Epoch [904 / 2000]: Train Loss=5.0784, Val Cor=0.0148, Time=0.1660 sec\n",
      "Epoch [905 / 2000]: Train Loss=5.0603, Val Cor=0.5329, Time=0.1629 sec\n",
      "Epoch [906 / 2000]: Train Loss=5.1006, Val Cor=-0.0608, Time=0.1647 sec\n",
      "Epoch [907 / 2000]: Train Loss=5.2163, Val Cor=0.5369, Time=0.1637 sec\n",
      "Epoch [908 / 2000]: Train Loss=5.2381, Val Cor=0.0877, Time=0.1628 sec\n",
      "Epoch [909 / 2000]: Train Loss=4.8994, Val Cor=0.5408, Time=0.1625 sec\n",
      "Epoch [910 / 2000]: Train Loss=4.9352, Val Cor=0.4878, Time=0.1621 sec\n",
      "Epoch [911 / 2000]: Train Loss=4.9917, Val Cor=0.5109, Time=0.1603 sec\n",
      "Epoch [912 / 2000]: Train Loss=4.9820, Val Cor=0.2985, Time=0.1603 sec\n",
      "Epoch [913 / 2000]: Train Loss=5.2807, Val Cor=0.4118, Time=0.1595 sec\n",
      "Epoch [914 / 2000]: Train Loss=5.2475, Val Cor=0.2690, Time=0.1598 sec\n",
      "Epoch [915 / 2000]: Train Loss=5.5112, Val Cor=0.5259, Time=0.1595 sec\n",
      "Epoch [916 / 2000]: Train Loss=5.1886, Val Cor=0.3650, Time=0.1597 sec\n",
      "Epoch [917 / 2000]: Train Loss=5.3817, Val Cor=0.2852, Time=0.1607 sec\n",
      "Epoch [918 / 2000]: Train Loss=5.3157, Val Cor=0.3285, Time=0.1608 sec\n",
      "Epoch [919 / 2000]: Train Loss=5.3146, Val Cor=0.4609, Time=0.1614 sec\n",
      "Epoch [920 / 2000]: Train Loss=5.1160, Val Cor=0.5340, Time=0.1608 sec\n",
      "Epoch [921 / 2000]: Train Loss=5.1292, Val Cor=0.4426, Time=0.1592 sec\n",
      "Epoch [922 / 2000]: Train Loss=5.3647, Val Cor=0.3862, Time=0.1598 sec\n",
      "Epoch [923 / 2000]: Train Loss=5.3669, Val Cor=0.4706, Time=0.1597 sec\n",
      "Epoch [924 / 2000]: Train Loss=5.2288, Val Cor=-0.2575, Time=0.1605 sec\n",
      "Epoch [925 / 2000]: Train Loss=5.2098, Val Cor=0.5358, Time=0.1603 sec\n",
      "Epoch [926 / 2000]: Train Loss=5.2875, Val Cor=0.3749, Time=0.1616 sec\n",
      "Epoch [927 / 2000]: Train Loss=5.0803, Val Cor=0.5258, Time=0.1621 sec\n",
      "Epoch [928 / 2000]: Train Loss=5.2047, Val Cor=0.5286, Time=0.1619 sec\n",
      "Epoch [929 / 2000]: Train Loss=5.1772, Val Cor=0.1114, Time=0.1622 sec\n",
      "Epoch [930 / 2000]: Train Loss=5.2586, Val Cor=0.4253, Time=0.1623 sec\n",
      "Epoch [931 / 2000]: Train Loss=5.1528, Val Cor=0.2191, Time=0.1601 sec\n",
      "Epoch [932 / 2000]: Train Loss=5.2831, Val Cor=0.2628, Time=0.1598 sec\n",
      "Epoch [933 / 2000]: Train Loss=5.1260, Val Cor=0.5209, Time=0.1589 sec\n",
      "Epoch [934 / 2000]: Train Loss=5.1923, Val Cor=-0.1192, Time=0.1596 sec\n",
      "Epoch [935 / 2000]: Train Loss=5.0033, Val Cor=0.5390, Time=0.1594 sec\n",
      "Epoch [936 / 2000]: Train Loss=5.2628, Val Cor=0.5601, Time=0.1611 sec\n",
      "Epoch [937 / 2000]: Train Loss=5.5601, Val Cor=0.3012, Time=0.1606 sec\n",
      "Epoch [938 / 2000]: Train Loss=5.0996, Val Cor=-0.2424, Time=0.1609 sec\n",
      "Epoch [939 / 2000]: Train Loss=5.4759, Val Cor=0.2812, Time=0.1605 sec\n",
      "Epoch [940 / 2000]: Train Loss=4.9995, Val Cor=0.5371, Time=0.1612 sec\n",
      "Epoch [941 / 2000]: Train Loss=5.0101, Val Cor=0.5363, Time=0.1605 sec\n",
      "Epoch [942 / 2000]: Train Loss=5.2695, Val Cor=0.3093, Time=0.1592 sec\n",
      "Epoch [943 / 2000]: Train Loss=5.1407, Val Cor=0.0582, Time=0.1594 sec\n",
      "Epoch [944 / 2000]: Train Loss=4.9309, Val Cor=0.4970, Time=0.1607 sec\n",
      "Epoch [945 / 2000]: Train Loss=4.9395, Val Cor=0.2944, Time=0.1607 sec\n",
      "Epoch [946 / 2000]: Train Loss=4.9434, Val Cor=0.4717, Time=0.1624 sec\n",
      "Epoch [947 / 2000]: Train Loss=5.1011, Val Cor=0.5358, Time=0.1620 sec\n",
      "Epoch [948 / 2000]: Train Loss=4.9221, Val Cor=0.5337, Time=0.1624 sec\n",
      "Epoch [949 / 2000]: Train Loss=5.4825, Val Cor=0.4731, Time=0.1628 sec\n",
      "Epoch [950 / 2000]: Train Loss=5.3473, Val Cor=0.5302, Time=0.1628 sec\n",
      "Epoch [951 / 2000]: Train Loss=5.3277, Val Cor=-0.4268, Time=0.1610 sec\n",
      "Epoch [952 / 2000]: Train Loss=5.0326, Val Cor=0.5491, Time=0.1613 sec\n",
      "Epoch [953 / 2000]: Train Loss=5.0643, Val Cor=0.5207, Time=0.1607 sec\n",
      "Epoch [954 / 2000]: Train Loss=4.9609, Val Cor=0.2618, Time=0.1611 sec\n",
      "Epoch [955 / 2000]: Train Loss=5.0018, Val Cor=0.5130, Time=0.1609 sec\n",
      "Epoch [956 / 2000]: Train Loss=5.1273, Val Cor=0.5383, Time=0.1611 sec\n",
      "Epoch [957 / 2000]: Train Loss=5.0027, Val Cor=0.5135, Time=0.1620 sec\n",
      "Epoch [958 / 2000]: Train Loss=4.9143, Val Cor=0.5305, Time=0.1621 sec\n",
      "Epoch [959 / 2000]: Train Loss=5.1680, Val Cor=-0.1359, Time=0.1625 sec\n",
      "Epoch [960 / 2000]: Train Loss=5.1068, Val Cor=0.5290, Time=0.1622 sec\n",
      "Epoch [961 / 2000]: Train Loss=5.1458, Val Cor=0.2089, Time=0.1608 sec\n",
      "Epoch [962 / 2000]: Train Loss=5.3894, Val Cor=0.5420, Time=0.1609 sec\n",
      "Epoch [963 / 2000]: Train Loss=5.1574, Val Cor=-0.0794, Time=0.1607 sec\n",
      "Epoch [964 / 2000]: Train Loss=5.1645, Val Cor=0.4941, Time=0.1609 sec\n",
      "Epoch [965 / 2000]: Train Loss=4.8993, Val Cor=0.5228, Time=0.1610 sec\n",
      "Epoch [966 / 2000]: Train Loss=5.6796, Val Cor=0.4320, Time=0.1621 sec\n",
      "Epoch [967 / 2000]: Train Loss=5.3989, Val Cor=-0.4967, Time=0.1624 sec\n",
      "Epoch [968 / 2000]: Train Loss=5.0915, Val Cor=-0.2791, Time=0.1684 sec\n",
      "Epoch [969 / 2000]: Train Loss=5.4231, Val Cor=0.3214, Time=0.1622 sec\n",
      "Epoch [970 / 2000]: Train Loss=5.4110, Val Cor=0.5403, Time=0.1618 sec\n",
      "Epoch [971 / 2000]: Train Loss=5.0084, Val Cor=0.1788, Time=0.1614 sec\n",
      "Epoch [972 / 2000]: Train Loss=5.2422, Val Cor=0.5391, Time=0.1618 sec\n",
      "Epoch [973 / 2000]: Train Loss=5.1947, Val Cor=0.5334, Time=0.1618 sec\n",
      "Epoch [974 / 2000]: Train Loss=4.9208, Val Cor=0.5221, Time=0.1619 sec\n",
      "Epoch [975 / 2000]: Train Loss=5.2589, Val Cor=0.4690, Time=0.1618 sec\n",
      "Epoch [976 / 2000]: Train Loss=5.4183, Val Cor=0.5059, Time=0.1630 sec\n",
      "Epoch [977 / 2000]: Train Loss=5.0238, Val Cor=0.0482, Time=0.1635 sec\n",
      "Epoch [978 / 2000]: Train Loss=4.8850, Val Cor=-0.1477, Time=0.1643 sec\n",
      "Epoch [979 / 2000]: Train Loss=4.9218, Val Cor=0.5275, Time=0.1622 sec\n",
      "Epoch [980 / 2000]: Train Loss=4.9613, Val Cor=0.5174, Time=0.1629 sec\n",
      "Epoch [981 / 2000]: Train Loss=4.7441, Val Cor=-0.0221, Time=0.1626 sec\n",
      "Epoch [982 / 2000]: Train Loss=4.9105, Val Cor=0.5477, Time=0.1612 sec\n",
      "Epoch [983 / 2000]: Train Loss=5.1577, Val Cor=0.2964, Time=0.1606 sec\n",
      "Epoch [984 / 2000]: Train Loss=5.0561, Val Cor=-0.4095, Time=0.1610 sec\n",
      "Epoch [985 / 2000]: Train Loss=4.9490, Val Cor=0.1150, Time=0.1604 sec\n",
      "Epoch [986 / 2000]: Train Loss=5.0615, Val Cor=0.4610, Time=0.1608 sec\n",
      "Epoch [987 / 2000]: Train Loss=5.0429, Val Cor=0.3276, Time=0.1621 sec\n",
      "Epoch [988 / 2000]: Train Loss=5.1156, Val Cor=0.5027, Time=0.1622 sec\n",
      "Epoch [989 / 2000]: Train Loss=5.0791, Val Cor=0.2359, Time=0.1618 sec\n",
      "Epoch [990 / 2000]: Train Loss=5.1783, Val Cor=0.0152, Time=0.1632 sec\n",
      "Epoch [991 / 2000]: Train Loss=5.2279, Val Cor=0.5327, Time=0.1620 sec\n",
      "Epoch [992 / 2000]: Train Loss=4.9439, Val Cor=0.5221, Time=0.1604 sec\n",
      "Epoch [993 / 2000]: Train Loss=5.0125, Val Cor=0.4453, Time=0.1601 sec\n",
      "Epoch [994 / 2000]: Train Loss=5.1372, Val Cor=0.5499, Time=0.1605 sec\n",
      "Epoch [995 / 2000]: Train Loss=4.9913, Val Cor=0.4222, Time=0.1607 sec\n",
      "Epoch [996 / 2000]: Train Loss=5.0720, Val Cor=0.3404, Time=0.1609 sec\n",
      "Epoch [997 / 2000]: Train Loss=5.1604, Val Cor=0.1263, Time=0.1611 sec\n",
      "Epoch [998 / 2000]: Train Loss=5.0354, Val Cor=0.5066, Time=0.1625 sec\n",
      "Epoch [999 / 2000]: Train Loss=5.0354, Val Cor=0.1405, Time=0.1629 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000 / 2000]: Train Loss=4.9730, Val Cor=-0.0696, Time=0.1630 sec\n",
      "Epoch [1001 / 2000]: Train Loss=5.0457, Val Cor=0.5015, Time=0.1620 sec\n",
      "Epoch [1002 / 2000]: Train Loss=4.9414, Val Cor=0.5325, Time=0.1612 sec\n",
      "Epoch [1003 / 2000]: Train Loss=4.8409, Val Cor=0.2719, Time=0.1609 sec\n",
      "Epoch [1004 / 2000]: Train Loss=4.8835, Val Cor=0.5155, Time=0.1618 sec\n",
      "Epoch [1005 / 2000]: Train Loss=4.9049, Val Cor=0.5375, Time=0.1617 sec\n",
      "Epoch [1006 / 2000]: Train Loss=4.9264, Val Cor=0.5101, Time=0.1622 sec\n",
      "Epoch [1007 / 2000]: Train Loss=4.9308, Val Cor=0.0178, Time=0.1621 sec\n",
      "Epoch [1008 / 2000]: Train Loss=5.3074, Val Cor=0.4913, Time=0.1630 sec\n",
      "Epoch [1009 / 2000]: Train Loss=5.1311, Val Cor=0.1194, Time=0.1628 sec\n",
      "Epoch [1010 / 2000]: Train Loss=5.0556, Val Cor=0.4116, Time=0.1636 sec\n",
      "Epoch [1011 / 2000]: Train Loss=5.0194, Val Cor=-0.4207, Time=0.1629 sec\n",
      "Epoch [1012 / 2000]: Train Loss=5.0646, Val Cor=-0.3838, Time=0.1620 sec\n",
      "Epoch [1013 / 2000]: Train Loss=5.2926, Val Cor=0.5478, Time=0.1627 sec\n",
      "Epoch [1014 / 2000]: Train Loss=5.1539, Val Cor=0.5163, Time=0.1631 sec\n",
      "Epoch [1015 / 2000]: Train Loss=5.1787, Val Cor=-0.1573, Time=0.1613 sec\n",
      "Epoch [1016 / 2000]: Train Loss=5.2066, Val Cor=-0.3038, Time=0.1611 sec\n",
      "Epoch [1017 / 2000]: Train Loss=4.9778, Val Cor=0.5181, Time=0.1621 sec\n",
      "Epoch [1018 / 2000]: Train Loss=5.1799, Val Cor=0.2438, Time=0.1620 sec\n",
      "Epoch [1019 / 2000]: Train Loss=4.9258, Val Cor=0.5444, Time=0.1616 sec\n",
      "Epoch [1020 / 2000]: Train Loss=4.9325, Val Cor=0.1453, Time=0.1626 sec\n",
      "Epoch [1021 / 2000]: Train Loss=4.9930, Val Cor=0.5321, Time=0.1623 sec\n",
      "Epoch [1022 / 2000]: Train Loss=4.8592, Val Cor=0.5307, Time=0.1622 sec\n",
      "Epoch [1023 / 2000]: Train Loss=4.9975, Val Cor=-0.5273, Time=0.1605 sec\n",
      "Epoch [1024 / 2000]: Train Loss=5.1142, Val Cor=0.5292, Time=0.1606 sec\n",
      "Epoch [1025 / 2000]: Train Loss=5.1326, Val Cor=0.5394, Time=0.1607 sec\n",
      "Epoch [1026 / 2000]: Train Loss=5.0341, Val Cor=0.5336, Time=0.1610 sec\n",
      "Epoch [1027 / 2000]: Train Loss=4.7558, Val Cor=0.5369, Time=0.1610 sec\n",
      "Epoch [1028 / 2000]: Train Loss=4.9711, Val Cor=-0.1839, Time=0.1621 sec\n",
      "Epoch [1029 / 2000]: Train Loss=5.0350, Val Cor=0.5254, Time=0.1623 sec\n",
      "Epoch [1030 / 2000]: Train Loss=5.0248, Val Cor=-0.2049, Time=0.1624 sec\n",
      "Epoch [1031 / 2000]: Train Loss=5.2272, Val Cor=0.2129, Time=0.1629 sec\n",
      "Epoch [1032 / 2000]: Train Loss=5.3285, Val Cor=0.0156, Time=0.1625 sec\n",
      "Epoch [1033 / 2000]: Train Loss=5.2432, Val Cor=0.4684, Time=0.1626 sec\n",
      "Epoch [1034 / 2000]: Train Loss=5.1870, Val Cor=0.4053, Time=0.1612 sec\n",
      "Epoch [1035 / 2000]: Train Loss=4.8612, Val Cor=0.2099, Time=0.1606 sec\n",
      "Epoch [1036 / 2000]: Train Loss=6.3679, Val Cor=0.2492, Time=0.1615 sec\n",
      "Epoch [1037 / 2000]: Train Loss=6.3479, Val Cor=0.5311, Time=0.1614 sec\n",
      "Epoch [1038 / 2000]: Train Loss=5.3509, Val Cor=0.5061, Time=0.1621 sec\n",
      "Epoch [1039 / 2000]: Train Loss=4.9938, Val Cor=0.4995, Time=0.1628 sec\n",
      "Epoch [1040 / 2000]: Train Loss=5.2656, Val Cor=0.5206, Time=0.1638 sec\n",
      "Epoch [1041 / 2000]: Train Loss=5.2281, Val Cor=-0.2905, Time=0.1632 sec\n",
      "Epoch [1042 / 2000]: Train Loss=4.9949, Val Cor=-0.2874, Time=0.1628 sec\n",
      "Epoch [1043 / 2000]: Train Loss=5.0534, Val Cor=0.3999, Time=0.1622 sec\n",
      "Epoch [1044 / 2000]: Train Loss=5.0480, Val Cor=0.5233, Time=0.1629 sec\n",
      "Epoch [1045 / 2000]: Train Loss=4.8002, Val Cor=-0.1505, Time=0.1629 sec\n",
      "Epoch [1046 / 2000]: Train Loss=4.9510, Val Cor=0.3860, Time=0.1608 sec\n",
      "Epoch [1047 / 2000]: Train Loss=5.0402, Val Cor=0.5159, Time=0.1610 sec\n",
      "Epoch [1048 / 2000]: Train Loss=4.8622, Val Cor=0.5280, Time=0.1610 sec\n",
      "Epoch [1049 / 2000]: Train Loss=4.8675, Val Cor=0.5196, Time=0.1614 sec\n",
      "Epoch [1050 / 2000]: Train Loss=4.7604, Val Cor=0.4438, Time=0.1624 sec\n",
      "Epoch [1051 / 2000]: Train Loss=4.6705, Val Cor=0.5217, Time=0.1618 sec\n",
      "Epoch [1052 / 2000]: Train Loss=4.9392, Val Cor=0.1673, Time=0.1618 sec\n",
      "Epoch [1053 / 2000]: Train Loss=4.9904, Val Cor=0.2434, Time=0.1624 sec\n",
      "Epoch [1054 / 2000]: Train Loss=4.8692, Val Cor=0.3970, Time=0.1621 sec\n",
      "Epoch [1055 / 2000]: Train Loss=4.7325, Val Cor=0.5178, Time=0.1607 sec\n",
      "Epoch [1056 / 2000]: Train Loss=5.0197, Val Cor=0.0645, Time=0.1603 sec\n",
      "Epoch [1057 / 2000]: Train Loss=5.1371, Val Cor=0.1497, Time=0.1602 sec\n",
      "Epoch [1058 / 2000]: Train Loss=5.3326, Val Cor=-0.1876, Time=0.1608 sec\n",
      "Epoch [1059 / 2000]: Train Loss=5.2673, Val Cor=0.0902, Time=0.1604 sec\n",
      "Epoch [1060 / 2000]: Train Loss=5.0543, Val Cor=0.2102, Time=0.1611 sec\n",
      "Epoch [1061 / 2000]: Train Loss=4.8540, Val Cor=0.5263, Time=0.1617 sec\n",
      "Epoch [1062 / 2000]: Train Loss=5.2108, Val Cor=0.5371, Time=0.1618 sec\n",
      "Epoch [1063 / 2000]: Train Loss=4.9481, Val Cor=0.3725, Time=0.1615 sec\n",
      "Epoch [1064 / 2000]: Train Loss=5.1423, Val Cor=0.3732, Time=0.1627 sec\n",
      "Epoch [1065 / 2000]: Train Loss=5.1265, Val Cor=0.4751, Time=0.1621 sec\n",
      "Epoch [1066 / 2000]: Train Loss=5.2884, Val Cor=-0.1233, Time=0.1619 sec\n",
      "Epoch [1067 / 2000]: Train Loss=5.1709, Val Cor=0.4936, Time=0.1607 sec\n",
      "Epoch [1068 / 2000]: Train Loss=5.4429, Val Cor=0.2036, Time=0.1609 sec\n",
      "Epoch [1069 / 2000]: Train Loss=5.0689, Val Cor=-0.0584, Time=0.1614 sec\n",
      "Epoch [1070 / 2000]: Train Loss=5.0871, Val Cor=0.3588, Time=0.1613 sec\n",
      "Epoch [1071 / 2000]: Train Loss=4.9462, Val Cor=-0.1256, Time=0.1611 sec\n",
      "Epoch [1072 / 2000]: Train Loss=5.0145, Val Cor=0.1314, Time=0.1611 sec\n",
      "Epoch [1073 / 2000]: Train Loss=5.0649, Val Cor=0.3900, Time=0.1617 sec\n",
      "Epoch [1074 / 2000]: Train Loss=5.1053, Val Cor=-0.2764, Time=0.1628 sec\n",
      "Epoch [1075 / 2000]: Train Loss=5.0754, Val Cor=0.0528, Time=0.1626 sec\n",
      "Epoch [1076 / 2000]: Train Loss=5.1407, Val Cor=-0.4032, Time=0.1631 sec\n",
      "Epoch [1077 / 2000]: Train Loss=8.5031, Val Cor=0.3837, Time=0.1626 sec\n",
      "Epoch [1078 / 2000]: Train Loss=8.2385, Val Cor=0.3620, Time=0.1617 sec\n",
      "Epoch [1079 / 2000]: Train Loss=8.0407, Val Cor=-0.1473, Time=0.1613 sec\n",
      "Epoch [1080 / 2000]: Train Loss=7.1464, Val Cor=0.4408, Time=0.1616 sec\n",
      "Epoch [1081 / 2000]: Train Loss=6.3927, Val Cor=-0.2365, Time=0.1618 sec\n",
      "Epoch [1082 / 2000]: Train Loss=5.6547, Val Cor=-0.2993, Time=0.1623 sec\n",
      "Epoch [1083 / 2000]: Train Loss=5.5378, Val Cor=-0.2837, Time=0.1615 sec\n",
      "Epoch [1084 / 2000]: Train Loss=5.3401, Val Cor=0.4355, Time=0.1621 sec\n",
      "Epoch [1085 / 2000]: Train Loss=5.3284, Val Cor=-0.0029, Time=0.1625 sec\n",
      "Epoch [1086 / 2000]: Train Loss=5.2732, Val Cor=-0.1557, Time=0.1628 sec\n",
      "Epoch [1087 / 2000]: Train Loss=5.3492, Val Cor=0.3052, Time=0.1624 sec\n",
      "Epoch [1088 / 2000]: Train Loss=4.8681, Val Cor=0.3900, Time=0.1627 sec\n",
      "Epoch [1089 / 2000]: Train Loss=5.0517, Val Cor=0.4803, Time=0.1626 sec\n",
      "Epoch [1090 / 2000]: Train Loss=5.0204, Val Cor=0.5214, Time=0.1629 sec\n",
      "Epoch [1091 / 2000]: Train Loss=5.1281, Val Cor=0.5243, Time=0.1620 sec\n",
      "Epoch [1092 / 2000]: Train Loss=4.9011, Val Cor=0.5256, Time=0.1613 sec\n",
      "Epoch [1093 / 2000]: Train Loss=5.0080, Val Cor=0.5166, Time=0.1604 sec\n",
      "Epoch [1094 / 2000]: Train Loss=4.9452, Val Cor=0.4343, Time=0.1604 sec\n",
      "Epoch [1095 / 2000]: Train Loss=4.9831, Val Cor=0.1005, Time=0.1604 sec\n",
      "Epoch [1096 / 2000]: Train Loss=4.9986, Val Cor=-0.2366, Time=0.1608 sec\n",
      "Epoch [1097 / 2000]: Train Loss=4.9516, Val Cor=0.3242, Time=0.1595 sec\n",
      "Epoch [1098 / 2000]: Train Loss=5.0394, Val Cor=0.5019, Time=0.1610 sec\n",
      "Epoch [1099 / 2000]: Train Loss=4.9624, Val Cor=0.4688, Time=0.1612 sec\n",
      "Epoch [1100 / 2000]: Train Loss=5.2457, Val Cor=-0.0006, Time=0.1608 sec\n",
      "Epoch [1101 / 2000]: Train Loss=5.3275, Val Cor=0.3485, Time=0.1607 sec\n",
      "Epoch [1102 / 2000]: Train Loss=4.8036, Val Cor=-0.1307, Time=0.1618 sec\n",
      "Epoch [1103 / 2000]: Train Loss=5.0577, Val Cor=0.3042, Time=0.1607 sec\n",
      "Epoch [1104 / 2000]: Train Loss=5.0788, Val Cor=0.2590, Time=0.1600 sec\n",
      "Epoch [1105 / 2000]: Train Loss=4.8554, Val Cor=-0.3289, Time=0.1595 sec\n",
      "Epoch [1106 / 2000]: Train Loss=5.0506, Val Cor=0.3510, Time=0.1590 sec\n",
      "Epoch [1107 / 2000]: Train Loss=5.2386, Val Cor=0.5040, Time=0.1591 sec\n",
      "Epoch [1108 / 2000]: Train Loss=5.1583, Val Cor=-0.2284, Time=0.1608 sec\n",
      "Epoch [1109 / 2000]: Train Loss=4.9044, Val Cor=0.1795, Time=0.1600 sec\n",
      "Epoch [1110 / 2000]: Train Loss=4.8733, Val Cor=-0.0469, Time=0.1606 sec\n",
      "Epoch [1111 / 2000]: Train Loss=5.0524, Val Cor=0.4950, Time=0.1606 sec\n",
      "Epoch [1112 / 2000]: Train Loss=5.0702, Val Cor=0.3349, Time=0.1610 sec\n",
      "Epoch [1113 / 2000]: Train Loss=5.0467, Val Cor=0.5501, Time=0.1609 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1114 / 2000]: Train Loss=5.0802, Val Cor=0.5001, Time=0.1605 sec\n",
      "Epoch [1115 / 2000]: Train Loss=4.7271, Val Cor=-0.2932, Time=0.1615 sec\n",
      "Epoch [1116 / 2000]: Train Loss=5.0001, Val Cor=-0.0620, Time=0.1607 sec\n",
      "Epoch [1117 / 2000]: Train Loss=4.9063, Val Cor=0.4696, Time=0.1590 sec\n",
      "Epoch [1118 / 2000]: Train Loss=4.9499, Val Cor=0.4159, Time=0.1590 sec\n",
      "Epoch [1119 / 2000]: Train Loss=4.8882, Val Cor=0.3623, Time=0.1595 sec\n",
      "Epoch [1120 / 2000]: Train Loss=5.1369, Val Cor=0.4611, Time=0.1597 sec\n",
      "Epoch [1121 / 2000]: Train Loss=4.9943, Val Cor=0.5221, Time=0.1595 sec\n",
      "Epoch [1122 / 2000]: Train Loss=4.9407, Val Cor=-0.5243, Time=0.1602 sec\n",
      "Epoch [1123 / 2000]: Train Loss=5.1183, Val Cor=0.4516, Time=0.1619 sec\n",
      "Epoch [1124 / 2000]: Train Loss=5.1254, Val Cor=-0.1447, Time=0.1616 sec\n",
      "Epoch [1125 / 2000]: Train Loss=5.1144, Val Cor=-0.3611, Time=0.1624 sec\n",
      "Epoch [1126 / 2000]: Train Loss=4.8667, Val Cor=-0.3308, Time=0.1626 sec\n",
      "Epoch [1127 / 2000]: Train Loss=4.9819, Val Cor=0.4901, Time=0.1620 sec\n",
      "Epoch [1128 / 2000]: Train Loss=5.0038, Val Cor=0.2107, Time=0.1615 sec\n",
      "Epoch [1129 / 2000]: Train Loss=5.0760, Val Cor=-0.1100, Time=0.1611 sec\n",
      "Epoch [1130 / 2000]: Train Loss=5.0007, Val Cor=0.5161, Time=0.1610 sec\n",
      "Epoch [1131 / 2000]: Train Loss=5.0298, Val Cor=0.3815, Time=0.1614 sec\n",
      "Epoch [1132 / 2000]: Train Loss=5.2785, Val Cor=0.2136, Time=0.1612 sec\n",
      "Epoch [1133 / 2000]: Train Loss=4.8565, Val Cor=0.5045, Time=0.1609 sec\n",
      "Epoch [1134 / 2000]: Train Loss=5.0343, Val Cor=0.3002, Time=0.1620 sec\n",
      "Epoch [1135 / 2000]: Train Loss=5.1380, Val Cor=-0.4074, Time=0.1622 sec\n",
      "Epoch [1136 / 2000]: Train Loss=5.2379, Val Cor=-0.4658, Time=0.1624 sec\n",
      "Early stopping triggered.\n",
      "Training RNN model 2:\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1 / 2000]: Train Loss=8.2917, Val Cor=0.1803, Time=0.1625 sec\n",
      "Epoch [2 / 2000]: Train Loss=8.2653, Val Cor=0.3407, Time=0.1619 sec\n",
      "Epoch [3 / 2000]: Train Loss=8.2891, Val Cor=0.3077, Time=0.1617 sec\n",
      "Epoch [4 / 2000]: Train Loss=8.2182, Val Cor=0.4825, Time=0.1616 sec\n",
      "Epoch [5 / 2000]: Train Loss=8.2141, Val Cor=0.4834, Time=0.1610 sec\n",
      "Epoch [6 / 2000]: Train Loss=8.1870, Val Cor=0.4867, Time=0.1612 sec\n",
      "Epoch [7 / 2000]: Train Loss=8.2267, Val Cor=0.4896, Time=0.1613 sec\n",
      "Epoch [8 / 2000]: Train Loss=8.2495, Val Cor=0.4690, Time=0.1617 sec\n",
      "Epoch [9 / 2000]: Train Loss=8.2285, Val Cor=0.4827, Time=0.1604 sec\n",
      "Epoch [10 / 2000]: Train Loss=8.1439, Val Cor=0.4744, Time=0.1612 sec\n",
      "Epoch [11 / 2000]: Train Loss=8.1510, Val Cor=0.4750, Time=0.1610 sec\n",
      "Epoch [12 / 2000]: Train Loss=8.1776, Val Cor=0.4813, Time=0.1610 sec\n",
      "Epoch [13 / 2000]: Train Loss=8.0862, Val Cor=0.4873, Time=0.1610 sec\n",
      "Epoch [14 / 2000]: Train Loss=8.0798, Val Cor=0.4938, Time=0.1624 sec\n",
      "Epoch [15 / 2000]: Train Loss=7.9916, Val Cor=0.4939, Time=0.1617 sec\n",
      "Epoch [16 / 2000]: Train Loss=8.2439, Val Cor=0.5091, Time=0.1619 sec\n",
      "Epoch [17 / 2000]: Train Loss=8.1773, Val Cor=0.5004, Time=0.1621 sec\n",
      "Epoch [18 / 2000]: Train Loss=8.0537, Val Cor=0.5020, Time=0.1633 sec\n",
      "Epoch [19 / 2000]: Train Loss=7.9857, Val Cor=0.5019, Time=0.1640 sec\n",
      "Epoch [20 / 2000]: Train Loss=8.1335, Val Cor=0.5030, Time=0.1639 sec\n",
      "Epoch [21 / 2000]: Train Loss=8.1330, Val Cor=0.5026, Time=0.1623 sec\n",
      "Epoch [22 / 2000]: Train Loss=8.0666, Val Cor=0.5025, Time=0.1602 sec\n",
      "Epoch [23 / 2000]: Train Loss=7.8302, Val Cor=0.5065, Time=0.1601 sec\n",
      "Epoch [24 / 2000]: Train Loss=7.6612, Val Cor=0.5107, Time=0.1606 sec\n",
      "Epoch [25 / 2000]: Train Loss=7.7521, Val Cor=0.5122, Time=0.1600 sec\n",
      "Epoch [26 / 2000]: Train Loss=7.5603, Val Cor=0.5127, Time=0.1604 sec\n",
      "Epoch [27 / 2000]: Train Loss=7.6061, Val Cor=0.5148, Time=0.1600 sec\n",
      "Epoch [28 / 2000]: Train Loss=7.5160, Val Cor=0.5182, Time=0.1605 sec\n",
      "Epoch [29 / 2000]: Train Loss=7.3130, Val Cor=0.5148, Time=0.1609 sec\n",
      "Epoch [30 / 2000]: Train Loss=7.4683, Val Cor=0.5196, Time=0.1608 sec\n",
      "Epoch [31 / 2000]: Train Loss=7.7341, Val Cor=0.5181, Time=0.1599 sec\n",
      "Epoch [32 / 2000]: Train Loss=7.4841, Val Cor=0.5202, Time=0.1616 sec\n",
      "Epoch [33 / 2000]: Train Loss=7.3921, Val Cor=0.5196, Time=0.1611 sec\n",
      "Epoch [34 / 2000]: Train Loss=7.2991, Val Cor=0.5213, Time=0.1612 sec\n",
      "Epoch [35 / 2000]: Train Loss=7.1387, Val Cor=0.5163, Time=0.1609 sec\n",
      "Epoch [36 / 2000]: Train Loss=7.3028, Val Cor=0.5146, Time=0.1617 sec\n",
      "Epoch [37 / 2000]: Train Loss=7.0642, Val Cor=0.5072, Time=0.1611 sec\n",
      "Epoch [38 / 2000]: Train Loss=7.5473, Val Cor=0.5138, Time=0.1596 sec\n",
      "Epoch [39 / 2000]: Train Loss=7.1683, Val Cor=0.5103, Time=0.1596 sec\n",
      "Epoch [40 / 2000]: Train Loss=7.5172, Val Cor=0.5137, Time=0.1601 sec\n",
      "Epoch [41 / 2000]: Train Loss=7.3365, Val Cor=0.5134, Time=0.1599 sec\n",
      "Epoch [42 / 2000]: Train Loss=7.1509, Val Cor=0.5108, Time=0.1601 sec\n",
      "Epoch [43 / 2000]: Train Loss=7.3586, Val Cor=0.5069, Time=0.1598 sec\n",
      "Epoch [44 / 2000]: Train Loss=7.1432, Val Cor=0.5106, Time=0.1600 sec\n",
      "Epoch [45 / 2000]: Train Loss=6.9177, Val Cor=0.5114, Time=0.1606 sec\n",
      "Epoch [46 / 2000]: Train Loss=7.3227, Val Cor=0.5178, Time=0.1622 sec\n",
      "Epoch [47 / 2000]: Train Loss=7.3431, Val Cor=0.5174, Time=0.1617 sec\n",
      "Epoch [48 / 2000]: Train Loss=7.1692, Val Cor=0.5170, Time=0.1615 sec\n",
      "Epoch [49 / 2000]: Train Loss=6.9588, Val Cor=0.5140, Time=0.1622 sec\n",
      "Epoch [50 / 2000]: Train Loss=6.9334, Val Cor=0.5133, Time=0.1615 sec\n",
      "Epoch [51 / 2000]: Train Loss=6.9063, Val Cor=0.5092, Time=0.1602 sec\n",
      "Epoch [52 / 2000]: Train Loss=6.8441, Val Cor=0.5060, Time=0.1602 sec\n",
      "Epoch [53 / 2000]: Train Loss=7.4195, Val Cor=0.5042, Time=0.1597 sec\n",
      "Epoch [54 / 2000]: Train Loss=7.0565, Val Cor=0.5074, Time=0.1599 sec\n",
      "Epoch [55 / 2000]: Train Loss=6.9498, Val Cor=0.5038, Time=0.1598 sec\n",
      "Epoch [56 / 2000]: Train Loss=6.9520, Val Cor=0.5029, Time=0.1604 sec\n",
      "Epoch [57 / 2000]: Train Loss=7.0456, Val Cor=0.5015, Time=0.1620 sec\n",
      "Epoch [58 / 2000]: Train Loss=6.9459, Val Cor=0.5056, Time=0.1614 sec\n",
      "Epoch [59 / 2000]: Train Loss=6.9583, Val Cor=0.5042, Time=0.1616 sec\n",
      "Epoch [60 / 2000]: Train Loss=6.8104, Val Cor=0.5039, Time=0.1617 sec\n",
      "Epoch [61 / 2000]: Train Loss=6.8829, Val Cor=0.5016, Time=0.1597 sec\n",
      "Epoch [62 / 2000]: Train Loss=7.1354, Val Cor=0.5018, Time=0.1596 sec\n",
      "Epoch [63 / 2000]: Train Loss=6.8097, Val Cor=0.4996, Time=0.1594 sec\n",
      "Epoch [64 / 2000]: Train Loss=6.7726, Val Cor=0.5000, Time=0.1598 sec\n",
      "Epoch [65 / 2000]: Train Loss=7.0605, Val Cor=0.5033, Time=0.1598 sec\n",
      "Epoch [66 / 2000]: Train Loss=7.1516, Val Cor=0.5087, Time=0.1600 sec\n",
      "Epoch [67 / 2000]: Train Loss=6.6514, Val Cor=0.5060, Time=0.1597 sec\n",
      "Epoch [68 / 2000]: Train Loss=6.9431, Val Cor=0.5042, Time=0.1613 sec\n",
      "Epoch [69 / 2000]: Train Loss=6.8408, Val Cor=0.5000, Time=0.1614 sec\n",
      "Epoch [70 / 2000]: Train Loss=6.8701, Val Cor=0.5035, Time=0.1618 sec\n",
      "Epoch [71 / 2000]: Train Loss=6.9242, Val Cor=0.5034, Time=0.1617 sec\n",
      "Epoch [72 / 2000]: Train Loss=6.6836, Val Cor=0.5032, Time=0.1601 sec\n",
      "Epoch [73 / 2000]: Train Loss=6.9442, Val Cor=0.5047, Time=0.1599 sec\n",
      "Epoch [74 / 2000]: Train Loss=6.5537, Val Cor=0.5032, Time=0.1602 sec\n",
      "Epoch [75 / 2000]: Train Loss=6.7915, Val Cor=0.5060, Time=0.1604 sec\n",
      "Epoch [76 / 2000]: Train Loss=6.5592, Val Cor=0.5018, Time=0.1624 sec\n",
      "Epoch [77 / 2000]: Train Loss=6.7268, Val Cor=0.5035, Time=0.1621 sec\n",
      "Epoch [78 / 2000]: Train Loss=6.7406, Val Cor=0.5052, Time=0.1624 sec\n",
      "Epoch [79 / 2000]: Train Loss=6.9882, Val Cor=0.5063, Time=0.1622 sec\n",
      "Epoch [80 / 2000]: Train Loss=6.6241, Val Cor=0.5072, Time=0.1611 sec\n",
      "Epoch [81 / 2000]: Train Loss=6.7313, Val Cor=0.5082, Time=0.1604 sec\n",
      "Epoch [82 / 2000]: Train Loss=6.6707, Val Cor=0.5076, Time=0.1606 sec\n",
      "Epoch [83 / 2000]: Train Loss=6.7153, Val Cor=0.5087, Time=0.1603 sec\n",
      "Epoch [84 / 2000]: Train Loss=6.7020, Val Cor=0.5101, Time=0.1603 sec\n",
      "Epoch [85 / 2000]: Train Loss=6.5068, Val Cor=0.5096, Time=0.1630 sec\n",
      "Epoch [86 / 2000]: Train Loss=6.6419, Val Cor=0.5101, Time=0.1621 sec\n",
      "Epoch [87 / 2000]: Train Loss=6.7281, Val Cor=0.5118, Time=0.1616 sec\n",
      "Epoch [88 / 2000]: Train Loss=6.7148, Val Cor=0.5150, Time=0.1630 sec\n",
      "Epoch [89 / 2000]: Train Loss=6.5420, Val Cor=0.5088, Time=0.1619 sec\n",
      "Epoch [90 / 2000]: Train Loss=6.5723, Val Cor=0.5097, Time=0.1617 sec\n",
      "Epoch [91 / 2000]: Train Loss=6.8846, Val Cor=0.5121, Time=0.1604 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92 / 2000]: Train Loss=7.0186, Val Cor=0.5140, Time=0.1599 sec\n",
      "Epoch [93 / 2000]: Train Loss=6.5740, Val Cor=0.5123, Time=0.1596 sec\n",
      "Epoch [94 / 2000]: Train Loss=6.5069, Val Cor=0.5089, Time=0.1599 sec\n",
      "Epoch [95 / 2000]: Train Loss=6.6062, Val Cor=0.5100, Time=0.1601 sec\n",
      "Epoch [96 / 2000]: Train Loss=6.5302, Val Cor=0.5135, Time=0.1612 sec\n",
      "Epoch [97 / 2000]: Train Loss=6.5800, Val Cor=0.5143, Time=0.1614 sec\n",
      "Epoch [98 / 2000]: Train Loss=6.5965, Val Cor=0.5158, Time=0.1612 sec\n",
      "Epoch [99 / 2000]: Train Loss=6.6938, Val Cor=0.5145, Time=0.1613 sec\n",
      "Epoch [100 / 2000]: Train Loss=6.9718, Val Cor=0.5199, Time=0.1616 sec\n",
      "Epoch [101 / 2000]: Train Loss=6.8609, Val Cor=0.5200, Time=0.1609 sec\n",
      "Epoch [102 / 2000]: Train Loss=6.7514, Val Cor=0.5195, Time=0.1606 sec\n",
      "Epoch [103 / 2000]: Train Loss=6.6281, Val Cor=0.5186, Time=0.1614 sec\n",
      "Epoch [104 / 2000]: Train Loss=6.3284, Val Cor=0.5170, Time=0.1613 sec\n",
      "Epoch [105 / 2000]: Train Loss=6.5635, Val Cor=0.5187, Time=0.1595 sec\n",
      "Epoch [106 / 2000]: Train Loss=6.3455, Val Cor=0.5163, Time=0.1599 sec\n",
      "Epoch [107 / 2000]: Train Loss=6.5707, Val Cor=0.5186, Time=0.1585 sec\n",
      "Epoch [108 / 2000]: Train Loss=6.7575, Val Cor=0.5199, Time=0.1588 sec\n",
      "Epoch [109 / 2000]: Train Loss=6.6579, Val Cor=0.5196, Time=0.1588 sec\n",
      "Epoch [110 / 2000]: Train Loss=6.7246, Val Cor=0.5186, Time=0.1603 sec\n",
      "Epoch [111 / 2000]: Train Loss=6.4864, Val Cor=0.5200, Time=0.1601 sec\n",
      "Epoch [112 / 2000]: Train Loss=6.8933, Val Cor=0.5220, Time=0.1604 sec\n",
      "Epoch [113 / 2000]: Train Loss=6.5633, Val Cor=0.5224, Time=0.1602 sec\n",
      "Epoch [114 / 2000]: Train Loss=6.4152, Val Cor=0.5180, Time=0.1604 sec\n",
      "Epoch [115 / 2000]: Train Loss=6.6026, Val Cor=0.5165, Time=0.1662 sec\n",
      "Epoch [116 / 2000]: Train Loss=6.5350, Val Cor=0.5168, Time=0.1604 sec\n",
      "Epoch [117 / 2000]: Train Loss=6.6963, Val Cor=0.5177, Time=0.1599 sec\n",
      "Epoch [118 / 2000]: Train Loss=6.4324, Val Cor=0.5174, Time=0.1594 sec\n",
      "Epoch [119 / 2000]: Train Loss=6.5452, Val Cor=0.5250, Time=0.1602 sec\n",
      "Epoch [120 / 2000]: Train Loss=6.7303, Val Cor=0.5220, Time=0.1603 sec\n",
      "Epoch [121 / 2000]: Train Loss=6.9007, Val Cor=0.5305, Time=0.1605 sec\n",
      "Epoch [122 / 2000]: Train Loss=6.7719, Val Cor=0.5340, Time=0.1598 sec\n",
      "Epoch [123 / 2000]: Train Loss=6.3954, Val Cor=0.5316, Time=0.1593 sec\n",
      "Epoch [124 / 2000]: Train Loss=6.3840, Val Cor=0.5268, Time=0.1600 sec\n",
      "Epoch [125 / 2000]: Train Loss=6.6326, Val Cor=0.5269, Time=0.1603 sec\n",
      "Epoch [126 / 2000]: Train Loss=6.3541, Val Cor=0.5251, Time=0.1609 sec\n",
      "Epoch [127 / 2000]: Train Loss=6.7740, Val Cor=0.5294, Time=0.1616 sec\n",
      "Epoch [128 / 2000]: Train Loss=6.3886, Val Cor=0.5265, Time=0.1614 sec\n",
      "Epoch [129 / 2000]: Train Loss=6.3869, Val Cor=0.5240, Time=0.1612 sec\n",
      "Epoch [130 / 2000]: Train Loss=6.5172, Val Cor=0.5243, Time=0.1623 sec\n",
      "Epoch [131 / 2000]: Train Loss=6.4951, Val Cor=0.5241, Time=0.1616 sec\n",
      "Epoch [132 / 2000]: Train Loss=6.2752, Val Cor=0.5279, Time=0.1610 sec\n",
      "Epoch [133 / 2000]: Train Loss=6.2341, Val Cor=0.5238, Time=0.1598 sec\n",
      "Epoch [134 / 2000]: Train Loss=6.3076, Val Cor=0.5281, Time=0.1594 sec\n",
      "Epoch [135 / 2000]: Train Loss=6.4770, Val Cor=0.5332, Time=0.1593 sec\n",
      "Epoch [136 / 2000]: Train Loss=6.2912, Val Cor=0.5306, Time=0.1597 sec\n",
      "Epoch [137 / 2000]: Train Loss=6.6551, Val Cor=0.5280, Time=0.1596 sec\n",
      "Epoch [138 / 2000]: Train Loss=6.3173, Val Cor=0.5320, Time=0.1601 sec\n",
      "Epoch [139 / 2000]: Train Loss=6.2402, Val Cor=0.5340, Time=0.1596 sec\n",
      "Epoch [140 / 2000]: Train Loss=6.3137, Val Cor=0.5355, Time=0.1595 sec\n",
      "Epoch [141 / 2000]: Train Loss=6.2517, Val Cor=0.5326, Time=0.1597 sec\n",
      "Epoch [142 / 2000]: Train Loss=6.3548, Val Cor=0.5372, Time=0.1615 sec\n",
      "Epoch [143 / 2000]: Train Loss=6.5407, Val Cor=0.5316, Time=0.1612 sec\n",
      "Epoch [144 / 2000]: Train Loss=6.3967, Val Cor=0.5292, Time=0.1619 sec\n",
      "Epoch [145 / 2000]: Train Loss=6.0488, Val Cor=0.5244, Time=0.1622 sec\n",
      "Epoch [146 / 2000]: Train Loss=6.0877, Val Cor=0.5273, Time=0.1631 sec\n",
      "Epoch [147 / 2000]: Train Loss=6.3311, Val Cor=0.5376, Time=0.1620 sec\n",
      "Epoch [148 / 2000]: Train Loss=6.2898, Val Cor=0.5350, Time=0.1614 sec\n",
      "Epoch [149 / 2000]: Train Loss=6.1681, Val Cor=0.5349, Time=0.1608 sec\n",
      "Epoch [150 / 2000]: Train Loss=6.4272, Val Cor=0.5339, Time=0.1601 sec\n",
      "Epoch [151 / 2000]: Train Loss=6.1015, Val Cor=0.5331, Time=0.1605 sec\n",
      "Epoch [152 / 2000]: Train Loss=6.3144, Val Cor=0.5375, Time=0.1599 sec\n",
      "Epoch [153 / 2000]: Train Loss=6.0533, Val Cor=0.5366, Time=0.1599 sec\n",
      "Epoch [154 / 2000]: Train Loss=5.8814, Val Cor=0.5411, Time=0.1602 sec\n",
      "Epoch [155 / 2000]: Train Loss=5.8971, Val Cor=0.5356, Time=0.1601 sec\n",
      "Epoch [156 / 2000]: Train Loss=6.7528, Val Cor=0.5390, Time=0.1617 sec\n",
      "Epoch [157 / 2000]: Train Loss=6.3332, Val Cor=0.5413, Time=0.1617 sec\n",
      "Epoch [158 / 2000]: Train Loss=6.1997, Val Cor=0.5359, Time=0.1616 sec\n",
      "Epoch [159 / 2000]: Train Loss=6.1841, Val Cor=0.5397, Time=0.1615 sec\n",
      "Epoch [160 / 2000]: Train Loss=6.3200, Val Cor=0.5335, Time=0.1620 sec\n",
      "Epoch [161 / 2000]: Train Loss=6.3269, Val Cor=0.5363, Time=0.1612 sec\n",
      "Epoch [162 / 2000]: Train Loss=6.2738, Val Cor=0.5411, Time=0.1597 sec\n",
      "Epoch [163 / 2000]: Train Loss=6.1821, Val Cor=0.5422, Time=0.1592 sec\n",
      "Epoch [164 / 2000]: Train Loss=6.1031, Val Cor=0.5424, Time=0.1593 sec\n",
      "Epoch [165 / 2000]: Train Loss=6.5191, Val Cor=0.5357, Time=0.1595 sec\n",
      "Epoch [166 / 2000]: Train Loss=6.3394, Val Cor=0.5421, Time=0.1603 sec\n",
      "Epoch [167 / 2000]: Train Loss=6.3779, Val Cor=0.5455, Time=0.1598 sec\n",
      "Epoch [168 / 2000]: Train Loss=6.1998, Val Cor=0.5453, Time=0.1598 sec\n",
      "Epoch [169 / 2000]: Train Loss=6.0890, Val Cor=0.5463, Time=0.1600 sec\n",
      "Epoch [170 / 2000]: Train Loss=5.8565, Val Cor=0.5405, Time=0.1597 sec\n",
      "Epoch [171 / 2000]: Train Loss=5.9160, Val Cor=0.5423, Time=0.1612 sec\n",
      "Epoch [172 / 2000]: Train Loss=6.2634, Val Cor=0.5487, Time=0.1611 sec\n",
      "Epoch [173 / 2000]: Train Loss=6.3439, Val Cor=0.5379, Time=0.1606 sec\n",
      "Epoch [174 / 2000]: Train Loss=6.1537, Val Cor=0.5512, Time=0.1618 sec\n",
      "Epoch [175 / 2000]: Train Loss=5.9489, Val Cor=0.5440, Time=0.1612 sec\n",
      "Epoch [176 / 2000]: Train Loss=6.3955, Val Cor=0.5286, Time=0.1616 sec\n",
      "Epoch [177 / 2000]: Train Loss=6.1524, Val Cor=0.5484, Time=0.1606 sec\n",
      "Epoch [178 / 2000]: Train Loss=6.1303, Val Cor=0.5569, Time=0.1603 sec\n",
      "Epoch [179 / 2000]: Train Loss=6.3063, Val Cor=0.5387, Time=0.1593 sec\n",
      "Epoch [180 / 2000]: Train Loss=6.0973, Val Cor=0.5545, Time=0.1595 sec\n",
      "Epoch [181 / 2000]: Train Loss=6.0932, Val Cor=0.5558, Time=0.1601 sec\n",
      "Epoch [182 / 2000]: Train Loss=5.9122, Val Cor=0.5469, Time=0.1599 sec\n",
      "Epoch [183 / 2000]: Train Loss=6.0763, Val Cor=0.5612, Time=0.1596 sec\n",
      "Epoch [184 / 2000]: Train Loss=6.4021, Val Cor=0.5463, Time=0.1595 sec\n",
      "Epoch [185 / 2000]: Train Loss=6.1847, Val Cor=0.5457, Time=0.1611 sec\n",
      "Epoch [186 / 2000]: Train Loss=6.3529, Val Cor=0.5520, Time=0.1613 sec\n",
      "Epoch [187 / 2000]: Train Loss=6.2293, Val Cor=0.5497, Time=0.1615 sec\n",
      "Epoch [188 / 2000]: Train Loss=6.3510, Val Cor=0.5484, Time=0.1611 sec\n",
      "Epoch [189 / 2000]: Train Loss=6.0401, Val Cor=0.5493, Time=0.1601 sec\n",
      "Epoch [190 / 2000]: Train Loss=5.9333, Val Cor=0.5422, Time=0.1595 sec\n",
      "Epoch [191 / 2000]: Train Loss=6.3354, Val Cor=0.5508, Time=0.1593 sec\n",
      "Epoch [192 / 2000]: Train Loss=6.0573, Val Cor=0.5527, Time=0.1602 sec\n",
      "Epoch [193 / 2000]: Train Loss=5.8703, Val Cor=0.5584, Time=0.1599 sec\n",
      "Epoch [194 / 2000]: Train Loss=5.7818, Val Cor=0.5578, Time=0.1597 sec\n",
      "Epoch [195 / 2000]: Train Loss=6.0570, Val Cor=0.5569, Time=0.1615 sec\n",
      "Epoch [196 / 2000]: Train Loss=6.0543, Val Cor=0.5583, Time=0.1619 sec\n",
      "Epoch [197 / 2000]: Train Loss=6.0863, Val Cor=0.5634, Time=0.1613 sec\n",
      "Epoch [198 / 2000]: Train Loss=6.0720, Val Cor=0.5603, Time=0.1616 sec\n",
      "Epoch [199 / 2000]: Train Loss=6.1493, Val Cor=0.5608, Time=0.1620 sec\n",
      "Epoch [200 / 2000]: Train Loss=6.3169, Val Cor=0.5569, Time=0.1621 sec\n",
      "Epoch [201 / 2000]: Train Loss=6.3033, Val Cor=0.5597, Time=0.1608 sec\n",
      "Epoch [202 / 2000]: Train Loss=6.0336, Val Cor=0.5507, Time=0.1604 sec\n",
      "Epoch [203 / 2000]: Train Loss=5.9617, Val Cor=0.5515, Time=0.1602 sec\n",
      "Epoch [204 / 2000]: Train Loss=6.1170, Val Cor=0.5556, Time=0.1607 sec\n",
      "Epoch [205 / 2000]: Train Loss=6.1262, Val Cor=0.5565, Time=0.1618 sec\n",
      "Epoch [206 / 2000]: Train Loss=6.1766, Val Cor=0.5601, Time=0.1616 sec\n",
      "Epoch [207 / 2000]: Train Loss=6.0645, Val Cor=0.5535, Time=0.1615 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [208 / 2000]: Train Loss=6.0712, Val Cor=0.5555, Time=0.1620 sec\n",
      "Epoch [209 / 2000]: Train Loss=5.9840, Val Cor=0.5568, Time=0.1615 sec\n",
      "Epoch [210 / 2000]: Train Loss=6.1687, Val Cor=0.5549, Time=0.1601 sec\n",
      "Epoch [211 / 2000]: Train Loss=5.9788, Val Cor=0.5524, Time=0.1600 sec\n",
      "Epoch [212 / 2000]: Train Loss=6.1257, Val Cor=0.5547, Time=0.1594 sec\n",
      "Epoch [213 / 2000]: Train Loss=5.8718, Val Cor=0.5594, Time=0.1600 sec\n",
      "Epoch [214 / 2000]: Train Loss=6.1720, Val Cor=0.5533, Time=0.1597 sec\n",
      "Epoch [215 / 2000]: Train Loss=6.1826, Val Cor=0.5606, Time=0.1614 sec\n",
      "Epoch [216 / 2000]: Train Loss=6.0743, Val Cor=0.5584, Time=0.1612 sec\n",
      "Epoch [217 / 2000]: Train Loss=5.8798, Val Cor=0.5548, Time=0.1612 sec\n",
      "Epoch [218 / 2000]: Train Loss=6.0289, Val Cor=0.5640, Time=0.1620 sec\n",
      "Epoch [219 / 2000]: Train Loss=5.9295, Val Cor=0.5653, Time=0.1614 sec\n",
      "Epoch [220 / 2000]: Train Loss=6.2159, Val Cor=0.5686, Time=0.1617 sec\n",
      "Epoch [221 / 2000]: Train Loss=6.0935, Val Cor=0.5670, Time=0.1616 sec\n",
      "Epoch [222 / 2000]: Train Loss=5.9474, Val Cor=0.5687, Time=0.1616 sec\n",
      "Epoch [223 / 2000]: Train Loss=5.8550, Val Cor=0.5663, Time=0.1609 sec\n",
      "Epoch [224 / 2000]: Train Loss=5.7014, Val Cor=0.5614, Time=0.1601 sec\n",
      "Epoch [225 / 2000]: Train Loss=5.8913, Val Cor=0.5616, Time=0.1602 sec\n",
      "Epoch [226 / 2000]: Train Loss=5.9150, Val Cor=0.5531, Time=0.1600 sec\n",
      "Epoch [227 / 2000]: Train Loss=6.1942, Val Cor=0.5678, Time=0.1595 sec\n",
      "Epoch [228 / 2000]: Train Loss=6.0024, Val Cor=0.5631, Time=0.1600 sec\n",
      "Epoch [229 / 2000]: Train Loss=5.8984, Val Cor=0.5626, Time=0.1599 sec\n",
      "Epoch [230 / 2000]: Train Loss=6.1015, Val Cor=0.5542, Time=0.1614 sec\n",
      "Epoch [231 / 2000]: Train Loss=5.9600, Val Cor=0.5743, Time=0.1611 sec\n",
      "Epoch [232 / 2000]: Train Loss=5.9459, Val Cor=0.5697, Time=0.1610 sec\n",
      "Epoch [233 / 2000]: Train Loss=5.8880, Val Cor=0.5675, Time=0.1618 sec\n",
      "Epoch [234 / 2000]: Train Loss=5.7231, Val Cor=0.5755, Time=0.1614 sec\n",
      "Epoch [235 / 2000]: Train Loss=6.0674, Val Cor=0.5690, Time=0.1608 sec\n",
      "Epoch [236 / 2000]: Train Loss=5.7813, Val Cor=0.5658, Time=0.1600 sec\n",
      "Epoch [237 / 2000]: Train Loss=5.9757, Val Cor=0.5672, Time=0.1583 sec\n",
      "Epoch [238 / 2000]: Train Loss=5.9547, Val Cor=0.5666, Time=0.1583 sec\n",
      "Epoch [239 / 2000]: Train Loss=5.9027, Val Cor=0.5697, Time=0.1587 sec\n",
      "Epoch [240 / 2000]: Train Loss=6.1531, Val Cor=0.5695, Time=0.1590 sec\n",
      "Epoch [241 / 2000]: Train Loss=5.9455, Val Cor=0.5605, Time=0.1588 sec\n",
      "Epoch [242 / 2000]: Train Loss=5.8288, Val Cor=0.5612, Time=0.1610 sec\n",
      "Epoch [243 / 2000]: Train Loss=5.8275, Val Cor=0.5740, Time=0.1607 sec\n",
      "Epoch [244 / 2000]: Train Loss=5.8643, Val Cor=0.5708, Time=0.1601 sec\n",
      "Epoch [245 / 2000]: Train Loss=6.3841, Val Cor=0.5776, Time=0.1584 sec\n",
      "Epoch [246 / 2000]: Train Loss=6.0653, Val Cor=0.5734, Time=0.1583 sec\n",
      "Epoch [247 / 2000]: Train Loss=6.0195, Val Cor=0.5777, Time=0.1584 sec\n",
      "Epoch [248 / 2000]: Train Loss=6.1023, Val Cor=0.5718, Time=0.1599 sec\n",
      "Epoch [249 / 2000]: Train Loss=5.9489, Val Cor=0.5728, Time=0.1601 sec\n",
      "Epoch [250 / 2000]: Train Loss=5.9996, Val Cor=0.5751, Time=0.1601 sec\n",
      "Epoch [251 / 2000]: Train Loss=5.8041, Val Cor=0.5685, Time=0.1604 sec\n",
      "Epoch [252 / 2000]: Train Loss=5.8166, Val Cor=0.5717, Time=0.1619 sec\n",
      "Epoch [253 / 2000]: Train Loss=6.1433, Val Cor=0.5369, Time=0.1624 sec\n",
      "Epoch [254 / 2000]: Train Loss=6.2474, Val Cor=0.5743, Time=0.1622 sec\n",
      "Epoch [255 / 2000]: Train Loss=5.9793, Val Cor=0.5715, Time=0.1627 sec\n",
      "Epoch [256 / 2000]: Train Loss=5.9508, Val Cor=0.5758, Time=0.1620 sec\n",
      "Epoch [257 / 2000]: Train Loss=5.7508, Val Cor=0.5761, Time=0.1607 sec\n",
      "Epoch [258 / 2000]: Train Loss=6.0396, Val Cor=0.5746, Time=0.1605 sec\n",
      "Epoch [259 / 2000]: Train Loss=5.6793, Val Cor=0.5781, Time=0.1606 sec\n",
      "Epoch [260 / 2000]: Train Loss=6.1156, Val Cor=0.5744, Time=0.1608 sec\n",
      "Epoch [261 / 2000]: Train Loss=6.0856, Val Cor=0.5651, Time=0.1609 sec\n",
      "Epoch [262 / 2000]: Train Loss=5.7438, Val Cor=0.5770, Time=0.1607 sec\n",
      "Epoch [263 / 2000]: Train Loss=5.7591, Val Cor=0.5763, Time=0.1617 sec\n",
      "Epoch [264 / 2000]: Train Loss=5.8262, Val Cor=0.5764, Time=0.1615 sec\n",
      "Epoch [265 / 2000]: Train Loss=5.8763, Val Cor=0.5794, Time=0.1614 sec\n",
      "Epoch [266 / 2000]: Train Loss=5.8150, Val Cor=0.5675, Time=0.1616 sec\n",
      "Epoch [267 / 2000]: Train Loss=5.7642, Val Cor=0.5771, Time=0.1619 sec\n",
      "Epoch [268 / 2000]: Train Loss=6.0918, Val Cor=0.5701, Time=0.1624 sec\n",
      "Epoch [269 / 2000]: Train Loss=6.4521, Val Cor=0.5725, Time=0.1615 sec\n",
      "Epoch [270 / 2000]: Train Loss=5.8463, Val Cor=0.5837, Time=0.1604 sec\n",
      "Epoch [271 / 2000]: Train Loss=5.8894, Val Cor=0.5734, Time=0.1598 sec\n",
      "Epoch [272 / 2000]: Train Loss=6.5813, Val Cor=0.5735, Time=0.1600 sec\n",
      "Epoch [273 / 2000]: Train Loss=6.0010, Val Cor=0.5835, Time=0.1598 sec\n",
      "Epoch [274 / 2000]: Train Loss=5.8193, Val Cor=0.5780, Time=0.1598 sec\n",
      "Epoch [275 / 2000]: Train Loss=5.7828, Val Cor=0.5798, Time=0.1599 sec\n",
      "Epoch [276 / 2000]: Train Loss=6.0352, Val Cor=0.5766, Time=0.1602 sec\n",
      "Epoch [277 / 2000]: Train Loss=6.0653, Val Cor=0.5813, Time=0.1614 sec\n",
      "Epoch [278 / 2000]: Train Loss=6.3827, Val Cor=0.3777, Time=0.1617 sec\n",
      "Epoch [279 / 2000]: Train Loss=5.9014, Val Cor=0.5771, Time=0.1617 sec\n",
      "Epoch [280 / 2000]: Train Loss=5.8561, Val Cor=0.5857, Time=0.1621 sec\n",
      "Epoch [281 / 2000]: Train Loss=5.8207, Val Cor=0.5681, Time=0.1618 sec\n",
      "Epoch [282 / 2000]: Train Loss=5.9724, Val Cor=0.5859, Time=0.1613 sec\n",
      "Epoch [283 / 2000]: Train Loss=5.8510, Val Cor=0.5700, Time=0.1607 sec\n",
      "Epoch [284 / 2000]: Train Loss=5.6796, Val Cor=0.5692, Time=0.1602 sec\n",
      "Epoch [285 / 2000]: Train Loss=5.7748, Val Cor=0.5779, Time=0.1600 sec\n",
      "Epoch [286 / 2000]: Train Loss=5.5637, Val Cor=0.5906, Time=0.1603 sec\n",
      "Epoch [287 / 2000]: Train Loss=5.5875, Val Cor=0.5746, Time=0.1595 sec\n",
      "Epoch [288 / 2000]: Train Loss=5.7668, Val Cor=0.5738, Time=0.1598 sec\n",
      "Epoch [289 / 2000]: Train Loss=5.8895, Val Cor=0.5873, Time=0.1600 sec\n",
      "Epoch [290 / 2000]: Train Loss=5.8343, Val Cor=0.5794, Time=0.1614 sec\n",
      "Epoch [291 / 2000]: Train Loss=5.7721, Val Cor=0.5771, Time=0.1614 sec\n",
      "Epoch [292 / 2000]: Train Loss=5.6934, Val Cor=0.5671, Time=0.1610 sec\n",
      "Epoch [293 / 2000]: Train Loss=6.0057, Val Cor=0.5915, Time=0.1609 sec\n",
      "Epoch [294 / 2000]: Train Loss=5.8509, Val Cor=0.5622, Time=0.1610 sec\n",
      "Epoch [295 / 2000]: Train Loss=6.0041, Val Cor=0.5788, Time=0.1618 sec\n",
      "Epoch [296 / 2000]: Train Loss=5.9952, Val Cor=0.5115, Time=0.1619 sec\n",
      "Epoch [297 / 2000]: Train Loss=5.8056, Val Cor=0.5895, Time=0.1611 sec\n",
      "Epoch [298 / 2000]: Train Loss=5.6582, Val Cor=0.5917, Time=0.1598 sec\n",
      "Epoch [299 / 2000]: Train Loss=5.7720, Val Cor=0.5738, Time=0.1592 sec\n",
      "Epoch [300 / 2000]: Train Loss=5.8344, Val Cor=0.5735, Time=0.1602 sec\n",
      "Epoch [301 / 2000]: Train Loss=6.0819, Val Cor=0.5685, Time=0.1604 sec\n",
      "Epoch [302 / 2000]: Train Loss=5.9799, Val Cor=0.5884, Time=0.1600 sec\n",
      "Epoch [303 / 2000]: Train Loss=6.0039, Val Cor=0.5743, Time=0.1612 sec\n",
      "Epoch [304 / 2000]: Train Loss=5.7277, Val Cor=0.5961, Time=0.1619 sec\n",
      "Epoch [305 / 2000]: Train Loss=5.6296, Val Cor=0.5688, Time=0.1615 sec\n",
      "Epoch [306 / 2000]: Train Loss=6.0082, Val Cor=0.5810, Time=0.1617 sec\n",
      "Epoch [307 / 2000]: Train Loss=6.1548, Val Cor=0.5732, Time=0.1616 sec\n",
      "Epoch [308 / 2000]: Train Loss=6.0292, Val Cor=0.5866, Time=0.1614 sec\n",
      "Epoch [309 / 2000]: Train Loss=5.8253, Val Cor=0.5915, Time=0.1596 sec\n",
      "Epoch [310 / 2000]: Train Loss=5.7114, Val Cor=0.5890, Time=0.1598 sec\n",
      "Epoch [311 / 2000]: Train Loss=5.7833, Val Cor=0.5795, Time=0.1601 sec\n",
      "Epoch [312 / 2000]: Train Loss=5.7073, Val Cor=0.5822, Time=0.1588 sec\n",
      "Epoch [313 / 2000]: Train Loss=5.8285, Val Cor=0.5921, Time=0.1585 sec\n",
      "Epoch [314 / 2000]: Train Loss=6.0938, Val Cor=0.4639, Time=0.1592 sec\n",
      "Epoch [315 / 2000]: Train Loss=6.3317, Val Cor=0.5915, Time=0.1594 sec\n",
      "Epoch [316 / 2000]: Train Loss=6.0138, Val Cor=0.5985, Time=0.1604 sec\n",
      "Epoch [317 / 2000]: Train Loss=5.9601, Val Cor=0.5873, Time=0.1599 sec\n",
      "Epoch [318 / 2000]: Train Loss=5.9422, Val Cor=0.5732, Time=0.1605 sec\n",
      "Epoch [319 / 2000]: Train Loss=5.9878, Val Cor=0.5948, Time=0.1605 sec\n",
      "Epoch [320 / 2000]: Train Loss=5.7475, Val Cor=0.5921, Time=0.1599 sec\n",
      "Epoch [321 / 2000]: Train Loss=5.9425, Val Cor=0.5916, Time=0.1585 sec\n",
      "Epoch [322 / 2000]: Train Loss=6.1454, Val Cor=0.5770, Time=0.1586 sec\n",
      "Epoch [323 / 2000]: Train Loss=6.0878, Val Cor=0.4126, Time=0.1592 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [324 / 2000]: Train Loss=6.1003, Val Cor=0.5828, Time=0.1603 sec\n",
      "Epoch [325 / 2000]: Train Loss=5.9467, Val Cor=0.6020, Time=0.1602 sec\n",
      "Epoch [326 / 2000]: Train Loss=5.8408, Val Cor=0.4985, Time=0.1603 sec\n",
      "Epoch [327 / 2000]: Train Loss=5.7018, Val Cor=0.5717, Time=0.1610 sec\n",
      "Epoch [328 / 2000]: Train Loss=5.9591, Val Cor=0.5295, Time=0.1618 sec\n",
      "Epoch [329 / 2000]: Train Loss=5.7784, Val Cor=0.5600, Time=0.1618 sec\n",
      "Epoch [330 / 2000]: Train Loss=5.9748, Val Cor=0.6037, Time=0.1621 sec\n",
      "Epoch [331 / 2000]: Train Loss=6.0943, Val Cor=0.5122, Time=0.1616 sec\n",
      "Epoch [332 / 2000]: Train Loss=5.7324, Val Cor=0.5765, Time=0.1625 sec\n",
      "Epoch [333 / 2000]: Train Loss=5.8288, Val Cor=0.5855, Time=0.1616 sec\n",
      "Epoch [334 / 2000]: Train Loss=5.9823, Val Cor=0.5615, Time=0.1611 sec\n",
      "Epoch [335 / 2000]: Train Loss=5.7203, Val Cor=0.5919, Time=0.1604 sec\n",
      "Epoch [336 / 2000]: Train Loss=6.1824, Val Cor=0.6070, Time=0.1611 sec\n",
      "Epoch [337 / 2000]: Train Loss=5.8808, Val Cor=0.5590, Time=0.1599 sec\n",
      "Epoch [338 / 2000]: Train Loss=5.6700, Val Cor=0.6057, Time=0.1606 sec\n",
      "Epoch [339 / 2000]: Train Loss=5.6051, Val Cor=0.5873, Time=0.1602 sec\n",
      "Epoch [340 / 2000]: Train Loss=6.1094, Val Cor=0.6016, Time=0.1603 sec\n",
      "Epoch [341 / 2000]: Train Loss=5.7058, Val Cor=0.5952, Time=0.1614 sec\n",
      "Epoch [342 / 2000]: Train Loss=5.9591, Val Cor=0.5776, Time=0.1619 sec\n",
      "Epoch [343 / 2000]: Train Loss=5.9588, Val Cor=0.6050, Time=0.1615 sec\n",
      "Epoch [344 / 2000]: Train Loss=5.7000, Val Cor=0.6060, Time=0.1612 sec\n",
      "Epoch [345 / 2000]: Train Loss=5.7614, Val Cor=0.6053, Time=0.1617 sec\n",
      "Epoch [346 / 2000]: Train Loss=5.6657, Val Cor=0.5817, Time=0.1618 sec\n",
      "Epoch [347 / 2000]: Train Loss=5.6318, Val Cor=0.5999, Time=0.1608 sec\n",
      "Epoch [348 / 2000]: Train Loss=5.8503, Val Cor=0.5832, Time=0.1604 sec\n",
      "Epoch [349 / 2000]: Train Loss=5.6845, Val Cor=0.5944, Time=0.1593 sec\n",
      "Epoch [350 / 2000]: Train Loss=5.5844, Val Cor=0.5860, Time=0.1605 sec\n",
      "Epoch [351 / 2000]: Train Loss=5.6025, Val Cor=0.5937, Time=0.1605 sec\n",
      "Epoch [352 / 2000]: Train Loss=5.6922, Val Cor=0.6055, Time=0.1602 sec\n",
      "Epoch [353 / 2000]: Train Loss=5.7899, Val Cor=0.5998, Time=0.1600 sec\n",
      "Epoch [354 / 2000]: Train Loss=5.9148, Val Cor=0.4944, Time=0.1619 sec\n",
      "Epoch [355 / 2000]: Train Loss=6.4390, Val Cor=-0.4121, Time=0.1619 sec\n",
      "Epoch [356 / 2000]: Train Loss=6.1933, Val Cor=0.2427, Time=0.1622 sec\n",
      "Epoch [357 / 2000]: Train Loss=5.8475, Val Cor=0.6072, Time=0.1613 sec\n",
      "Epoch [358 / 2000]: Train Loss=5.5603, Val Cor=0.5699, Time=0.1611 sec\n",
      "Epoch [359 / 2000]: Train Loss=5.4596, Val Cor=0.3519, Time=0.1598 sec\n",
      "Epoch [360 / 2000]: Train Loss=6.2935, Val Cor=0.3350, Time=0.1599 sec\n",
      "Epoch [361 / 2000]: Train Loss=5.8297, Val Cor=0.5379, Time=0.1598 sec\n",
      "Epoch [362 / 2000]: Train Loss=5.8036, Val Cor=0.6118, Time=0.1589 sec\n",
      "Epoch [363 / 2000]: Train Loss=5.7996, Val Cor=0.5865, Time=0.1585 sec\n",
      "Epoch [364 / 2000]: Train Loss=5.6912, Val Cor=0.5164, Time=0.1592 sec\n",
      "Epoch [365 / 2000]: Train Loss=5.9588, Val Cor=0.4362, Time=0.1600 sec\n",
      "Epoch [366 / 2000]: Train Loss=5.8682, Val Cor=0.5886, Time=0.1604 sec\n",
      "Epoch [367 / 2000]: Train Loss=5.8553, Val Cor=0.5845, Time=0.1599 sec\n",
      "Epoch [368 / 2000]: Train Loss=5.9257, Val Cor=0.6092, Time=0.1603 sec\n",
      "Epoch [369 / 2000]: Train Loss=5.6139, Val Cor=0.5898, Time=0.1601 sec\n",
      "Epoch [370 / 2000]: Train Loss=5.6021, Val Cor=0.5943, Time=0.1584 sec\n",
      "Epoch [371 / 2000]: Train Loss=5.7836, Val Cor=0.5822, Time=0.1585 sec\n",
      "Epoch [372 / 2000]: Train Loss=5.6532, Val Cor=0.6059, Time=0.1586 sec\n",
      "Epoch [373 / 2000]: Train Loss=5.7465, Val Cor=0.5998, Time=0.1597 sec\n",
      "Epoch [374 / 2000]: Train Loss=5.5580, Val Cor=0.6000, Time=0.1616 sec\n",
      "Epoch [375 / 2000]: Train Loss=5.7737, Val Cor=0.6068, Time=0.1617 sec\n",
      "Epoch [376 / 2000]: Train Loss=5.8282, Val Cor=0.6042, Time=0.1618 sec\n",
      "Epoch [377 / 2000]: Train Loss=5.6640, Val Cor=0.5894, Time=0.1623 sec\n",
      "Epoch [378 / 2000]: Train Loss=5.8945, Val Cor=0.5948, Time=0.1624 sec\n",
      "Epoch [379 / 2000]: Train Loss=5.7553, Val Cor=0.6105, Time=0.1622 sec\n",
      "Epoch [380 / 2000]: Train Loss=5.5412, Val Cor=0.6105, Time=0.1614 sec\n",
      "Epoch [381 / 2000]: Train Loss=5.7891, Val Cor=0.6047, Time=0.1608 sec\n",
      "Epoch [382 / 2000]: Train Loss=6.0602, Val Cor=0.6094, Time=0.1608 sec\n",
      "Epoch [383 / 2000]: Train Loss=5.5684, Val Cor=0.5345, Time=0.1608 sec\n",
      "Epoch [384 / 2000]: Train Loss=5.6944, Val Cor=0.6169, Time=0.1611 sec\n",
      "Epoch [385 / 2000]: Train Loss=5.4081, Val Cor=0.5882, Time=0.1605 sec\n",
      "Epoch [386 / 2000]: Train Loss=5.8823, Val Cor=0.6048, Time=0.1622 sec\n",
      "Epoch [387 / 2000]: Train Loss=5.6096, Val Cor=0.5733, Time=0.1622 sec\n",
      "Epoch [388 / 2000]: Train Loss=5.5720, Val Cor=0.5737, Time=0.1607 sec\n",
      "Epoch [389 / 2000]: Train Loss=5.5974, Val Cor=0.6026, Time=0.1606 sec\n",
      "Epoch [390 / 2000]: Train Loss=5.6521, Val Cor=0.5994, Time=0.1610 sec\n",
      "Epoch [391 / 2000]: Train Loss=5.4525, Val Cor=0.5975, Time=0.1611 sec\n",
      "Epoch [392 / 2000]: Train Loss=5.6644, Val Cor=0.6088, Time=0.1603 sec\n",
      "Epoch [393 / 2000]: Train Loss=5.5481, Val Cor=0.5999, Time=0.1603 sec\n",
      "Epoch [394 / 2000]: Train Loss=5.5029, Val Cor=0.6070, Time=0.1619 sec\n",
      "Epoch [395 / 2000]: Train Loss=6.0854, Val Cor=-0.0182, Time=0.1618 sec\n",
      "Epoch [396 / 2000]: Train Loss=5.7427, Val Cor=0.5974, Time=0.1622 sec\n",
      "Epoch [397 / 2000]: Train Loss=5.6599, Val Cor=0.5363, Time=0.1620 sec\n",
      "Epoch [398 / 2000]: Train Loss=5.6213, Val Cor=0.5991, Time=0.1619 sec\n",
      "Epoch [399 / 2000]: Train Loss=5.5440, Val Cor=0.6209, Time=0.1605 sec\n",
      "Epoch [400 / 2000]: Train Loss=5.5775, Val Cor=0.6135, Time=0.1610 sec\n",
      "Epoch [401 / 2000]: Train Loss=5.6910, Val Cor=0.6199, Time=0.1601 sec\n",
      "Epoch [402 / 2000]: Train Loss=5.7378, Val Cor=0.6129, Time=0.1594 sec\n",
      "Epoch [403 / 2000]: Train Loss=5.6009, Val Cor=0.5717, Time=0.1599 sec\n",
      "Epoch [404 / 2000]: Train Loss=5.5786, Val Cor=0.6150, Time=0.1598 sec\n",
      "Epoch [405 / 2000]: Train Loss=5.5444, Val Cor=0.6114, Time=0.1599 sec\n",
      "Epoch [406 / 2000]: Train Loss=5.5956, Val Cor=0.6105, Time=0.1599 sec\n",
      "Epoch [407 / 2000]: Train Loss=5.7275, Val Cor=0.6092, Time=0.1602 sec\n",
      "Epoch [408 / 2000]: Train Loss=5.4683, Val Cor=0.6074, Time=0.1612 sec\n",
      "Epoch [409 / 2000]: Train Loss=5.6464, Val Cor=0.6084, Time=0.1614 sec\n",
      "Epoch [410 / 2000]: Train Loss=5.6803, Val Cor=0.5431, Time=0.1611 sec\n",
      "Epoch [411 / 2000]: Train Loss=5.6840, Val Cor=0.6135, Time=0.1617 sec\n",
      "Epoch [412 / 2000]: Train Loss=5.8348, Val Cor=0.5790, Time=0.1620 sec\n",
      "Epoch [413 / 2000]: Train Loss=5.5848, Val Cor=0.6016, Time=0.1617 sec\n",
      "Epoch [414 / 2000]: Train Loss=5.5260, Val Cor=0.6178, Time=0.1602 sec\n",
      "Epoch [415 / 2000]: Train Loss=5.8824, Val Cor=0.2501, Time=0.1599 sec\n",
      "Epoch [416 / 2000]: Train Loss=6.0521, Val Cor=0.6152, Time=0.1601 sec\n",
      "Epoch [417 / 2000]: Train Loss=5.7582, Val Cor=0.6015, Time=0.1596 sec\n",
      "Epoch [418 / 2000]: Train Loss=5.7135, Val Cor=0.6322, Time=0.1599 sec\n",
      "Epoch [419 / 2000]: Train Loss=5.6963, Val Cor=0.6219, Time=0.1596 sec\n",
      "Epoch [420 / 2000]: Train Loss=5.6751, Val Cor=0.6361, Time=0.1613 sec\n",
      "Epoch [421 / 2000]: Train Loss=5.7488, Val Cor=0.6172, Time=0.1611 sec\n",
      "Epoch [422 / 2000]: Train Loss=5.8334, Val Cor=0.3950, Time=0.1614 sec\n",
      "Epoch [423 / 2000]: Train Loss=5.8791, Val Cor=0.6152, Time=0.1608 sec\n",
      "Epoch [424 / 2000]: Train Loss=5.5468, Val Cor=0.6009, Time=0.1619 sec\n",
      "Epoch [425 / 2000]: Train Loss=5.6835, Val Cor=0.5983, Time=0.1615 sec\n",
      "Epoch [426 / 2000]: Train Loss=5.5812, Val Cor=0.6192, Time=0.1611 sec\n",
      "Epoch [427 / 2000]: Train Loss=5.5069, Val Cor=0.5324, Time=0.1598 sec\n",
      "Epoch [428 / 2000]: Train Loss=6.1142, Val Cor=0.6186, Time=0.1594 sec\n",
      "Epoch [429 / 2000]: Train Loss=6.0189, Val Cor=0.5872, Time=0.1596 sec\n",
      "Epoch [430 / 2000]: Train Loss=6.0523, Val Cor=0.6138, Time=0.1605 sec\n",
      "Epoch [431 / 2000]: Train Loss=5.6210, Val Cor=0.6062, Time=0.1604 sec\n",
      "Epoch [432 / 2000]: Train Loss=5.7759, Val Cor=0.4844, Time=0.1620 sec\n",
      "Epoch [433 / 2000]: Train Loss=5.6348, Val Cor=0.5555, Time=0.1614 sec\n",
      "Epoch [434 / 2000]: Train Loss=5.9017, Val Cor=0.5677, Time=0.1622 sec\n",
      "Epoch [435 / 2000]: Train Loss=5.6629, Val Cor=0.5691, Time=0.1622 sec\n",
      "Epoch [436 / 2000]: Train Loss=5.7838, Val Cor=0.4986, Time=0.1622 sec\n",
      "Epoch [437 / 2000]: Train Loss=5.6352, Val Cor=0.6138, Time=0.1613 sec\n",
      "Epoch [438 / 2000]: Train Loss=5.5814, Val Cor=0.6096, Time=0.1602 sec\n",
      "Epoch [439 / 2000]: Train Loss=5.7258, Val Cor=0.5644, Time=0.1602 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [440 / 2000]: Train Loss=5.6475, Val Cor=0.6063, Time=0.1602 sec\n",
      "Epoch [441 / 2000]: Train Loss=5.5360, Val Cor=0.5444, Time=0.1603 sec\n",
      "Epoch [442 / 2000]: Train Loss=5.5808, Val Cor=0.4508, Time=0.1604 sec\n",
      "Epoch [443 / 2000]: Train Loss=5.6803, Val Cor=0.6198, Time=0.1609 sec\n",
      "Epoch [444 / 2000]: Train Loss=5.6535, Val Cor=0.6225, Time=0.1612 sec\n",
      "Epoch [445 / 2000]: Train Loss=5.6364, Val Cor=0.5809, Time=0.1609 sec\n",
      "Epoch [446 / 2000]: Train Loss=5.6488, Val Cor=0.6116, Time=0.1615 sec\n",
      "Epoch [447 / 2000]: Train Loss=5.6269, Val Cor=0.5966, Time=0.1615 sec\n",
      "Epoch [448 / 2000]: Train Loss=5.8953, Val Cor=0.5796, Time=0.1611 sec\n",
      "Epoch [449 / 2000]: Train Loss=5.5530, Val Cor=0.6232, Time=0.1598 sec\n",
      "Epoch [450 / 2000]: Train Loss=5.5018, Val Cor=0.6087, Time=0.1598 sec\n",
      "Epoch [451 / 2000]: Train Loss=5.5279, Val Cor=0.5951, Time=0.1599 sec\n",
      "Epoch [452 / 2000]: Train Loss=5.3743, Val Cor=0.6195, Time=0.1597 sec\n",
      "Epoch [453 / 2000]: Train Loss=5.7930, Val Cor=0.6049, Time=0.1599 sec\n",
      "Epoch [454 / 2000]: Train Loss=5.5709, Val Cor=0.6006, Time=0.1604 sec\n",
      "Epoch [455 / 2000]: Train Loss=5.4274, Val Cor=0.6327, Time=0.1615 sec\n",
      "Epoch [456 / 2000]: Train Loss=5.6501, Val Cor=0.6283, Time=0.1615 sec\n",
      "Epoch [457 / 2000]: Train Loss=5.4332, Val Cor=0.6225, Time=0.1618 sec\n",
      "Epoch [458 / 2000]: Train Loss=5.5066, Val Cor=0.6361, Time=0.1629 sec\n",
      "Epoch [459 / 2000]: Train Loss=5.7243, Val Cor=0.6277, Time=0.1628 sec\n",
      "Epoch [460 / 2000]: Train Loss=5.7243, Val Cor=0.5958, Time=0.1613 sec\n",
      "Epoch [461 / 2000]: Train Loss=5.5747, Val Cor=0.6141, Time=0.1628 sec\n",
      "Epoch [462 / 2000]: Train Loss=5.5560, Val Cor=0.6029, Time=0.1638 sec\n",
      "Epoch [463 / 2000]: Train Loss=5.5810, Val Cor=0.5266, Time=0.1620 sec\n",
      "Epoch [464 / 2000]: Train Loss=5.8086, Val Cor=0.5581, Time=0.1621 sec\n",
      "Epoch [465 / 2000]: Train Loss=5.8369, Val Cor=0.5828, Time=0.1610 sec\n",
      "Epoch [466 / 2000]: Train Loss=5.7821, Val Cor=0.6226, Time=0.1602 sec\n",
      "Epoch [467 / 2000]: Train Loss=5.6048, Val Cor=0.6246, Time=0.1595 sec\n",
      "Epoch [468 / 2000]: Train Loss=5.5225, Val Cor=0.6225, Time=0.1601 sec\n",
      "Epoch [469 / 2000]: Train Loss=5.5415, Val Cor=0.6246, Time=0.1601 sec\n",
      "Epoch [470 / 2000]: Train Loss=5.4127, Val Cor=0.6225, Time=0.1600 sec\n",
      "Epoch [471 / 2000]: Train Loss=5.6083, Val Cor=0.6285, Time=0.1613 sec\n",
      "Epoch [472 / 2000]: Train Loss=5.7049, Val Cor=0.6151, Time=0.1611 sec\n",
      "Epoch [473 / 2000]: Train Loss=5.5491, Val Cor=0.6118, Time=0.1608 sec\n",
      "Epoch [474 / 2000]: Train Loss=5.5383, Val Cor=0.5492, Time=0.1616 sec\n",
      "Epoch [475 / 2000]: Train Loss=5.5249, Val Cor=0.6294, Time=0.1611 sec\n",
      "Epoch [476 / 2000]: Train Loss=5.6808, Val Cor=0.6112, Time=0.1597 sec\n",
      "Epoch [477 / 2000]: Train Loss=5.5453, Val Cor=0.6316, Time=0.1592 sec\n",
      "Epoch [478 / 2000]: Train Loss=5.4305, Val Cor=0.6279, Time=0.1598 sec\n",
      "Epoch [479 / 2000]: Train Loss=5.5925, Val Cor=0.5942, Time=0.1595 sec\n",
      "Epoch [480 / 2000]: Train Loss=5.6943, Val Cor=0.6183, Time=0.1603 sec\n",
      "Epoch [481 / 2000]: Train Loss=5.7550, Val Cor=0.6242, Time=0.1615 sec\n",
      "Epoch [482 / 2000]: Train Loss=5.5692, Val Cor=0.6374, Time=0.1612 sec\n",
      "Epoch [483 / 2000]: Train Loss=5.6375, Val Cor=0.6063, Time=0.1612 sec\n",
      "Epoch [484 / 2000]: Train Loss=5.5283, Val Cor=0.6319, Time=0.1624 sec\n",
      "Epoch [485 / 2000]: Train Loss=5.5460, Val Cor=0.6270, Time=0.1614 sec\n",
      "Epoch [486 / 2000]: Train Loss=5.7435, Val Cor=0.6259, Time=0.1599 sec\n",
      "Epoch [487 / 2000]: Train Loss=5.5972, Val Cor=0.5718, Time=0.1595 sec\n",
      "Epoch [488 / 2000]: Train Loss=5.5387, Val Cor=0.6263, Time=0.1602 sec\n",
      "Epoch [489 / 2000]: Train Loss=5.2748, Val Cor=0.6293, Time=0.1599 sec\n",
      "Epoch [490 / 2000]: Train Loss=5.4849, Val Cor=0.6244, Time=0.1601 sec\n",
      "Epoch [491 / 2000]: Train Loss=5.4181, Val Cor=0.6308, Time=0.1614 sec\n",
      "Epoch [492 / 2000]: Train Loss=5.5771, Val Cor=0.6077, Time=0.1612 sec\n",
      "Epoch [493 / 2000]: Train Loss=5.3221, Val Cor=0.6215, Time=0.1613 sec\n",
      "Epoch [494 / 2000]: Train Loss=5.6328, Val Cor=0.5352, Time=0.1614 sec\n",
      "Epoch [495 / 2000]: Train Loss=5.4081, Val Cor=0.6309, Time=0.1597 sec\n",
      "Epoch [496 / 2000]: Train Loss=5.9637, Val Cor=0.6337, Time=0.1606 sec\n",
      "Epoch [497 / 2000]: Train Loss=5.5239, Val Cor=0.6323, Time=0.1607 sec\n",
      "Epoch [498 / 2000]: Train Loss=5.5428, Val Cor=0.6352, Time=0.1617 sec\n",
      "Epoch [499 / 2000]: Train Loss=5.6054, Val Cor=0.6098, Time=0.1612 sec\n",
      "Epoch [500 / 2000]: Train Loss=5.4828, Val Cor=0.6262, Time=0.1604 sec\n",
      "Epoch [501 / 2000]: Train Loss=5.4254, Val Cor=0.6410, Time=0.1594 sec\n",
      "Epoch [502 / 2000]: Train Loss=5.6599, Val Cor=0.6308, Time=0.1591 sec\n",
      "Epoch [503 / 2000]: Train Loss=5.3928, Val Cor=0.6309, Time=0.1593 sec\n",
      "Epoch [504 / 2000]: Train Loss=5.2550, Val Cor=0.6314, Time=0.1590 sec\n",
      "Epoch [505 / 2000]: Train Loss=5.9594, Val Cor=0.6294, Time=0.1585 sec\n",
      "Epoch [506 / 2000]: Train Loss=5.5260, Val Cor=0.6431, Time=0.1591 sec\n",
      "Epoch [507 / 2000]: Train Loss=5.5930, Val Cor=0.6146, Time=0.1582 sec\n",
      "Epoch [508 / 2000]: Train Loss=5.6604, Val Cor=0.6303, Time=0.1604 sec\n",
      "Epoch [509 / 2000]: Train Loss=5.6133, Val Cor=0.5401, Time=0.1601 sec\n",
      "Epoch [510 / 2000]: Train Loss=5.3436, Val Cor=0.6269, Time=0.1596 sec\n",
      "Epoch [511 / 2000]: Train Loss=5.7149, Val Cor=0.6282, Time=0.1608 sec\n",
      "Epoch [512 / 2000]: Train Loss=5.4140, Val Cor=0.6191, Time=0.1603 sec\n",
      "Epoch [513 / 2000]: Train Loss=5.5776, Val Cor=0.5924, Time=0.1605 sec\n",
      "Epoch [514 / 2000]: Train Loss=5.4508, Val Cor=0.6398, Time=0.1606 sec\n",
      "Epoch [515 / 2000]: Train Loss=5.5661, Val Cor=0.6347, Time=0.1599 sec\n",
      "Epoch [516 / 2000]: Train Loss=5.3880, Val Cor=0.6393, Time=0.1600 sec\n",
      "Epoch [517 / 2000]: Train Loss=5.6347, Val Cor=0.5914, Time=0.1601 sec\n",
      "Epoch [518 / 2000]: Train Loss=5.5128, Val Cor=0.6306, Time=0.1604 sec\n",
      "Epoch [519 / 2000]: Train Loss=5.3908, Val Cor=0.6042, Time=0.1601 sec\n",
      "Epoch [520 / 2000]: Train Loss=5.4342, Val Cor=0.6346, Time=0.1613 sec\n",
      "Epoch [521 / 2000]: Train Loss=5.5144, Val Cor=0.6338, Time=0.1616 sec\n",
      "Epoch [522 / 2000]: Train Loss=5.3873, Val Cor=0.6078, Time=0.1616 sec\n",
      "Epoch [523 / 2000]: Train Loss=5.6655, Val Cor=0.6297, Time=0.1622 sec\n",
      "Epoch [524 / 2000]: Train Loss=5.5370, Val Cor=0.6347, Time=0.1622 sec\n",
      "Epoch [525 / 2000]: Train Loss=5.5212, Val Cor=0.5772, Time=0.1606 sec\n",
      "Epoch [526 / 2000]: Train Loss=5.3342, Val Cor=0.6407, Time=0.1610 sec\n",
      "Epoch [527 / 2000]: Train Loss=5.3802, Val Cor=0.6439, Time=0.1606 sec\n",
      "Epoch [528 / 2000]: Train Loss=5.3356, Val Cor=0.6253, Time=0.1605 sec\n",
      "Epoch [529 / 2000]: Train Loss=5.5145, Val Cor=0.6377, Time=0.1604 sec\n",
      "Epoch [530 / 2000]: Train Loss=5.5945, Val Cor=0.6323, Time=0.1607 sec\n",
      "Epoch [531 / 2000]: Train Loss=5.4508, Val Cor=0.6094, Time=0.1607 sec\n",
      "Epoch [532 / 2000]: Train Loss=5.5322, Val Cor=0.6504, Time=0.1618 sec\n",
      "Epoch [533 / 2000]: Train Loss=5.4876, Val Cor=0.6456, Time=0.1615 sec\n",
      "Epoch [534 / 2000]: Train Loss=5.4474, Val Cor=0.6360, Time=0.1617 sec\n",
      "Epoch [535 / 2000]: Train Loss=5.6067, Val Cor=0.6257, Time=0.1618 sec\n",
      "Epoch [536 / 2000]: Train Loss=5.4389, Val Cor=0.6454, Time=0.1623 sec\n",
      "Epoch [537 / 2000]: Train Loss=5.4297, Val Cor=0.6452, Time=0.1612 sec\n",
      "Epoch [538 / 2000]: Train Loss=5.4044, Val Cor=0.6201, Time=0.1597 sec\n",
      "Epoch [539 / 2000]: Train Loss=5.4893, Val Cor=0.5827, Time=0.1594 sec\n",
      "Epoch [540 / 2000]: Train Loss=5.6652, Val Cor=0.6085, Time=0.1599 sec\n",
      "Epoch [541 / 2000]: Train Loss=5.3490, Val Cor=0.6383, Time=0.1599 sec\n",
      "Epoch [542 / 2000]: Train Loss=5.4007, Val Cor=0.6241, Time=0.1612 sec\n",
      "Epoch [543 / 2000]: Train Loss=5.6053, Val Cor=0.6106, Time=0.1614 sec\n",
      "Epoch [544 / 2000]: Train Loss=5.5529, Val Cor=0.6358, Time=0.1618 sec\n",
      "Epoch [545 / 2000]: Train Loss=5.2913, Val Cor=0.6504, Time=0.1615 sec\n",
      "Epoch [546 / 2000]: Train Loss=5.4355, Val Cor=0.6268, Time=0.1620 sec\n",
      "Epoch [547 / 2000]: Train Loss=5.4246, Val Cor=0.6269, Time=0.1613 sec\n",
      "Epoch [548 / 2000]: Train Loss=5.6040, Val Cor=0.6358, Time=0.1594 sec\n",
      "Epoch [549 / 2000]: Train Loss=5.5779, Val Cor=0.6353, Time=0.1595 sec\n",
      "Epoch [550 / 2000]: Train Loss=5.4374, Val Cor=0.5032, Time=0.1599 sec\n",
      "Epoch [551 / 2000]: Train Loss=5.5677, Val Cor=0.6361, Time=0.1602 sec\n",
      "Epoch [552 / 2000]: Train Loss=5.3810, Val Cor=0.6069, Time=0.1598 sec\n",
      "Epoch [553 / 2000]: Train Loss=5.4085, Val Cor=0.5962, Time=0.1613 sec\n",
      "Epoch [554 / 2000]: Train Loss=5.3572, Val Cor=0.6444, Time=0.1614 sec\n",
      "Epoch [555 / 2000]: Train Loss=5.5142, Val Cor=0.6380, Time=0.1609 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [556 / 2000]: Train Loss=5.7771, Val Cor=0.6473, Time=0.1614 sec\n",
      "Epoch [557 / 2000]: Train Loss=5.2019, Val Cor=0.6372, Time=0.1610 sec\n",
      "Epoch [558 / 2000]: Train Loss=5.7203, Val Cor=0.6203, Time=0.1600 sec\n",
      "Epoch [559 / 2000]: Train Loss=5.5572, Val Cor=0.5873, Time=0.1593 sec\n",
      "Epoch [560 / 2000]: Train Loss=5.3028, Val Cor=0.6204, Time=0.1600 sec\n",
      "Epoch [561 / 2000]: Train Loss=5.3327, Val Cor=0.6177, Time=0.1600 sec\n",
      "Epoch [562 / 2000]: Train Loss=5.7053, Val Cor=0.6380, Time=0.1602 sec\n",
      "Epoch [563 / 2000]: Train Loss=6.2564, Val Cor=0.6310, Time=0.1614 sec\n",
      "Epoch [564 / 2000]: Train Loss=5.7038, Val Cor=0.6024, Time=0.1613 sec\n",
      "Epoch [565 / 2000]: Train Loss=5.5903, Val Cor=0.6522, Time=0.1613 sec\n",
      "Epoch [566 / 2000]: Train Loss=5.9242, Val Cor=0.6294, Time=0.1611 sec\n",
      "Epoch [567 / 2000]: Train Loss=5.6999, Val Cor=0.6177, Time=0.1621 sec\n",
      "Epoch [568 / 2000]: Train Loss=5.5602, Val Cor=0.6368, Time=0.1621 sec\n",
      "Epoch [569 / 2000]: Train Loss=5.4658, Val Cor=0.6354, Time=0.1612 sec\n",
      "Epoch [570 / 2000]: Train Loss=5.4709, Val Cor=0.6314, Time=0.1602 sec\n",
      "Epoch [571 / 2000]: Train Loss=5.5854, Val Cor=0.3839, Time=0.1601 sec\n",
      "Epoch [572 / 2000]: Train Loss=5.3100, Val Cor=0.6458, Time=0.1600 sec\n",
      "Epoch [573 / 2000]: Train Loss=5.4055, Val Cor=0.6233, Time=0.1600 sec\n",
      "Epoch [574 / 2000]: Train Loss=5.4618, Val Cor=0.6209, Time=0.1616 sec\n",
      "Epoch [575 / 2000]: Train Loss=5.5998, Val Cor=0.6376, Time=0.1611 sec\n",
      "Epoch [576 / 2000]: Train Loss=5.4843, Val Cor=0.6511, Time=0.1616 sec\n",
      "Epoch [577 / 2000]: Train Loss=5.5565, Val Cor=0.6184, Time=0.1610 sec\n",
      "Epoch [578 / 2000]: Train Loss=5.5201, Val Cor=0.6406, Time=0.1618 sec\n",
      "Epoch [579 / 2000]: Train Loss=5.3337, Val Cor=0.4152, Time=0.1613 sec\n",
      "Epoch [580 / 2000]: Train Loss=6.2463, Val Cor=0.6438, Time=0.1597 sec\n",
      "Epoch [581 / 2000]: Train Loss=5.7016, Val Cor=0.6407, Time=0.1592 sec\n",
      "Epoch [582 / 2000]: Train Loss=5.3952, Val Cor=0.6231, Time=0.1585 sec\n",
      "Epoch [583 / 2000]: Train Loss=5.7394, Val Cor=0.4829, Time=0.1588 sec\n",
      "Epoch [584 / 2000]: Train Loss=5.6907, Val Cor=0.3931, Time=0.1589 sec\n",
      "Epoch [585 / 2000]: Train Loss=5.6330, Val Cor=0.6366, Time=0.1589 sec\n",
      "Epoch [586 / 2000]: Train Loss=5.3140, Val Cor=0.6024, Time=0.1591 sec\n",
      "Epoch [587 / 2000]: Train Loss=5.3616, Val Cor=0.6229, Time=0.1603 sec\n",
      "Epoch [588 / 2000]: Train Loss=5.3606, Val Cor=0.6420, Time=0.1601 sec\n",
      "Epoch [589 / 2000]: Train Loss=5.2600, Val Cor=0.6436, Time=0.1607 sec\n",
      "Epoch [590 / 2000]: Train Loss=5.3608, Val Cor=0.6267, Time=0.1600 sec\n",
      "Epoch [591 / 2000]: Train Loss=5.3964, Val Cor=0.5994, Time=0.1590 sec\n",
      "Epoch [592 / 2000]: Train Loss=5.5372, Val Cor=0.6129, Time=0.1586 sec\n",
      "Epoch [593 / 2000]: Train Loss=5.4441, Val Cor=0.6360, Time=0.1593 sec\n",
      "Epoch [594 / 2000]: Train Loss=5.6235, Val Cor=0.5973, Time=0.1596 sec\n",
      "Epoch [595 / 2000]: Train Loss=5.3778, Val Cor=0.6417, Time=0.1599 sec\n",
      "Epoch [596 / 2000]: Train Loss=5.1893, Val Cor=0.6449, Time=0.1597 sec\n",
      "Epoch [597 / 2000]: Train Loss=5.1779, Val Cor=0.6494, Time=0.1601 sec\n",
      "Epoch [598 / 2000]: Train Loss=5.5797, Val Cor=-0.3349, Time=0.1612 sec\n",
      "Epoch [599 / 2000]: Train Loss=6.4737, Val Cor=0.6494, Time=0.1624 sec\n",
      "Epoch [600 / 2000]: Train Loss=5.4634, Val Cor=-0.2988, Time=0.1620 sec\n",
      "Epoch [601 / 2000]: Train Loss=5.3512, Val Cor=0.6447, Time=0.1623 sec\n",
      "Epoch [602 / 2000]: Train Loss=5.4266, Val Cor=0.6421, Time=0.1626 sec\n",
      "Epoch [603 / 2000]: Train Loss=5.2562, Val Cor=0.6444, Time=0.1621 sec\n",
      "Epoch [604 / 2000]: Train Loss=5.1654, Val Cor=0.6585, Time=0.1614 sec\n",
      "Epoch [605 / 2000]: Train Loss=5.6463, Val Cor=0.5714, Time=0.1615 sec\n",
      "Epoch [606 / 2000]: Train Loss=5.2899, Val Cor=0.4788, Time=0.1625 sec\n",
      "Epoch [607 / 2000]: Train Loss=5.3777, Val Cor=0.6119, Time=0.1597 sec\n",
      "Epoch [608 / 2000]: Train Loss=5.8489, Val Cor=0.6415, Time=0.1602 sec\n",
      "Epoch [609 / 2000]: Train Loss=5.2465, Val Cor=0.6439, Time=0.1599 sec\n",
      "Epoch [610 / 2000]: Train Loss=5.6881, Val Cor=0.6381, Time=0.1606 sec\n",
      "Epoch [611 / 2000]: Train Loss=5.3586, Val Cor=0.6353, Time=0.1601 sec\n",
      "Epoch [612 / 2000]: Train Loss=5.3186, Val Cor=0.6335, Time=0.1605 sec\n",
      "Epoch [613 / 2000]: Train Loss=5.7477, Val Cor=0.6443, Time=0.1608 sec\n",
      "Epoch [614 / 2000]: Train Loss=5.2291, Val Cor=0.6212, Time=0.1617 sec\n",
      "Epoch [615 / 2000]: Train Loss=5.4543, Val Cor=0.6507, Time=0.1615 sec\n",
      "Epoch [616 / 2000]: Train Loss=5.2105, Val Cor=0.6642, Time=0.1614 sec\n",
      "Epoch [617 / 2000]: Train Loss=5.3104, Val Cor=0.6585, Time=0.1609 sec\n",
      "Epoch [618 / 2000]: Train Loss=5.4047, Val Cor=0.6677, Time=0.1625 sec\n",
      "Epoch [619 / 2000]: Train Loss=5.7095, Val Cor=0.5468, Time=0.1623 sec\n",
      "Epoch [620 / 2000]: Train Loss=5.7322, Val Cor=0.6487, Time=0.1622 sec\n",
      "Epoch [621 / 2000]: Train Loss=5.5516, Val Cor=0.6041, Time=0.1613 sec\n",
      "Epoch [622 / 2000]: Train Loss=5.7217, Val Cor=0.6044, Time=0.1604 sec\n",
      "Epoch [623 / 2000]: Train Loss=5.7298, Val Cor=0.6357, Time=0.1605 sec\n",
      "Epoch [624 / 2000]: Train Loss=5.4727, Val Cor=0.6642, Time=0.1605 sec\n",
      "Epoch [625 / 2000]: Train Loss=5.4970, Val Cor=0.6180, Time=0.1601 sec\n",
      "Epoch [626 / 2000]: Train Loss=5.5607, Val Cor=0.6482, Time=0.1606 sec\n",
      "Epoch [627 / 2000]: Train Loss=5.5417, Val Cor=0.6585, Time=0.1616 sec\n",
      "Epoch [628 / 2000]: Train Loss=5.5349, Val Cor=0.4981, Time=0.1614 sec\n",
      "Epoch [629 / 2000]: Train Loss=5.1908, Val Cor=0.6644, Time=0.1611 sec\n",
      "Epoch [630 / 2000]: Train Loss=5.4299, Val Cor=0.6520, Time=0.1620 sec\n",
      "Epoch [631 / 2000]: Train Loss=5.4637, Val Cor=0.6501, Time=0.1612 sec\n",
      "Epoch [632 / 2000]: Train Loss=5.7504, Val Cor=0.6581, Time=0.1599 sec\n",
      "Epoch [633 / 2000]: Train Loss=5.2470, Val Cor=0.6564, Time=0.1595 sec\n",
      "Epoch [634 / 2000]: Train Loss=5.2938, Val Cor=0.6414, Time=0.1594 sec\n",
      "Epoch [635 / 2000]: Train Loss=5.3663, Val Cor=0.6546, Time=0.1596 sec\n",
      "Epoch [636 / 2000]: Train Loss=5.3458, Val Cor=0.6615, Time=0.1600 sec\n",
      "Epoch [637 / 2000]: Train Loss=5.1820, Val Cor=0.6446, Time=0.1599 sec\n",
      "Epoch [638 / 2000]: Train Loss=5.8459, Val Cor=0.6140, Time=0.1615 sec\n",
      "Epoch [639 / 2000]: Train Loss=5.4453, Val Cor=0.3616, Time=0.1619 sec\n",
      "Epoch [640 / 2000]: Train Loss=5.4490, Val Cor=0.6393, Time=0.1620 sec\n",
      "Epoch [641 / 2000]: Train Loss=5.3564, Val Cor=0.3575, Time=0.1617 sec\n",
      "Epoch [642 / 2000]: Train Loss=5.6119, Val Cor=0.6501, Time=0.1624 sec\n",
      "Epoch [643 / 2000]: Train Loss=5.4503, Val Cor=-0.5203, Time=0.1618 sec\n",
      "Epoch [644 / 2000]: Train Loss=5.3282, Val Cor=0.6586, Time=0.1601 sec\n",
      "Epoch [645 / 2000]: Train Loss=5.3856, Val Cor=-0.2836, Time=0.1600 sec\n",
      "Epoch [646 / 2000]: Train Loss=5.7537, Val Cor=-0.2022, Time=0.1609 sec\n",
      "Epoch [647 / 2000]: Train Loss=5.9144, Val Cor=-0.1443, Time=0.1603 sec\n",
      "Epoch [648 / 2000]: Train Loss=6.1973, Val Cor=-0.5447, Time=0.1608 sec\n",
      "Epoch [649 / 2000]: Train Loss=5.6990, Val Cor=-0.6477, Time=0.1610 sec\n",
      "Epoch [650 / 2000]: Train Loss=5.7517, Val Cor=0.1868, Time=0.1618 sec\n",
      "Epoch [651 / 2000]: Train Loss=5.6321, Val Cor=0.6517, Time=0.1618 sec\n",
      "Epoch [652 / 2000]: Train Loss=5.5321, Val Cor=0.6406, Time=0.1619 sec\n",
      "Epoch [653 / 2000]: Train Loss=5.2764, Val Cor=0.6412, Time=0.1612 sec\n",
      "Epoch [654 / 2000]: Train Loss=5.4380, Val Cor=0.6578, Time=0.1598 sec\n",
      "Epoch [655 / 2000]: Train Loss=5.6194, Val Cor=0.3957, Time=0.1600 sec\n",
      "Epoch [656 / 2000]: Train Loss=5.5503, Val Cor=0.6543, Time=0.1598 sec\n",
      "Epoch [657 / 2000]: Train Loss=5.2300, Val Cor=0.5902, Time=0.1600 sec\n",
      "Epoch [658 / 2000]: Train Loss=5.2830, Val Cor=0.6494, Time=0.1605 sec\n",
      "Epoch [659 / 2000]: Train Loss=5.6158, Val Cor=-0.4733, Time=0.1614 sec\n",
      "Epoch [660 / 2000]: Train Loss=5.3513, Val Cor=0.5726, Time=0.1612 sec\n",
      "Epoch [661 / 2000]: Train Loss=5.3060, Val Cor=0.6524, Time=0.1610 sec\n",
      "Epoch [662 / 2000]: Train Loss=5.2966, Val Cor=0.6687, Time=0.1616 sec\n",
      "Epoch [663 / 2000]: Train Loss=5.9529, Val Cor=0.6659, Time=0.1615 sec\n",
      "Epoch [664 / 2000]: Train Loss=5.5552, Val Cor=0.6671, Time=0.1615 sec\n",
      "Epoch [665 / 2000]: Train Loss=5.5774, Val Cor=0.6533, Time=0.1600 sec\n",
      "Epoch [666 / 2000]: Train Loss=5.4459, Val Cor=0.2973, Time=0.1597 sec\n",
      "Epoch [667 / 2000]: Train Loss=5.7305, Val Cor=0.6526, Time=0.1595 sec\n",
      "Epoch [668 / 2000]: Train Loss=5.3348, Val Cor=0.6536, Time=0.1603 sec\n",
      "Epoch [669 / 2000]: Train Loss=5.3847, Val Cor=0.6505, Time=0.1597 sec\n",
      "Epoch [670 / 2000]: Train Loss=5.4136, Val Cor=0.6480, Time=0.1603 sec\n",
      "Epoch [671 / 2000]: Train Loss=5.4691, Val Cor=0.6262, Time=0.1613 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [672 / 2000]: Train Loss=5.3888, Val Cor=0.6695, Time=0.1612 sec\n",
      "Epoch [673 / 2000]: Train Loss=5.4622, Val Cor=0.6537, Time=0.1609 sec\n",
      "Epoch [674 / 2000]: Train Loss=5.3212, Val Cor=0.6514, Time=0.1620 sec\n",
      "Epoch [675 / 2000]: Train Loss=5.2848, Val Cor=0.6588, Time=0.1614 sec\n",
      "Epoch [676 / 2000]: Train Loss=5.3918, Val Cor=0.6546, Time=0.1600 sec\n",
      "Epoch [677 / 2000]: Train Loss=5.1506, Val Cor=0.6679, Time=0.1585 sec\n",
      "Epoch [678 / 2000]: Train Loss=5.3517, Val Cor=-0.4407, Time=0.1587 sec\n",
      "Epoch [679 / 2000]: Train Loss=5.3109, Val Cor=0.6551, Time=0.1590 sec\n",
      "Epoch [680 / 2000]: Train Loss=5.4340, Val Cor=0.6135, Time=0.1587 sec\n",
      "Epoch [681 / 2000]: Train Loss=5.5340, Val Cor=0.6587, Time=0.1589 sec\n",
      "Epoch [682 / 2000]: Train Loss=5.3310, Val Cor=0.6508, Time=0.1596 sec\n",
      "Epoch [683 / 2000]: Train Loss=5.2231, Val Cor=0.6633, Time=0.1603 sec\n",
      "Epoch [684 / 2000]: Train Loss=5.2352, Val Cor=0.6495, Time=0.1598 sec\n",
      "Epoch [685 / 2000]: Train Loss=5.3155, Val Cor=0.6588, Time=0.1606 sec\n",
      "Epoch [686 / 2000]: Train Loss=5.1463, Val Cor=0.6566, Time=0.1602 sec\n",
      "Epoch [687 / 2000]: Train Loss=5.1421, Val Cor=0.6545, Time=0.1591 sec\n",
      "Epoch [688 / 2000]: Train Loss=5.7631, Val Cor=0.3319, Time=0.1600 sec\n",
      "Epoch [689 / 2000]: Train Loss=5.8363, Val Cor=-0.4346, Time=0.1602 sec\n",
      "Epoch [690 / 2000]: Train Loss=5.6472, Val Cor=0.6491, Time=0.1610 sec\n",
      "Epoch [691 / 2000]: Train Loss=5.6867, Val Cor=0.0698, Time=0.1610 sec\n",
      "Epoch [692 / 2000]: Train Loss=5.4857, Val Cor=0.6617, Time=0.1610 sec\n",
      "Epoch [693 / 2000]: Train Loss=5.3131, Val Cor=0.6508, Time=0.1610 sec\n",
      "Epoch [694 / 2000]: Train Loss=5.4148, Val Cor=0.6596, Time=0.1613 sec\n",
      "Epoch [695 / 2000]: Train Loss=5.5191, Val Cor=0.6729, Time=0.1622 sec\n",
      "Epoch [696 / 2000]: Train Loss=5.5890, Val Cor=-0.1389, Time=0.1620 sec\n",
      "Epoch [697 / 2000]: Train Loss=5.5390, Val Cor=-0.0797, Time=0.1621 sec\n",
      "Epoch [698 / 2000]: Train Loss=5.5610, Val Cor=0.6527, Time=0.1621 sec\n",
      "Epoch [699 / 2000]: Train Loss=5.2557, Val Cor=0.6587, Time=0.1624 sec\n",
      "Epoch [700 / 2000]: Train Loss=5.3227, Val Cor=0.6518, Time=0.1626 sec\n",
      "Epoch [701 / 2000]: Train Loss=5.6164, Val Cor=0.6489, Time=0.1623 sec\n",
      "Epoch [702 / 2000]: Train Loss=5.1680, Val Cor=0.6598, Time=0.1605 sec\n",
      "Epoch [703 / 2000]: Train Loss=5.1869, Val Cor=0.5091, Time=0.1604 sec\n",
      "Epoch [704 / 2000]: Train Loss=5.3524, Val Cor=0.5211, Time=0.1601 sec\n",
      "Epoch [705 / 2000]: Train Loss=5.5000, Val Cor=0.6631, Time=0.1596 sec\n",
      "Epoch [706 / 2000]: Train Loss=5.5186, Val Cor=-0.6635, Time=0.1603 sec\n",
      "Epoch [707 / 2000]: Train Loss=5.6134, Val Cor=-0.6357, Time=0.1600 sec\n",
      "Epoch [708 / 2000]: Train Loss=5.4937, Val Cor=-0.6140, Time=0.1599 sec\n",
      "Epoch [709 / 2000]: Train Loss=5.4923, Val Cor=-0.6161, Time=0.1602 sec\n",
      "Epoch [710 / 2000]: Train Loss=5.5125, Val Cor=0.6061, Time=0.1617 sec\n",
      "Epoch [711 / 2000]: Train Loss=5.3744, Val Cor=0.3877, Time=0.1611 sec\n",
      "Epoch [712 / 2000]: Train Loss=5.2608, Val Cor=0.6539, Time=0.1611 sec\n",
      "Epoch [713 / 2000]: Train Loss=5.3368, Val Cor=0.6433, Time=0.1618 sec\n",
      "Epoch [714 / 2000]: Train Loss=5.3237, Val Cor=0.6361, Time=0.1616 sec\n",
      "Epoch [715 / 2000]: Train Loss=5.1266, Val Cor=0.6637, Time=0.1597 sec\n",
      "Epoch [716 / 2000]: Train Loss=5.2586, Val Cor=0.6489, Time=0.1599 sec\n",
      "Epoch [717 / 2000]: Train Loss=5.3317, Val Cor=0.6235, Time=0.1601 sec\n",
      "Epoch [718 / 2000]: Train Loss=5.2224, Val Cor=0.6618, Time=0.1599 sec\n",
      "Epoch [719 / 2000]: Train Loss=5.5527, Val Cor=-0.6074, Time=0.1614 sec\n",
      "Epoch [720 / 2000]: Train Loss=5.2075, Val Cor=0.6684, Time=0.1614 sec\n",
      "Epoch [721 / 2000]: Train Loss=5.5474, Val Cor=0.6137, Time=0.1623 sec\n",
      "Epoch [722 / 2000]: Train Loss=5.1959, Val Cor=0.3189, Time=0.1621 sec\n",
      "Epoch [723 / 2000]: Train Loss=5.4586, Val Cor=0.6339, Time=0.1603 sec\n",
      "Epoch [724 / 2000]: Train Loss=5.4105, Val Cor=0.6292, Time=0.1601 sec\n",
      "Epoch [725 / 2000]: Train Loss=5.2967, Val Cor=0.6600, Time=0.1603 sec\n",
      "Epoch [726 / 2000]: Train Loss=5.0914, Val Cor=-0.5741, Time=0.1608 sec\n",
      "Epoch [727 / 2000]: Train Loss=5.2206, Val Cor=0.6499, Time=0.1607 sec\n",
      "Epoch [728 / 2000]: Train Loss=5.0822, Val Cor=-0.6024, Time=0.1616 sec\n",
      "Epoch [729 / 2000]: Train Loss=5.3391, Val Cor=0.5313, Time=0.1728 sec\n",
      "Epoch [730 / 2000]: Train Loss=5.2416, Val Cor=0.6624, Time=0.1679 sec\n",
      "Epoch [731 / 2000]: Train Loss=5.5272, Val Cor=-0.6029, Time=0.1619 sec\n",
      "Epoch [732 / 2000]: Train Loss=5.4644, Val Cor=0.6560, Time=0.1623 sec\n",
      "Epoch [733 / 2000]: Train Loss=5.5772, Val Cor=0.6523, Time=0.1623 sec\n",
      "Epoch [734 / 2000]: Train Loss=5.3196, Val Cor=0.6530, Time=0.1622 sec\n",
      "Epoch [735 / 2000]: Train Loss=5.3309, Val Cor=0.6506, Time=0.1601 sec\n",
      "Epoch [736 / 2000]: Train Loss=5.4458, Val Cor=-0.6341, Time=0.1603 sec\n",
      "Epoch [737 / 2000]: Train Loss=5.6624, Val Cor=-0.0273, Time=0.1603 sec\n",
      "Epoch [738 / 2000]: Train Loss=5.3946, Val Cor=-0.6432, Time=0.1613 sec\n",
      "Epoch [739 / 2000]: Train Loss=5.5378, Val Cor=0.6543, Time=0.1622 sec\n",
      "Epoch [740 / 2000]: Train Loss=5.1737, Val Cor=-0.5009, Time=0.1625 sec\n",
      "Epoch [741 / 2000]: Train Loss=5.2915, Val Cor=0.6635, Time=0.1621 sec\n",
      "Epoch [742 / 2000]: Train Loss=4.9724, Val Cor=0.5739, Time=0.1623 sec\n",
      "Epoch [743 / 2000]: Train Loss=5.1130, Val Cor=0.6667, Time=0.1624 sec\n",
      "Epoch [744 / 2000]: Train Loss=5.3059, Val Cor=-0.6661, Time=0.1623 sec\n",
      "Epoch [745 / 2000]: Train Loss=5.4423, Val Cor=-0.6457, Time=0.1631 sec\n",
      "Epoch [746 / 2000]: Train Loss=5.6109, Val Cor=0.6635, Time=0.1632 sec\n",
      "Epoch [747 / 2000]: Train Loss=5.3902, Val Cor=0.6431, Time=0.1623 sec\n",
      "Epoch [748 / 2000]: Train Loss=5.2311, Val Cor=0.6430, Time=0.1608 sec\n",
      "Epoch [749 / 2000]: Train Loss=5.1437, Val Cor=0.2874, Time=0.1604 sec\n",
      "Epoch [750 / 2000]: Train Loss=5.1548, Val Cor=0.6528, Time=0.1607 sec\n",
      "Epoch [751 / 2000]: Train Loss=5.7798, Val Cor=0.3888, Time=0.1617 sec\n",
      "Epoch [752 / 2000]: Train Loss=5.6939, Val Cor=0.0194, Time=0.1623 sec\n",
      "Epoch [753 / 2000]: Train Loss=5.4698, Val Cor=0.6131, Time=0.1619 sec\n",
      "Epoch [754 / 2000]: Train Loss=5.5917, Val Cor=-0.5868, Time=0.1624 sec\n",
      "Epoch [755 / 2000]: Train Loss=5.6281, Val Cor=0.5372, Time=0.1619 sec\n",
      "Epoch [756 / 2000]: Train Loss=5.1550, Val Cor=0.6629, Time=0.1601 sec\n",
      "Epoch [757 / 2000]: Train Loss=5.2135, Val Cor=0.2516, Time=0.1600 sec\n",
      "Epoch [758 / 2000]: Train Loss=5.5233, Val Cor=0.5807, Time=0.1602 sec\n",
      "Epoch [759 / 2000]: Train Loss=5.1357, Val Cor=0.6476, Time=0.1600 sec\n",
      "Epoch [760 / 2000]: Train Loss=5.2888, Val Cor=0.2161, Time=0.1603 sec\n",
      "Epoch [761 / 2000]: Train Loss=5.1483, Val Cor=0.6576, Time=0.1605 sec\n",
      "Epoch [762 / 2000]: Train Loss=5.2372, Val Cor=0.6630, Time=0.1613 sec\n",
      "Epoch [763 / 2000]: Train Loss=5.1981, Val Cor=0.4311, Time=0.1614 sec\n",
      "Epoch [764 / 2000]: Train Loss=5.3634, Val Cor=-0.4076, Time=0.1614 sec\n",
      "Epoch [765 / 2000]: Train Loss=5.1794, Val Cor=0.6574, Time=0.1615 sec\n",
      "Epoch [766 / 2000]: Train Loss=5.2710, Val Cor=0.6603, Time=0.1619 sec\n",
      "Epoch [767 / 2000]: Train Loss=5.1487, Val Cor=0.6593, Time=0.1599 sec\n",
      "Epoch [768 / 2000]: Train Loss=5.5013, Val Cor=0.6653, Time=0.1588 sec\n",
      "Epoch [769 / 2000]: Train Loss=5.4579, Val Cor=0.6302, Time=0.1589 sec\n",
      "Epoch [770 / 2000]: Train Loss=5.4361, Val Cor=0.3159, Time=0.1590 sec\n",
      "Epoch [771 / 2000]: Train Loss=5.0085, Val Cor=0.6673, Time=0.1590 sec\n",
      "Epoch [772 / 2000]: Train Loss=5.3746, Val Cor=-0.5442, Time=0.1591 sec\n",
      "Epoch [773 / 2000]: Train Loss=5.1871, Val Cor=0.6653, Time=0.1598 sec\n",
      "Epoch [774 / 2000]: Train Loss=5.4357, Val Cor=0.6546, Time=0.1602 sec\n",
      "Epoch [775 / 2000]: Train Loss=5.2367, Val Cor=0.4620, Time=0.1601 sec\n",
      "Epoch [776 / 2000]: Train Loss=5.4189, Val Cor=0.6436, Time=0.1601 sec\n",
      "Epoch [777 / 2000]: Train Loss=5.3598, Val Cor=0.0696, Time=0.1607 sec\n",
      "Epoch [778 / 2000]: Train Loss=5.5575, Val Cor=0.3630, Time=0.1605 sec\n",
      "Epoch [779 / 2000]: Train Loss=5.2679, Val Cor=0.6100, Time=0.1597 sec\n",
      "Epoch [780 / 2000]: Train Loss=5.3934, Val Cor=0.5075, Time=0.1602 sec\n",
      "Epoch [781 / 2000]: Train Loss=5.1304, Val Cor=0.6181, Time=0.1603 sec\n",
      "Epoch [782 / 2000]: Train Loss=5.4501, Val Cor=0.3235, Time=0.1606 sec\n",
      "Epoch [783 / 2000]: Train Loss=5.2128, Val Cor=-0.4406, Time=0.1605 sec\n",
      "Epoch [784 / 2000]: Train Loss=5.3670, Val Cor=0.4148, Time=0.1604 sec\n",
      "Epoch [785 / 2000]: Train Loss=5.1931, Val Cor=0.4067, Time=0.1605 sec\n",
      "Epoch [786 / 2000]: Train Loss=5.2284, Val Cor=0.5630, Time=0.1617 sec\n",
      "Epoch [787 / 2000]: Train Loss=5.3680, Val Cor=0.6439, Time=0.1617 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [788 / 2000]: Train Loss=5.3837, Val Cor=0.5724, Time=0.1623 sec\n",
      "Epoch [789 / 2000]: Train Loss=5.2295, Val Cor=0.5450, Time=0.1619 sec\n",
      "Epoch [790 / 2000]: Train Loss=5.1020, Val Cor=0.6382, Time=0.1627 sec\n",
      "Epoch [791 / 2000]: Train Loss=5.2016, Val Cor=0.6463, Time=0.1621 sec\n",
      "Epoch [792 / 2000]: Train Loss=5.2425, Val Cor=0.4153, Time=0.1611 sec\n",
      "Epoch [793 / 2000]: Train Loss=5.3504, Val Cor=0.5995, Time=0.1600 sec\n",
      "Epoch [794 / 2000]: Train Loss=5.1193, Val Cor=0.5798, Time=0.1605 sec\n",
      "Epoch [795 / 2000]: Train Loss=5.0764, Val Cor=0.6660, Time=0.1604 sec\n",
      "Epoch [796 / 2000]: Train Loss=5.2398, Val Cor=0.5272, Time=0.1609 sec\n",
      "Epoch [797 / 2000]: Train Loss=5.4735, Val Cor=0.3756, Time=0.1618 sec\n",
      "Epoch [798 / 2000]: Train Loss=5.4357, Val Cor=0.6543, Time=0.1621 sec\n",
      "Epoch [799 / 2000]: Train Loss=5.1674, Val Cor=0.6599, Time=0.1615 sec\n",
      "Epoch [800 / 2000]: Train Loss=5.1777, Val Cor=0.3913, Time=0.1619 sec\n",
      "Epoch [801 / 2000]: Train Loss=5.3183, Val Cor=0.5526, Time=0.1618 sec\n",
      "Epoch [802 / 2000]: Train Loss=5.2132, Val Cor=0.5915, Time=0.1605 sec\n",
      "Epoch [803 / 2000]: Train Loss=5.2078, Val Cor=0.2571, Time=0.1601 sec\n",
      "Epoch [804 / 2000]: Train Loss=5.2282, Val Cor=0.6701, Time=0.1597 sec\n",
      "Epoch [805 / 2000]: Train Loss=5.0849, Val Cor=0.6649, Time=0.1601 sec\n",
      "Epoch [806 / 2000]: Train Loss=5.2827, Val Cor=0.6273, Time=0.1599 sec\n",
      "Epoch [807 / 2000]: Train Loss=5.6117, Val Cor=0.6530, Time=0.1605 sec\n",
      "Epoch [808 / 2000]: Train Loss=5.2108, Val Cor=0.6639, Time=0.1614 sec\n",
      "Epoch [809 / 2000]: Train Loss=5.3530, Val Cor=0.6063, Time=0.1614 sec\n",
      "Epoch [810 / 2000]: Train Loss=5.1646, Val Cor=0.6770, Time=0.1611 sec\n",
      "Epoch [811 / 2000]: Train Loss=5.1106, Val Cor=0.6759, Time=0.1611 sec\n",
      "Epoch [812 / 2000]: Train Loss=5.2948, Val Cor=0.2921, Time=0.1620 sec\n",
      "Epoch [813 / 2000]: Train Loss=5.2258, Val Cor=0.6517, Time=0.1615 sec\n",
      "Epoch [814 / 2000]: Train Loss=5.1166, Val Cor=-0.1436, Time=0.1611 sec\n",
      "Epoch [815 / 2000]: Train Loss=5.0667, Val Cor=0.6722, Time=0.1604 sec\n",
      "Epoch [816 / 2000]: Train Loss=5.3873, Val Cor=-0.0839, Time=0.1606 sec\n",
      "Epoch [817 / 2000]: Train Loss=5.5200, Val Cor=0.5540, Time=0.1604 sec\n",
      "Epoch [818 / 2000]: Train Loss=5.4624, Val Cor=-0.4769, Time=0.1601 sec\n",
      "Epoch [819 / 2000]: Train Loss=5.2693, Val Cor=-0.1068, Time=0.1601 sec\n",
      "Epoch [820 / 2000]: Train Loss=5.3521, Val Cor=0.3312, Time=0.1610 sec\n",
      "Epoch [821 / 2000]: Train Loss=5.3935, Val Cor=0.6704, Time=0.1612 sec\n",
      "Epoch [822 / 2000]: Train Loss=5.1375, Val Cor=0.6470, Time=0.1619 sec\n",
      "Epoch [823 / 2000]: Train Loss=5.3836, Val Cor=0.4674, Time=0.1617 sec\n",
      "Epoch [824 / 2000]: Train Loss=5.2704, Val Cor=0.6445, Time=0.1619 sec\n",
      "Epoch [825 / 2000]: Train Loss=5.3042, Val Cor=0.5666, Time=0.1598 sec\n",
      "Epoch [826 / 2000]: Train Loss=5.2010, Val Cor=0.6658, Time=0.1597 sec\n",
      "Epoch [827 / 2000]: Train Loss=5.2704, Val Cor=-0.0477, Time=0.1600 sec\n",
      "Epoch [828 / 2000]: Train Loss=5.1551, Val Cor=0.6137, Time=0.1587 sec\n",
      "Epoch [829 / 2000]: Train Loss=5.1673, Val Cor=0.4077, Time=0.1588 sec\n",
      "Epoch [830 / 2000]: Train Loss=5.5707, Val Cor=0.6465, Time=0.1596 sec\n",
      "Epoch [831 / 2000]: Train Loss=5.3409, Val Cor=0.6378, Time=0.1604 sec\n",
      "Epoch [832 / 2000]: Train Loss=5.5554, Val Cor=0.6311, Time=0.1603 sec\n",
      "Epoch [833 / 2000]: Train Loss=5.4110, Val Cor=-0.6112, Time=0.1600 sec\n",
      "Epoch [834 / 2000]: Train Loss=5.4958, Val Cor=0.5663, Time=0.1616 sec\n",
      "Epoch [835 / 2000]: Train Loss=5.1352, Val Cor=0.6754, Time=0.1600 sec\n",
      "Epoch [836 / 2000]: Train Loss=5.0137, Val Cor=0.6163, Time=0.1596 sec\n",
      "Epoch [837 / 2000]: Train Loss=5.1454, Val Cor=0.5701, Time=0.1593 sec\n",
      "Epoch [838 / 2000]: Train Loss=5.0925, Val Cor=0.6326, Time=0.1591 sec\n",
      "Epoch [839 / 2000]: Train Loss=5.3978, Val Cor=0.6589, Time=0.1602 sec\n",
      "Epoch [840 / 2000]: Train Loss=5.1793, Val Cor=-0.2928, Time=0.1620 sec\n",
      "Epoch [841 / 2000]: Train Loss=5.0762, Val Cor=0.6471, Time=0.1614 sec\n",
      "Epoch [842 / 2000]: Train Loss=5.0033, Val Cor=0.6401, Time=0.1612 sec\n",
      "Epoch [843 / 2000]: Train Loss=5.1627, Val Cor=0.6641, Time=0.1621 sec\n",
      "Epoch [844 / 2000]: Train Loss=5.1507, Val Cor=0.6606, Time=0.1619 sec\n",
      "Epoch [845 / 2000]: Train Loss=5.1236, Val Cor=0.6170, Time=0.1606 sec\n",
      "Epoch [846 / 2000]: Train Loss=5.4355, Val Cor=0.6419, Time=0.1605 sec\n",
      "Epoch [847 / 2000]: Train Loss=5.5315, Val Cor=-0.6585, Time=0.1609 sec\n",
      "Epoch [848 / 2000]: Train Loss=5.4639, Val Cor=0.6514, Time=0.1605 sec\n",
      "Epoch [849 / 2000]: Train Loss=5.4276, Val Cor=-0.1970, Time=0.1602 sec\n",
      "Epoch [850 / 2000]: Train Loss=5.5734, Val Cor=-0.1217, Time=0.1609 sec\n",
      "Epoch [851 / 2000]: Train Loss=5.6232, Val Cor=0.4257, Time=0.1619 sec\n",
      "Epoch [852 / 2000]: Train Loss=5.2022, Val Cor=0.6841, Time=0.1625 sec\n",
      "Epoch [853 / 2000]: Train Loss=5.5254, Val Cor=-0.4986, Time=0.1617 sec\n",
      "Epoch [854 / 2000]: Train Loss=5.2973, Val Cor=-0.5537, Time=0.1620 sec\n",
      "Epoch [855 / 2000]: Train Loss=5.3609, Val Cor=-0.4483, Time=0.1624 sec\n",
      "Epoch [856 / 2000]: Train Loss=5.1656, Val Cor=0.0035, Time=0.1620 sec\n",
      "Epoch [857 / 2000]: Train Loss=5.6204, Val Cor=-0.3493, Time=0.1603 sec\n",
      "Epoch [858 / 2000]: Train Loss=5.8352, Val Cor=0.2627, Time=0.1595 sec\n",
      "Epoch [859 / 2000]: Train Loss=5.7180, Val Cor=-0.0612, Time=0.1602 sec\n",
      "Epoch [860 / 2000]: Train Loss=5.1665, Val Cor=0.1378, Time=0.1601 sec\n",
      "Epoch [861 / 2000]: Train Loss=5.1423, Val Cor=0.6492, Time=0.1603 sec\n",
      "Epoch [862 / 2000]: Train Loss=5.2117, Val Cor=0.3583, Time=0.1603 sec\n",
      "Epoch [863 / 2000]: Train Loss=5.4534, Val Cor=-0.3385, Time=0.1605 sec\n",
      "Epoch [864 / 2000]: Train Loss=5.4360, Val Cor=0.6543, Time=0.1617 sec\n",
      "Epoch [865 / 2000]: Train Loss=5.0058, Val Cor=0.6675, Time=0.1617 sec\n",
      "Epoch [866 / 2000]: Train Loss=5.2190, Val Cor=0.6647, Time=0.1615 sec\n",
      "Epoch [867 / 2000]: Train Loss=5.5806, Val Cor=-0.4132, Time=0.1619 sec\n",
      "Epoch [868 / 2000]: Train Loss=5.4692, Val Cor=0.6615, Time=0.1617 sec\n",
      "Epoch [869 / 2000]: Train Loss=5.0821, Val Cor=0.6671, Time=0.1597 sec\n",
      "Epoch [870 / 2000]: Train Loss=5.0641, Val Cor=0.3426, Time=0.1598 sec\n",
      "Epoch [871 / 2000]: Train Loss=5.0824, Val Cor=0.6614, Time=0.1600 sec\n",
      "Epoch [872 / 2000]: Train Loss=5.3966, Val Cor=-0.1959, Time=0.1607 sec\n",
      "Epoch [873 / 2000]: Train Loss=5.3499, Val Cor=0.5818, Time=0.1592 sec\n",
      "Epoch [874 / 2000]: Train Loss=5.1617, Val Cor=0.6325, Time=0.1593 sec\n",
      "Epoch [875 / 2000]: Train Loss=5.4774, Val Cor=0.6571, Time=0.1604 sec\n",
      "Epoch [876 / 2000]: Train Loss=5.7099, Val Cor=-0.3342, Time=0.1605 sec\n",
      "Epoch [877 / 2000]: Train Loss=5.3029, Val Cor=0.2834, Time=0.1600 sec\n",
      "Epoch [878 / 2000]: Train Loss=5.1439, Val Cor=0.6592, Time=0.1603 sec\n",
      "Epoch [879 / 2000]: Train Loss=5.3895, Val Cor=0.6133, Time=0.1603 sec\n",
      "Epoch [880 / 2000]: Train Loss=5.3276, Val Cor=0.5969, Time=0.1601 sec\n",
      "Epoch [881 / 2000]: Train Loss=5.1660, Val Cor=-0.4539, Time=0.1593 sec\n",
      "Epoch [882 / 2000]: Train Loss=5.1663, Val Cor=0.5987, Time=0.1589 sec\n",
      "Epoch [883 / 2000]: Train Loss=5.3610, Val Cor=-0.6548, Time=0.1584 sec\n",
      "Epoch [884 / 2000]: Train Loss=5.3462, Val Cor=-0.5169, Time=0.1601 sec\n",
      "Epoch [885 / 2000]: Train Loss=4.9197, Val Cor=-0.3525, Time=0.1607 sec\n",
      "Epoch [886 / 2000]: Train Loss=5.1748, Val Cor=0.6631, Time=0.1606 sec\n",
      "Epoch [887 / 2000]: Train Loss=5.1599, Val Cor=-0.5864, Time=0.1605 sec\n",
      "Epoch [888 / 2000]: Train Loss=5.2505, Val Cor=0.2814, Time=0.1607 sec\n",
      "Epoch [889 / 2000]: Train Loss=5.2725, Val Cor=0.6448, Time=0.1609 sec\n",
      "Epoch [890 / 2000]: Train Loss=5.3246, Val Cor=0.5680, Time=0.1625 sec\n",
      "Epoch [891 / 2000]: Train Loss=5.1342, Val Cor=0.6442, Time=0.1620 sec\n",
      "Epoch [892 / 2000]: Train Loss=5.0728, Val Cor=0.0831, Time=0.1628 sec\n",
      "Epoch [893 / 2000]: Train Loss=5.1571, Val Cor=0.6017, Time=0.1623 sec\n",
      "Epoch [894 / 2000]: Train Loss=5.3046, Val Cor=0.6749, Time=0.1608 sec\n",
      "Epoch [895 / 2000]: Train Loss=5.0512, Val Cor=0.6327, Time=0.1606 sec\n",
      "Epoch [896 / 2000]: Train Loss=5.1045, Val Cor=0.6595, Time=0.1612 sec\n",
      "Epoch [897 / 2000]: Train Loss=5.2688, Val Cor=0.6249, Time=0.1611 sec\n",
      "Epoch [898 / 2000]: Train Loss=5.2153, Val Cor=-0.1956, Time=0.1608 sec\n",
      "Epoch [899 / 2000]: Train Loss=5.4654, Val Cor=0.5984, Time=0.1603 sec\n",
      "Epoch [900 / 2000]: Train Loss=5.3544, Val Cor=0.6789, Time=0.1608 sec\n",
      "Epoch [901 / 2000]: Train Loss=5.0896, Val Cor=0.6660, Time=0.1607 sec\n",
      "Epoch [902 / 2000]: Train Loss=5.0081, Val Cor=0.5883, Time=0.1618 sec\n",
      "Epoch [903 / 2000]: Train Loss=5.0941, Val Cor=0.6652, Time=0.1616 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [904 / 2000]: Train Loss=5.5479, Val Cor=0.3429, Time=0.1620 sec\n",
      "Epoch [905 / 2000]: Train Loss=5.0793, Val Cor=0.6567, Time=0.1623 sec\n",
      "Epoch [906 / 2000]: Train Loss=5.3324, Val Cor=0.6474, Time=0.1619 sec\n",
      "Epoch [907 / 2000]: Train Loss=5.2687, Val Cor=0.5426, Time=0.1610 sec\n",
      "Epoch [908 / 2000]: Train Loss=5.1406, Val Cor=0.4577, Time=0.1600 sec\n",
      "Epoch [909 / 2000]: Train Loss=5.4136, Val Cor=0.6056, Time=0.1603 sec\n",
      "Epoch [910 / 2000]: Train Loss=5.3522, Val Cor=-0.6493, Time=0.1606 sec\n",
      "Epoch [911 / 2000]: Train Loss=5.3290, Val Cor=-0.6253, Time=0.1600 sec\n",
      "Epoch [912 / 2000]: Train Loss=5.7051, Val Cor=-0.2538, Time=0.1603 sec\n",
      "Epoch [913 / 2000]: Train Loss=5.3470, Val Cor=-0.6261, Time=0.1615 sec\n",
      "Epoch [914 / 2000]: Train Loss=5.4756, Val Cor=-0.4962, Time=0.1613 sec\n",
      "Epoch [915 / 2000]: Train Loss=5.2842, Val Cor=-0.3732, Time=0.1597 sec\n",
      "Epoch [916 / 2000]: Train Loss=5.2239, Val Cor=-0.5886, Time=0.1598 sec\n",
      "Epoch [917 / 2000]: Train Loss=5.2259, Val Cor=-0.5255, Time=0.1600 sec\n",
      "Epoch [918 / 2000]: Train Loss=5.6366, Val Cor=0.6224, Time=0.1600 sec\n",
      "Epoch [919 / 2000]: Train Loss=5.2642, Val Cor=0.6657, Time=0.1601 sec\n",
      "Epoch [920 / 2000]: Train Loss=5.3459, Val Cor=-0.3077, Time=0.1601 sec\n",
      "Epoch [921 / 2000]: Train Loss=5.3672, Val Cor=-0.5478, Time=0.1616 sec\n",
      "Epoch [922 / 2000]: Train Loss=5.0336, Val Cor=0.6781, Time=0.1616 sec\n",
      "Epoch [923 / 2000]: Train Loss=5.0294, Val Cor=0.6683, Time=0.1612 sec\n",
      "Epoch [924 / 2000]: Train Loss=5.0398, Val Cor=0.6362, Time=0.1623 sec\n",
      "Epoch [925 / 2000]: Train Loss=5.2187, Val Cor=0.6607, Time=0.1619 sec\n",
      "Epoch [926 / 2000]: Train Loss=5.1179, Val Cor=0.6245, Time=0.1610 sec\n",
      "Epoch [927 / 2000]: Train Loss=5.1619, Val Cor=-0.5114, Time=0.1602 sec\n",
      "Epoch [928 / 2000]: Train Loss=5.4448, Val Cor=0.2341, Time=0.1599 sec\n",
      "Epoch [929 / 2000]: Train Loss=5.3497, Val Cor=-0.6282, Time=0.1598 sec\n",
      "Epoch [930 / 2000]: Train Loss=5.2525, Val Cor=0.6091, Time=0.1599 sec\n",
      "Epoch [931 / 2000]: Train Loss=5.2455, Val Cor=0.6087, Time=0.1604 sec\n",
      "Epoch [932 / 2000]: Train Loss=5.3130, Val Cor=0.4924, Time=0.1615 sec\n",
      "Epoch [933 / 2000]: Train Loss=4.9578, Val Cor=0.6558, Time=0.1614 sec\n",
      "Epoch [934 / 2000]: Train Loss=5.2321, Val Cor=-0.2008, Time=0.1620 sec\n",
      "Epoch [935 / 2000]: Train Loss=5.1324, Val Cor=-0.1797, Time=0.1611 sec\n",
      "Epoch [936 / 2000]: Train Loss=5.2367, Val Cor=-0.5169, Time=0.1613 sec\n",
      "Epoch [937 / 2000]: Train Loss=5.1589, Val Cor=-0.6081, Time=0.1618 sec\n",
      "Epoch [938 / 2000]: Train Loss=4.9730, Val Cor=0.5622, Time=0.1622 sec\n",
      "Epoch [939 / 2000]: Train Loss=4.9841, Val Cor=0.4442, Time=0.1615 sec\n",
      "Epoch [940 / 2000]: Train Loss=5.0578, Val Cor=0.6483, Time=0.1596 sec\n",
      "Epoch [941 / 2000]: Train Loss=4.9564, Val Cor=0.5050, Time=0.1601 sec\n",
      "Epoch [942 / 2000]: Train Loss=4.9742, Val Cor=0.6543, Time=0.1601 sec\n",
      "Epoch [943 / 2000]: Train Loss=4.8988, Val Cor=0.6427, Time=0.1602 sec\n",
      "Epoch [944 / 2000]: Train Loss=5.2144, Val Cor=0.6557, Time=0.1603 sec\n",
      "Epoch [945 / 2000]: Train Loss=5.1218, Val Cor=0.5684, Time=0.1599 sec\n",
      "Epoch [946 / 2000]: Train Loss=5.1063, Val Cor=0.6137, Time=0.1606 sec\n",
      "Epoch [947 / 2000]: Train Loss=5.0892, Val Cor=0.3604, Time=0.1614 sec\n",
      "Epoch [948 / 2000]: Train Loss=5.3624, Val Cor=0.2052, Time=0.1614 sec\n",
      "Epoch [949 / 2000]: Train Loss=5.2655, Val Cor=0.0571, Time=0.1613 sec\n",
      "Epoch [950 / 2000]: Train Loss=5.2256, Val Cor=-0.5954, Time=0.1619 sec\n",
      "Epoch [951 / 2000]: Train Loss=5.1842, Val Cor=-0.4896, Time=0.1619 sec\n",
      "Epoch [952 / 2000]: Train Loss=5.1205, Val Cor=0.6550, Time=0.1616 sec\n",
      "Epoch [953 / 2000]: Train Loss=5.1431, Val Cor=0.6631, Time=0.1584 sec\n",
      "Epoch [954 / 2000]: Train Loss=5.0945, Val Cor=0.6529, Time=0.1584 sec\n",
      "Epoch [955 / 2000]: Train Loss=4.9758, Val Cor=-0.0204, Time=0.1584 sec\n",
      "Epoch [956 / 2000]: Train Loss=5.4062, Val Cor=0.5656, Time=0.1590 sec\n",
      "Epoch [957 / 2000]: Train Loss=5.1761, Val Cor=0.6273, Time=0.1588 sec\n",
      "Epoch [958 / 2000]: Train Loss=4.9759, Val Cor=0.5033, Time=0.1588 sec\n",
      "Epoch [959 / 2000]: Train Loss=5.0365, Val Cor=0.5737, Time=0.1593 sec\n",
      "Epoch [960 / 2000]: Train Loss=4.8637, Val Cor=0.6363, Time=0.1605 sec\n",
      "Epoch [961 / 2000]: Train Loss=5.3483, Val Cor=-0.6247, Time=0.1604 sec\n",
      "Epoch [962 / 2000]: Train Loss=5.4434, Val Cor=0.0770, Time=0.1608 sec\n",
      "Epoch [963 / 2000]: Train Loss=5.1068, Val Cor=-0.0283, Time=0.1604 sec\n",
      "Epoch [964 / 2000]: Train Loss=5.0380, Val Cor=0.2986, Time=0.1617 sec\n",
      "Epoch [965 / 2000]: Train Loss=5.0264, Val Cor=0.3727, Time=0.1610 sec\n",
      "Epoch [966 / 2000]: Train Loss=5.5063, Val Cor=-0.0713, Time=0.1609 sec\n",
      "Epoch [967 / 2000]: Train Loss=5.6911, Val Cor=0.6686, Time=0.1609 sec\n",
      "Epoch [968 / 2000]: Train Loss=5.2531, Val Cor=-0.4971, Time=0.1613 sec\n",
      "Epoch [969 / 2000]: Train Loss=5.3490, Val Cor=0.1634, Time=0.1612 sec\n",
      "Epoch [970 / 2000]: Train Loss=5.1321, Val Cor=0.4783, Time=0.1629 sec\n",
      "Epoch [971 / 2000]: Train Loss=5.2951, Val Cor=-0.6032, Time=0.1622 sec\n",
      "Epoch [972 / 2000]: Train Loss=5.0953, Val Cor=0.4252, Time=0.1626 sec\n",
      "Epoch [973 / 2000]: Train Loss=5.1896, Val Cor=0.6458, Time=0.1624 sec\n",
      "Epoch [974 / 2000]: Train Loss=4.8811, Val Cor=0.5664, Time=0.1624 sec\n",
      "Epoch [975 / 2000]: Train Loss=5.0779, Val Cor=0.3282, Time=0.1611 sec\n",
      "Epoch [976 / 2000]: Train Loss=5.1478, Val Cor=0.3960, Time=0.1613 sec\n",
      "Epoch [977 / 2000]: Train Loss=5.5478, Val Cor=-0.0134, Time=0.1612 sec\n",
      "Epoch [978 / 2000]: Train Loss=5.0702, Val Cor=0.6489, Time=0.1600 sec\n",
      "Epoch [979 / 2000]: Train Loss=5.2792, Val Cor=0.6530, Time=0.1602 sec\n",
      "Epoch [980 / 2000]: Train Loss=5.1779, Val Cor=-0.2421, Time=0.1618 sec\n",
      "Epoch [981 / 2000]: Train Loss=5.4036, Val Cor=0.4804, Time=0.1613 sec\n",
      "Epoch [982 / 2000]: Train Loss=5.1580, Val Cor=0.6504, Time=0.1614 sec\n",
      "Epoch [983 / 2000]: Train Loss=5.1343, Val Cor=0.6569, Time=0.1618 sec\n",
      "Epoch [984 / 2000]: Train Loss=4.9915, Val Cor=0.6522, Time=0.1619 sec\n",
      "Epoch [985 / 2000]: Train Loss=4.9194, Val Cor=0.6429, Time=0.1608 sec\n",
      "Epoch [986 / 2000]: Train Loss=5.0329, Val Cor=0.6475, Time=0.1603 sec\n",
      "Epoch [987 / 2000]: Train Loss=4.9802, Val Cor=0.5880, Time=0.1598 sec\n",
      "Epoch [988 / 2000]: Train Loss=4.9099, Val Cor=-0.5523, Time=0.1588 sec\n",
      "Epoch [989 / 2000]: Train Loss=4.8817, Val Cor=0.5429, Time=0.1590 sec\n",
      "Epoch [990 / 2000]: Train Loss=4.8693, Val Cor=0.6778, Time=0.1593 sec\n",
      "Epoch [991 / 2000]: Train Loss=5.0558, Val Cor=0.1957, Time=0.1591 sec\n",
      "Epoch [992 / 2000]: Train Loss=5.3715, Val Cor=-0.6496, Time=0.1598 sec\n",
      "Epoch [993 / 2000]: Train Loss=5.5198, Val Cor=0.6563, Time=0.1606 sec\n",
      "Epoch [994 / 2000]: Train Loss=5.2902, Val Cor=0.3986, Time=0.1608 sec\n",
      "Epoch [995 / 2000]: Train Loss=5.3481, Val Cor=0.6634, Time=0.1603 sec\n",
      "Epoch [996 / 2000]: Train Loss=5.3965, Val Cor=0.1031, Time=0.1607 sec\n",
      "Epoch [997 / 2000]: Train Loss=5.5769, Val Cor=-0.0949, Time=0.1603 sec\n",
      "Epoch [998 / 2000]: Train Loss=5.1609, Val Cor=0.6580, Time=0.1587 sec\n",
      "Epoch [999 / 2000]: Train Loss=5.1213, Val Cor=0.6803, Time=0.1598 sec\n",
      "Epoch [1000 / 2000]: Train Loss=5.0016, Val Cor=-0.5648, Time=0.1603 sec\n",
      "Epoch [1001 / 2000]: Train Loss=5.0282, Val Cor=0.6536, Time=0.1606 sec\n",
      "Epoch [1002 / 2000]: Train Loss=5.1889, Val Cor=0.0502, Time=0.1610 sec\n",
      "Epoch [1003 / 2000]: Train Loss=5.2083, Val Cor=0.6189, Time=0.1608 sec\n",
      "Epoch [1004 / 2000]: Train Loss=5.0171, Val Cor=0.6578, Time=0.1623 sec\n",
      "Epoch [1005 / 2000]: Train Loss=4.8757, Val Cor=-0.5678, Time=0.1619 sec\n",
      "Epoch [1006 / 2000]: Train Loss=5.0314, Val Cor=0.6847, Time=0.1628 sec\n",
      "Epoch [1007 / 2000]: Train Loss=5.1237, Val Cor=0.4610, Time=0.1625 sec\n",
      "Epoch [1008 / 2000]: Train Loss=5.1975, Val Cor=0.6563, Time=0.1623 sec\n",
      "Epoch [1009 / 2000]: Train Loss=5.0336, Val Cor=0.5381, Time=0.1613 sec\n",
      "Epoch [1010 / 2000]: Train Loss=5.5888, Val Cor=0.4953, Time=0.1612 sec\n",
      "Epoch [1011 / 2000]: Train Loss=5.1346, Val Cor=-0.0218, Time=0.1608 sec\n",
      "Epoch [1012 / 2000]: Train Loss=5.6459, Val Cor=-0.1864, Time=0.1614 sec\n",
      "Epoch [1013 / 2000]: Train Loss=5.2046, Val Cor=-0.2740, Time=0.1601 sec\n",
      "Epoch [1014 / 2000]: Train Loss=5.1947, Val Cor=-0.1022, Time=0.1599 sec\n",
      "Epoch [1015 / 2000]: Train Loss=5.4261, Val Cor=0.6566, Time=0.1611 sec\n",
      "Epoch [1016 / 2000]: Train Loss=5.0959, Val Cor=0.3787, Time=0.1614 sec\n",
      "Epoch [1017 / 2000]: Train Loss=5.0873, Val Cor=0.6522, Time=0.1615 sec\n",
      "Epoch [1018 / 2000]: Train Loss=5.0581, Val Cor=0.5641, Time=0.1611 sec\n",
      "Epoch [1019 / 2000]: Train Loss=4.9155, Val Cor=0.6539, Time=0.1606 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1020 / 2000]: Train Loss=5.1011, Val Cor=-0.6372, Time=0.1602 sec\n",
      "Epoch [1021 / 2000]: Train Loss=5.0407, Val Cor=0.6457, Time=0.1597 sec\n",
      "Epoch [1022 / 2000]: Train Loss=4.8983, Val Cor=0.2139, Time=0.1600 sec\n",
      "Epoch [1023 / 2000]: Train Loss=5.0146, Val Cor=0.5423, Time=0.1601 sec\n",
      "Epoch [1024 / 2000]: Train Loss=5.1755, Val Cor=0.6279, Time=0.1607 sec\n",
      "Epoch [1025 / 2000]: Train Loss=4.8753, Val Cor=0.6567, Time=0.1611 sec\n",
      "Epoch [1026 / 2000]: Train Loss=4.8993, Val Cor=0.3961, Time=0.1614 sec\n",
      "Epoch [1027 / 2000]: Train Loss=4.8729, Val Cor=0.6588, Time=0.1615 sec\n",
      "Epoch [1028 / 2000]: Train Loss=5.2768, Val Cor=0.6869, Time=0.1612 sec\n",
      "Epoch [1029 / 2000]: Train Loss=5.0994, Val Cor=0.5439, Time=0.1606 sec\n",
      "Epoch [1030 / 2000]: Train Loss=5.0248, Val Cor=0.6421, Time=0.1617 sec\n",
      "Epoch [1031 / 2000]: Train Loss=5.2261, Val Cor=0.6636, Time=0.1613 sec\n",
      "Epoch [1032 / 2000]: Train Loss=5.2129, Val Cor=0.6547, Time=0.1614 sec\n",
      "Epoch [1033 / 2000]: Train Loss=4.9320, Val Cor=0.6692, Time=0.1589 sec\n",
      "Epoch [1034 / 2000]: Train Loss=5.1559, Val Cor=0.6517, Time=0.1583 sec\n",
      "Epoch [1035 / 2000]: Train Loss=5.0280, Val Cor=0.3038, Time=0.1584 sec\n",
      "Epoch [1036 / 2000]: Train Loss=5.3538, Val Cor=-0.6584, Time=0.1587 sec\n",
      "Epoch [1037 / 2000]: Train Loss=5.0992, Val Cor=0.6239, Time=0.1581 sec\n",
      "Epoch [1038 / 2000]: Train Loss=5.0378, Val Cor=0.6375, Time=0.1593 sec\n",
      "Epoch [1039 / 2000]: Train Loss=5.0281, Val Cor=0.5552, Time=0.1603 sec\n",
      "Epoch [1040 / 2000]: Train Loss=5.0936, Val Cor=0.3518, Time=0.1602 sec\n",
      "Epoch [1041 / 2000]: Train Loss=5.2235, Val Cor=-0.6619, Time=0.2961 sec\n",
      "Epoch [1042 / 2000]: Train Loss=5.0403, Val Cor=0.6031, Time=0.1609 sec\n",
      "Epoch [1043 / 2000]: Train Loss=4.8700, Val Cor=0.6628, Time=0.1630 sec\n",
      "Epoch [1044 / 2000]: Train Loss=4.8852, Val Cor=0.6305, Time=0.1639 sec\n",
      "Epoch [1045 / 2000]: Train Loss=4.9746, Val Cor=0.5102, Time=0.1622 sec\n",
      "Epoch [1046 / 2000]: Train Loss=5.1238, Val Cor=-0.5241, Time=0.1600 sec\n",
      "Epoch [1047 / 2000]: Train Loss=4.9022, Val Cor=0.6692, Time=0.1605 sec\n",
      "Epoch [1048 / 2000]: Train Loss=5.2185, Val Cor=-0.6552, Time=0.1606 sec\n",
      "Epoch [1049 / 2000]: Train Loss=5.3612, Val Cor=0.5654, Time=0.1606 sec\n",
      "Epoch [1050 / 2000]: Train Loss=5.0952, Val Cor=-0.4236, Time=0.1607 sec\n",
      "Epoch [1051 / 2000]: Train Loss=5.3240, Val Cor=-0.5682, Time=0.1624 sec\n",
      "Epoch [1052 / 2000]: Train Loss=5.1335, Val Cor=0.5220, Time=0.1617 sec\n",
      "Epoch [1053 / 2000]: Train Loss=5.2300, Val Cor=-0.1753, Time=0.1620 sec\n",
      "Epoch [1054 / 2000]: Train Loss=5.2624, Val Cor=0.5975, Time=0.1614 sec\n",
      "Epoch [1055 / 2000]: Train Loss=5.6081, Val Cor=-0.6479, Time=0.1600 sec\n",
      "Epoch [1056 / 2000]: Train Loss=5.4923, Val Cor=0.5386, Time=0.1596 sec\n",
      "Epoch [1057 / 2000]: Train Loss=5.2876, Val Cor=0.0540, Time=0.1604 sec\n",
      "Epoch [1058 / 2000]: Train Loss=5.0739, Val Cor=0.6641, Time=0.1602 sec\n",
      "Epoch [1059 / 2000]: Train Loss=4.8734, Val Cor=0.6075, Time=0.1597 sec\n",
      "Epoch [1060 / 2000]: Train Loss=5.0999, Val Cor=-0.5982, Time=0.1612 sec\n",
      "Epoch [1061 / 2000]: Train Loss=5.3638, Val Cor=0.6478, Time=0.1618 sec\n",
      "Epoch [1062 / 2000]: Train Loss=4.9292, Val Cor=0.6591, Time=0.1616 sec\n",
      "Epoch [1063 / 2000]: Train Loss=4.9796, Val Cor=-0.6237, Time=0.1621 sec\n",
      "Epoch [1064 / 2000]: Train Loss=5.1141, Val Cor=0.5729, Time=0.1616 sec\n",
      "Epoch [1065 / 2000]: Train Loss=5.2690, Val Cor=-0.2008, Time=0.1624 sec\n",
      "Epoch [1066 / 2000]: Train Loss=5.1590, Val Cor=-0.5284, Time=0.1622 sec\n",
      "Epoch [1067 / 2000]: Train Loss=5.2946, Val Cor=-0.4602, Time=0.1619 sec\n",
      "Epoch [1068 / 2000]: Train Loss=5.2477, Val Cor=-0.6109, Time=0.1606 sec\n",
      "Epoch [1069 / 2000]: Train Loss=4.9829, Val Cor=-0.6603, Time=0.1605 sec\n",
      "Epoch [1070 / 2000]: Train Loss=4.8139, Val Cor=0.5734, Time=0.1613 sec\n",
      "Epoch [1071 / 2000]: Train Loss=5.0747, Val Cor=0.5425, Time=0.1615 sec\n",
      "Epoch [1072 / 2000]: Train Loss=4.9901, Val Cor=0.6897, Time=0.1609 sec\n",
      "Epoch [1073 / 2000]: Train Loss=5.1428, Val Cor=0.0839, Time=0.1611 sec\n",
      "Epoch [1074 / 2000]: Train Loss=5.1100, Val Cor=-0.0817, Time=0.1609 sec\n",
      "Epoch [1075 / 2000]: Train Loss=5.1459, Val Cor=0.6839, Time=0.1622 sec\n",
      "Epoch [1076 / 2000]: Train Loss=5.1632, Val Cor=0.6569, Time=0.1620 sec\n",
      "Epoch [1077 / 2000]: Train Loss=5.0677, Val Cor=0.6560, Time=0.1624 sec\n",
      "Epoch [1078 / 2000]: Train Loss=5.0744, Val Cor=0.6542, Time=0.1626 sec\n",
      "Epoch [1079 / 2000]: Train Loss=5.0444, Val Cor=0.3586, Time=0.1633 sec\n",
      "Epoch [1080 / 2000]: Train Loss=4.9082, Val Cor=0.5983, Time=0.1637 sec\n",
      "Epoch [1081 / 2000]: Train Loss=5.0742, Val Cor=0.4553, Time=0.1631 sec\n",
      "Epoch [1082 / 2000]: Train Loss=5.0041, Val Cor=0.4475, Time=0.1601 sec\n",
      "Epoch [1083 / 2000]: Train Loss=4.9758, Val Cor=0.6472, Time=0.1598 sec\n",
      "Epoch [1084 / 2000]: Train Loss=4.9475, Val Cor=0.6176, Time=0.1603 sec\n",
      "Epoch [1085 / 2000]: Train Loss=5.0391, Val Cor=-0.1639, Time=0.1608 sec\n",
      "Epoch [1086 / 2000]: Train Loss=4.7255, Val Cor=0.3229, Time=0.1603 sec\n",
      "Epoch [1087 / 2000]: Train Loss=5.0375, Val Cor=0.6888, Time=0.1603 sec\n",
      "Epoch [1088 / 2000]: Train Loss=5.0530, Val Cor=-0.6645, Time=0.1613 sec\n",
      "Epoch [1089 / 2000]: Train Loss=5.0513, Val Cor=0.2886, Time=0.1614 sec\n",
      "Epoch [1090 / 2000]: Train Loss=5.0557, Val Cor=0.6441, Time=0.1612 sec\n",
      "Epoch [1091 / 2000]: Train Loss=4.9850, Val Cor=0.6569, Time=0.1620 sec\n",
      "Epoch [1092 / 2000]: Train Loss=4.7818, Val Cor=0.6678, Time=0.1616 sec\n",
      "Epoch [1093 / 2000]: Train Loss=4.9129, Val Cor=0.6527, Time=0.1616 sec\n",
      "Epoch [1094 / 2000]: Train Loss=4.8851, Val Cor=0.5720, Time=0.1605 sec\n",
      "Epoch [1095 / 2000]: Train Loss=4.9519, Val Cor=0.5889, Time=0.1602 sec\n",
      "Epoch [1096 / 2000]: Train Loss=5.2071, Val Cor=-0.0329, Time=0.1598 sec\n",
      "Epoch [1097 / 2000]: Train Loss=7.5166, Val Cor=-0.1978, Time=0.1591 sec\n",
      "Epoch [1098 / 2000]: Train Loss=5.4777, Val Cor=0.6088, Time=0.1590 sec\n",
      "Epoch [1099 / 2000]: Train Loss=5.1270, Val Cor=0.5390, Time=0.1596 sec\n",
      "Epoch [1100 / 2000]: Train Loss=4.9957, Val Cor=0.5964, Time=0.1604 sec\n",
      "Epoch [1101 / 2000]: Train Loss=4.9686, Val Cor=0.6441, Time=0.1604 sec\n",
      "Epoch [1102 / 2000]: Train Loss=5.0247, Val Cor=0.6750, Time=0.1664 sec\n",
      "Epoch [1103 / 2000]: Train Loss=4.9087, Val Cor=0.6587, Time=0.1689 sec\n",
      "Epoch [1104 / 2000]: Train Loss=5.1379, Val Cor=-0.6429, Time=0.1595 sec\n",
      "Epoch [1105 / 2000]: Train Loss=4.9368, Val Cor=0.5747, Time=0.1586 sec\n",
      "Epoch [1106 / 2000]: Train Loss=5.1412, Val Cor=0.6637, Time=0.1584 sec\n",
      "Epoch [1107 / 2000]: Train Loss=5.1471, Val Cor=0.6897, Time=0.1587 sec\n",
      "Epoch [1108 / 2000]: Train Loss=4.9205, Val Cor=0.5289, Time=0.1592 sec\n",
      "Epoch [1109 / 2000]: Train Loss=4.7970, Val Cor=0.6541, Time=0.1597 sec\n",
      "Epoch [1110 / 2000]: Train Loss=4.9727, Val Cor=0.3082, Time=0.1601 sec\n",
      "Epoch [1111 / 2000]: Train Loss=5.0640, Val Cor=0.6169, Time=0.1608 sec\n",
      "Epoch [1112 / 2000]: Train Loss=5.4936, Val Cor=0.4981, Time=0.1614 sec\n",
      "Epoch [1113 / 2000]: Train Loss=5.0581, Val Cor=0.3765, Time=0.1616 sec\n",
      "Epoch [1114 / 2000]: Train Loss=5.1044, Val Cor=-0.1112, Time=0.1614 sec\n",
      "Epoch [1115 / 2000]: Train Loss=4.9650, Val Cor=0.6718, Time=0.1620 sec\n",
      "Epoch [1116 / 2000]: Train Loss=5.0307, Val Cor=0.6120, Time=0.1609 sec\n",
      "Epoch [1117 / 2000]: Train Loss=5.0071, Val Cor=0.6223, Time=0.1598 sec\n",
      "Epoch [1118 / 2000]: Train Loss=5.2061, Val Cor=0.3349, Time=0.1599 sec\n",
      "Epoch [1119 / 2000]: Train Loss=4.9665, Val Cor=0.6677, Time=0.1600 sec\n",
      "Epoch [1120 / 2000]: Train Loss=4.8387, Val Cor=0.6182, Time=0.1602 sec\n",
      "Epoch [1121 / 2000]: Train Loss=4.9929, Val Cor=0.0174, Time=0.1599 sec\n",
      "Epoch [1122 / 2000]: Train Loss=5.1613, Val Cor=0.5951, Time=0.1613 sec\n",
      "Epoch [1123 / 2000]: Train Loss=5.2489, Val Cor=-0.2020, Time=0.1612 sec\n",
      "Epoch [1124 / 2000]: Train Loss=4.9796, Val Cor=-0.6701, Time=0.1612 sec\n",
      "Epoch [1125 / 2000]: Train Loss=5.3383, Val Cor=0.6595, Time=0.1619 sec\n",
      "Epoch [1126 / 2000]: Train Loss=5.3832, Val Cor=-0.5777, Time=0.1615 sec\n",
      "Epoch [1127 / 2000]: Train Loss=5.1748, Val Cor=-0.6789, Time=0.1599 sec\n",
      "Epoch [1128 / 2000]: Train Loss=5.1627, Val Cor=-0.6744, Time=0.1597 sec\n",
      "Epoch [1129 / 2000]: Train Loss=5.1943, Val Cor=-0.5821, Time=0.1602 sec\n",
      "Epoch [1130 / 2000]: Train Loss=5.0117, Val Cor=-0.4503, Time=0.1603 sec\n",
      "Epoch [1131 / 2000]: Train Loss=5.0815, Val Cor=0.6377, Time=0.1604 sec\n",
      "Epoch [1132 / 2000]: Train Loss=5.1173, Val Cor=0.3896, Time=0.1617 sec\n",
      "Epoch [1133 / 2000]: Train Loss=5.0283, Val Cor=0.2308, Time=0.1615 sec\n",
      "Epoch [1134 / 2000]: Train Loss=5.6345, Val Cor=0.1865, Time=0.1618 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1135 / 2000]: Train Loss=5.4395, Val Cor=-0.4911, Time=0.1615 sec\n",
      "Epoch [1136 / 2000]: Train Loss=5.2076, Val Cor=0.4232, Time=0.1595 sec\n",
      "Epoch [1137 / 2000]: Train Loss=5.2166, Val Cor=-0.2640, Time=0.1588 sec\n",
      "Epoch [1138 / 2000]: Train Loss=5.4763, Val Cor=0.4957, Time=0.1585 sec\n",
      "Epoch [1139 / 2000]: Train Loss=5.2584, Val Cor=-0.4649, Time=0.1590 sec\n",
      "Epoch [1140 / 2000]: Train Loss=5.1217, Val Cor=0.6566, Time=0.1588 sec\n",
      "Epoch [1141 / 2000]: Train Loss=5.1942, Val Cor=-0.1012, Time=0.1587 sec\n",
      "Epoch [1142 / 2000]: Train Loss=5.1695, Val Cor=-0.2328, Time=0.1590 sec\n",
      "Epoch [1143 / 2000]: Train Loss=5.2442, Val Cor=-0.6324, Time=0.1595 sec\n",
      "Epoch [1144 / 2000]: Train Loss=5.2654, Val Cor=-0.2630, Time=0.1605 sec\n",
      "Epoch [1145 / 2000]: Train Loss=5.1021, Val Cor=-0.1290, Time=0.1607 sec\n",
      "Epoch [1146 / 2000]: Train Loss=5.0034, Val Cor=0.6747, Time=0.1603 sec\n",
      "Epoch [1147 / 2000]: Train Loss=5.0011, Val Cor=0.5835, Time=0.1600 sec\n",
      "Epoch [1148 / 2000]: Train Loss=5.0870, Val Cor=0.6169, Time=0.1617 sec\n",
      "Epoch [1149 / 2000]: Train Loss=4.8937, Val Cor=0.1940, Time=0.1629 sec\n",
      "Epoch [1150 / 2000]: Train Loss=4.9238, Val Cor=0.6441, Time=0.1624 sec\n",
      "Epoch [1151 / 2000]: Train Loss=5.0415, Val Cor=0.6535, Time=0.1612 sec\n",
      "Epoch [1152 / 2000]: Train Loss=4.8709, Val Cor=0.6630, Time=0.1608 sec\n",
      "Epoch [1153 / 2000]: Train Loss=4.9411, Val Cor=0.6546, Time=0.1605 sec\n",
      "Epoch [1154 / 2000]: Train Loss=4.9714, Val Cor=0.6916, Time=0.1607 sec\n",
      "Epoch [1155 / 2000]: Train Loss=5.1613, Val Cor=-0.1598, Time=0.1606 sec\n",
      "Epoch [1156 / 2000]: Train Loss=4.9894, Val Cor=0.5926, Time=0.1608 sec\n",
      "Epoch [1157 / 2000]: Train Loss=5.0233, Val Cor=0.2928, Time=0.1611 sec\n",
      "Epoch [1158 / 2000]: Train Loss=5.0372, Val Cor=0.6603, Time=0.1615 sec\n",
      "Epoch [1159 / 2000]: Train Loss=4.8714, Val Cor=0.2951, Time=0.1619 sec\n",
      "Epoch [1160 / 2000]: Train Loss=4.9404, Val Cor=0.6516, Time=0.1620 sec\n",
      "Epoch [1161 / 2000]: Train Loss=4.8499, Val Cor=0.6051, Time=0.1626 sec\n",
      "Epoch [1162 / 2000]: Train Loss=4.7181, Val Cor=0.6557, Time=0.1623 sec\n",
      "Epoch [1163 / 2000]: Train Loss=4.8454, Val Cor=0.6891, Time=0.1610 sec\n",
      "Epoch [1164 / 2000]: Train Loss=4.8663, Val Cor=0.3007, Time=0.1608 sec\n",
      "Epoch [1165 / 2000]: Train Loss=4.9976, Val Cor=0.6560, Time=0.1610 sec\n",
      "Epoch [1166 / 2000]: Train Loss=5.2464, Val Cor=-0.6298, Time=0.1608 sec\n",
      "Epoch [1167 / 2000]: Train Loss=5.1452, Val Cor=-0.0802, Time=0.1617 sec\n",
      "Epoch [1168 / 2000]: Train Loss=4.9566, Val Cor=-0.2914, Time=0.1613 sec\n",
      "Epoch [1169 / 2000]: Train Loss=5.0670, Val Cor=-0.0998, Time=0.1621 sec\n",
      "Epoch [1170 / 2000]: Train Loss=5.1745, Val Cor=0.6571, Time=0.1618 sec\n",
      "Epoch [1171 / 2000]: Train Loss=5.2936, Val Cor=0.6589, Time=0.1619 sec\n",
      "Epoch [1172 / 2000]: Train Loss=5.0437, Val Cor=-0.5543, Time=0.1598 sec\n",
      "Epoch [1173 / 2000]: Train Loss=4.9745, Val Cor=0.3853, Time=0.1594 sec\n",
      "Epoch [1174 / 2000]: Train Loss=4.9194, Val Cor=-0.3680, Time=0.1599 sec\n",
      "Epoch [1175 / 2000]: Train Loss=5.1166, Val Cor=-0.3622, Time=0.1598 sec\n",
      "Epoch [1176 / 2000]: Train Loss=5.3023, Val Cor=-0.6469, Time=0.1602 sec\n",
      "Epoch [1177 / 2000]: Train Loss=5.2904, Val Cor=0.2753, Time=0.1615 sec\n",
      "Epoch [1178 / 2000]: Train Loss=5.1845, Val Cor=0.0936, Time=0.1613 sec\n",
      "Epoch [1179 / 2000]: Train Loss=5.0354, Val Cor=0.4666, Time=0.1618 sec\n",
      "Epoch [1180 / 2000]: Train Loss=4.8604, Val Cor=0.6452, Time=0.1616 sec\n",
      "Epoch [1181 / 2000]: Train Loss=5.1043, Val Cor=0.6502, Time=0.1617 sec\n",
      "Epoch [1182 / 2000]: Train Loss=4.9483, Val Cor=0.3974, Time=0.1597 sec\n",
      "Epoch [1183 / 2000]: Train Loss=4.9605, Val Cor=-0.6636, Time=0.1601 sec\n",
      "Epoch [1184 / 2000]: Train Loss=5.0468, Val Cor=0.6119, Time=0.1598 sec\n",
      "Epoch [1185 / 2000]: Train Loss=4.8909, Val Cor=0.5839, Time=0.1596 sec\n",
      "Epoch [1186 / 2000]: Train Loss=5.0883, Val Cor=-0.4188, Time=0.1614 sec\n",
      "Epoch [1187 / 2000]: Train Loss=5.0671, Val Cor=-0.4064, Time=0.1612 sec\n",
      "Epoch [1188 / 2000]: Train Loss=5.3934, Val Cor=-0.3665, Time=0.1616 sec\n",
      "Epoch [1189 / 2000]: Train Loss=5.1777, Val Cor=-0.1752, Time=0.1616 sec\n",
      "Epoch [1190 / 2000]: Train Loss=5.1527, Val Cor=0.3263, Time=0.1600 sec\n",
      "Epoch [1191 / 2000]: Train Loss=5.1122, Val Cor=0.0913, Time=0.1597 sec\n",
      "Epoch [1192 / 2000]: Train Loss=5.3696, Val Cor=0.4696, Time=0.1598 sec\n",
      "Epoch [1193 / 2000]: Train Loss=5.0223, Val Cor=-0.0765, Time=0.1599 sec\n",
      "Epoch [1194 / 2000]: Train Loss=5.2016, Val Cor=0.6756, Time=0.1600 sec\n",
      "Epoch [1195 / 2000]: Train Loss=4.9231, Val Cor=0.6676, Time=0.1614 sec\n",
      "Epoch [1196 / 2000]: Train Loss=4.8613, Val Cor=0.6712, Time=0.1611 sec\n",
      "Epoch [1197 / 2000]: Train Loss=4.7096, Val Cor=0.6303, Time=0.1619 sec\n",
      "Epoch [1198 / 2000]: Train Loss=4.9489, Val Cor=-0.3371, Time=0.1612 sec\n",
      "Epoch [1199 / 2000]: Train Loss=4.9154, Val Cor=0.5312, Time=0.1598 sec\n",
      "Epoch [1200 / 2000]: Train Loss=5.0334, Val Cor=-0.5876, Time=0.1598 sec\n",
      "Epoch [1201 / 2000]: Train Loss=5.3551, Val Cor=0.3777, Time=0.1600 sec\n",
      "Epoch [1202 / 2000]: Train Loss=5.3125, Val Cor=0.5914, Time=0.1597 sec\n",
      "Epoch [1203 / 2000]: Train Loss=5.0141, Val Cor=0.5712, Time=0.1600 sec\n",
      "Epoch [1204 / 2000]: Train Loss=4.9280, Val Cor=0.4886, Time=0.1615 sec\n",
      "Epoch [1205 / 2000]: Train Loss=5.1664, Val Cor=-0.4278, Time=0.1616 sec\n",
      "Epoch [1206 / 2000]: Train Loss=5.3196, Val Cor=-0.0037, Time=0.1630 sec\n",
      "Epoch [1207 / 2000]: Train Loss=5.1664, Val Cor=-0.6570, Time=0.1621 sec\n",
      "Epoch [1208 / 2000]: Train Loss=4.9168, Val Cor=0.6680, Time=0.1606 sec\n",
      "Epoch [1209 / 2000]: Train Loss=5.0918, Val Cor=-0.0078, Time=0.1604 sec\n",
      "Epoch [1210 / 2000]: Train Loss=5.1181, Val Cor=0.6423, Time=0.1605 sec\n",
      "Epoch [1211 / 2000]: Train Loss=4.7373, Val Cor=0.4786, Time=0.1607 sec\n",
      "Epoch [1212 / 2000]: Train Loss=4.9750, Val Cor=0.5772, Time=0.1600 sec\n",
      "Epoch [1213 / 2000]: Train Loss=4.8195, Val Cor=0.5932, Time=0.1614 sec\n",
      "Epoch [1214 / 2000]: Train Loss=5.0560, Val Cor=0.4560, Time=0.1613 sec\n",
      "Epoch [1215 / 2000]: Train Loss=5.0896, Val Cor=0.1757, Time=0.1611 sec\n",
      "Epoch [1216 / 2000]: Train Loss=4.7762, Val Cor=-0.5516, Time=0.1618 sec\n",
      "Epoch [1217 / 2000]: Train Loss=5.0065, Val Cor=0.6448, Time=0.1616 sec\n",
      "Epoch [1218 / 2000]: Train Loss=4.8834, Val Cor=-0.1028, Time=0.1604 sec\n",
      "Epoch [1219 / 2000]: Train Loss=5.1550, Val Cor=-0.5443, Time=0.1604 sec\n",
      "Epoch [1220 / 2000]: Train Loss=5.2111, Val Cor=0.6590, Time=0.1596 sec\n",
      "Epoch [1221 / 2000]: Train Loss=5.2149, Val Cor=0.6129, Time=0.1600 sec\n",
      "Epoch [1222 / 2000]: Train Loss=5.1769, Val Cor=0.6609, Time=0.1586 sec\n",
      "Epoch [1223 / 2000]: Train Loss=5.0348, Val Cor=-0.1564, Time=0.1591 sec\n",
      "Epoch [1224 / 2000]: Train Loss=4.8773, Val Cor=0.6342, Time=0.1605 sec\n",
      "Epoch [1225 / 2000]: Train Loss=4.7646, Val Cor=0.6612, Time=0.1600 sec\n",
      "Epoch [1226 / 2000]: Train Loss=4.7702, Val Cor=0.6031, Time=0.1604 sec\n",
      "Epoch [1227 / 2000]: Train Loss=4.7355, Val Cor=0.6370, Time=0.1606 sec\n",
      "Epoch [1228 / 2000]: Train Loss=4.9849, Val Cor=0.3533, Time=0.1606 sec\n",
      "Epoch [1229 / 2000]: Train Loss=5.1999, Val Cor=0.2344, Time=0.1601 sec\n",
      "Epoch [1230 / 2000]: Train Loss=4.9751, Val Cor=0.5451, Time=0.1583 sec\n",
      "Epoch [1231 / 2000]: Train Loss=5.0167, Val Cor=0.5379, Time=0.1588 sec\n",
      "Epoch [1232 / 2000]: Train Loss=4.9965, Val Cor=0.5741, Time=0.1589 sec\n",
      "Epoch [1233 / 2000]: Train Loss=4.9218, Val Cor=0.6137, Time=0.1604 sec\n",
      "Epoch [1234 / 2000]: Train Loss=4.9490, Val Cor=-0.1877, Time=0.1604 sec\n",
      "Epoch [1235 / 2000]: Train Loss=5.0095, Val Cor=-0.0845, Time=0.1617 sec\n",
      "Epoch [1236 / 2000]: Train Loss=4.8597, Val Cor=0.1805, Time=0.1612 sec\n",
      "Epoch [1237 / 2000]: Train Loss=4.7643, Val Cor=0.6596, Time=0.1622 sec\n",
      "Epoch [1238 / 2000]: Train Loss=4.7799, Val Cor=0.6642, Time=0.1624 sec\n",
      "Epoch [1239 / 2000]: Train Loss=4.9062, Val Cor=0.5793, Time=0.1616 sec\n",
      "Epoch [1240 / 2000]: Train Loss=5.0966, Val Cor=0.4059, Time=0.1610 sec\n",
      "Epoch [1241 / 2000]: Train Loss=5.1313, Val Cor=0.3913, Time=0.1610 sec\n",
      "Epoch [1242 / 2000]: Train Loss=4.9418, Val Cor=0.5386, Time=0.1603 sec\n",
      "Epoch [1243 / 2000]: Train Loss=4.9799, Val Cor=0.6486, Time=0.1606 sec\n",
      "Epoch [1244 / 2000]: Train Loss=4.7479, Val Cor=0.5841, Time=0.1604 sec\n",
      "Epoch [1245 / 2000]: Train Loss=4.8690, Val Cor=0.6232, Time=0.1620 sec\n",
      "Epoch [1246 / 2000]: Train Loss=4.7683, Val Cor=0.6622, Time=0.1619 sec\n",
      "Epoch [1247 / 2000]: Train Loss=4.9697, Val Cor=-0.2223, Time=0.1620 sec\n",
      "Epoch [1248 / 2000]: Train Loss=4.6481, Val Cor=0.6361, Time=0.1608 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1249 / 2000]: Train Loss=4.7369, Val Cor=-0.4596, Time=0.1622 sec\n",
      "Epoch [1250 / 2000]: Train Loss=4.7004, Val Cor=-0.6460, Time=0.1620 sec\n",
      "Epoch [1251 / 2000]: Train Loss=4.8170, Val Cor=0.5172, Time=0.1618 sec\n",
      "Epoch [1252 / 2000]: Train Loss=4.8133, Val Cor=0.2983, Time=0.1603 sec\n",
      "Epoch [1253 / 2000]: Train Loss=4.8837, Val Cor=0.5987, Time=0.1603 sec\n",
      "Epoch [1254 / 2000]: Train Loss=4.8657, Val Cor=-0.1300, Time=0.1605 sec\n",
      "Epoch [1255 / 2000]: Train Loss=4.9228, Val Cor=0.3148, Time=0.1602 sec\n",
      "Epoch [1256 / 2000]: Train Loss=4.9344, Val Cor=-0.2422, Time=0.1602 sec\n",
      "Epoch [1257 / 2000]: Train Loss=5.0334, Val Cor=0.5954, Time=0.1611 sec\n",
      "Epoch [1258 / 2000]: Train Loss=5.1635, Val Cor=0.6219, Time=0.1612 sec\n",
      "Epoch [1259 / 2000]: Train Loss=5.0179, Val Cor=0.6483, Time=0.1617 sec\n",
      "Epoch [1260 / 2000]: Train Loss=4.9135, Val Cor=0.6417, Time=0.1617 sec\n",
      "Epoch [1261 / 2000]: Train Loss=4.9938, Val Cor=0.4303, Time=0.1624 sec\n",
      "Epoch [1262 / 2000]: Train Loss=4.7628, Val Cor=0.6558, Time=0.1618 sec\n",
      "Epoch [1263 / 2000]: Train Loss=4.7931, Val Cor=-0.6289, Time=0.1618 sec\n",
      "Epoch [1264 / 2000]: Train Loss=5.1709, Val Cor=0.2022, Time=0.1598 sec\n",
      "Epoch [1265 / 2000]: Train Loss=5.1801, Val Cor=0.5574, Time=0.1597 sec\n",
      "Epoch [1266 / 2000]: Train Loss=5.2315, Val Cor=-0.0421, Time=0.1602 sec\n",
      "Epoch [1267 / 2000]: Train Loss=5.1390, Val Cor=-0.4060, Time=0.1587 sec\n",
      "Epoch [1268 / 2000]: Train Loss=4.9531, Val Cor=0.6637, Time=0.1586 sec\n",
      "Epoch [1269 / 2000]: Train Loss=4.8912, Val Cor=-0.6086, Time=0.1591 sec\n",
      "Epoch [1270 / 2000]: Train Loss=5.1719, Val Cor=0.5227, Time=0.1604 sec\n",
      "Epoch [1271 / 2000]: Train Loss=5.0961, Val Cor=-0.0524, Time=0.1605 sec\n",
      "Epoch [1272 / 2000]: Train Loss=5.3298, Val Cor=0.6401, Time=0.1600 sec\n",
      "Epoch [1273 / 2000]: Train Loss=5.1901, Val Cor=-0.5735, Time=0.1606 sec\n",
      "Epoch [1274 / 2000]: Train Loss=5.1717, Val Cor=-0.5573, Time=0.1606 sec\n",
      "Epoch [1275 / 2000]: Train Loss=5.0335, Val Cor=-0.1445, Time=0.1607 sec\n",
      "Epoch [1276 / 2000]: Train Loss=5.2898, Val Cor=0.6734, Time=0.1590 sec\n",
      "Epoch [1277 / 2000]: Train Loss=5.0644, Val Cor=0.4508, Time=0.1588 sec\n",
      "Epoch [1278 / 2000]: Train Loss=4.9625, Val Cor=-0.2686, Time=0.1601 sec\n",
      "Epoch [1279 / 2000]: Train Loss=4.9540, Val Cor=0.0691, Time=0.1605 sec\n",
      "Epoch [1280 / 2000]: Train Loss=5.0873, Val Cor=0.1300, Time=0.1607 sec\n",
      "Epoch [1281 / 2000]: Train Loss=5.0395, Val Cor=0.5555, Time=0.1605 sec\n",
      "Epoch [1282 / 2000]: Train Loss=5.0453, Val Cor=-0.4784, Time=0.1606 sec\n",
      "Epoch [1283 / 2000]: Train Loss=4.9463, Val Cor=0.3070, Time=0.1619 sec\n",
      "Epoch [1284 / 2000]: Train Loss=4.8024, Val Cor=0.6659, Time=0.1621 sec\n",
      "Epoch [1285 / 2000]: Train Loss=4.8441, Val Cor=0.3813, Time=0.1621 sec\n",
      "Epoch [1286 / 2000]: Train Loss=4.9951, Val Cor=0.3046, Time=0.1623 sec\n",
      "Epoch [1287 / 2000]: Train Loss=4.8796, Val Cor=0.6655, Time=0.1622 sec\n",
      "Epoch [1288 / 2000]: Train Loss=4.8887, Val Cor=-0.2760, Time=0.1615 sec\n",
      "Epoch [1289 / 2000]: Train Loss=4.9466, Val Cor=0.0578, Time=0.1610 sec\n",
      "Epoch [1290 / 2000]: Train Loss=4.9954, Val Cor=0.4033, Time=0.1605 sec\n",
      "Epoch [1291 / 2000]: Train Loss=4.7309, Val Cor=0.6512, Time=0.1613 sec\n",
      "Epoch [1292 / 2000]: Train Loss=4.6417, Val Cor=0.6632, Time=0.1603 sec\n",
      "Epoch [1293 / 2000]: Train Loss=4.7937, Val Cor=0.6559, Time=0.1600 sec\n",
      "Epoch [1294 / 2000]: Train Loss=4.8652, Val Cor=0.3391, Time=0.1607 sec\n",
      "Epoch [1295 / 2000]: Train Loss=5.0232, Val Cor=-0.0254, Time=0.1622 sec\n",
      "Epoch [1296 / 2000]: Train Loss=5.1937, Val Cor=-0.4337, Time=0.1620 sec\n",
      "Epoch [1297 / 2000]: Train Loss=4.9973, Val Cor=-0.2945, Time=0.1623 sec\n",
      "Epoch [1298 / 2000]: Train Loss=4.9158, Val Cor=-0.6782, Time=0.1607 sec\n",
      "Epoch [1299 / 2000]: Train Loss=5.0903, Val Cor=0.4609, Time=0.1605 sec\n",
      "Epoch [1300 / 2000]: Train Loss=5.2435, Val Cor=0.3531, Time=0.1605 sec\n",
      "Epoch [1301 / 2000]: Train Loss=5.2318, Val Cor=-0.5114, Time=0.1608 sec\n",
      "Epoch [1302 / 2000]: Train Loss=5.0672, Val Cor=-0.4315, Time=0.1601 sec\n",
      "Epoch [1303 / 2000]: Train Loss=4.8553, Val Cor=0.6646, Time=0.1601 sec\n",
      "Epoch [1304 / 2000]: Train Loss=5.1387, Val Cor=0.4293, Time=0.1617 sec\n",
      "Epoch [1305 / 2000]: Train Loss=5.0512, Val Cor=0.0056, Time=0.1616 sec\n",
      "Epoch [1306 / 2000]: Train Loss=4.9626, Val Cor=0.4818, Time=0.1617 sec\n",
      "Epoch [1307 / 2000]: Train Loss=4.8241, Val Cor=0.2904, Time=0.1619 sec\n",
      "Epoch [1308 / 2000]: Train Loss=4.9870, Val Cor=0.6334, Time=0.1619 sec\n",
      "Epoch [1309 / 2000]: Train Loss=4.7967, Val Cor=0.6427, Time=0.1601 sec\n",
      "Epoch [1310 / 2000]: Train Loss=4.9727, Val Cor=0.1787, Time=0.1596 sec\n",
      "Epoch [1311 / 2000]: Train Loss=4.7766, Val Cor=0.5091, Time=0.1601 sec\n",
      "Epoch [1312 / 2000]: Train Loss=4.9083, Val Cor=0.6357, Time=0.1596 sec\n",
      "Epoch [1313 / 2000]: Train Loss=5.0113, Val Cor=0.5133, Time=0.1600 sec\n",
      "Epoch [1314 / 2000]: Train Loss=5.1575, Val Cor=0.6265, Time=0.1595 sec\n",
      "Epoch [1315 / 2000]: Train Loss=5.1729, Val Cor=-0.0370, Time=0.1600 sec\n",
      "Epoch [1316 / 2000]: Train Loss=5.1268, Val Cor=-0.1540, Time=0.1600 sec\n",
      "Epoch [1317 / 2000]: Train Loss=4.9797, Val Cor=0.5643, Time=0.1604 sec\n",
      "Epoch [1318 / 2000]: Train Loss=4.7344, Val Cor=0.6901, Time=0.1615 sec\n",
      "Epoch [1319 / 2000]: Train Loss=5.1547, Val Cor=-0.1079, Time=0.1615 sec\n",
      "Epoch [1320 / 2000]: Train Loss=5.0927, Val Cor=0.6543, Time=0.1614 sec\n",
      "Epoch [1321 / 2000]: Train Loss=4.8200, Val Cor=0.1572, Time=0.1621 sec\n",
      "Epoch [1322 / 2000]: Train Loss=4.8595, Val Cor=0.4319, Time=0.1620 sec\n",
      "Epoch [1323 / 2000]: Train Loss=4.9300, Val Cor=0.3141, Time=0.1616 sec\n",
      "Epoch [1324 / 2000]: Train Loss=5.0736, Val Cor=-0.5756, Time=0.1607 sec\n",
      "Epoch [1325 / 2000]: Train Loss=4.9454, Val Cor=0.6525, Time=0.1618 sec\n",
      "Epoch [1326 / 2000]: Train Loss=4.8559, Val Cor=0.1950, Time=0.1614 sec\n",
      "Epoch [1327 / 2000]: Train Loss=4.9014, Val Cor=0.2939, Time=0.1617 sec\n",
      "Epoch [1328 / 2000]: Train Loss=5.0190, Val Cor=0.5678, Time=0.1614 sec\n",
      "Epoch [1329 / 2000]: Train Loss=4.9390, Val Cor=0.5525, Time=0.1615 sec\n",
      "Epoch [1330 / 2000]: Train Loss=4.9181, Val Cor=0.0984, Time=0.1604 sec\n",
      "Epoch [1331 / 2000]: Train Loss=4.8849, Val Cor=0.6646, Time=0.1598 sec\n",
      "Epoch [1332 / 2000]: Train Loss=4.8366, Val Cor=0.4057, Time=0.1583 sec\n",
      "Epoch [1333 / 2000]: Train Loss=4.9129, Val Cor=0.6409, Time=0.1593 sec\n",
      "Epoch [1334 / 2000]: Train Loss=4.7736, Val Cor=0.2623, Time=0.1586 sec\n",
      "Epoch [1335 / 2000]: Train Loss=4.7587, Val Cor=0.5537, Time=0.1587 sec\n",
      "Epoch [1336 / 2000]: Train Loss=4.8236, Val Cor=0.3409, Time=0.1605 sec\n",
      "Epoch [1337 / 2000]: Train Loss=5.0029, Val Cor=0.5101, Time=0.1603 sec\n",
      "Epoch [1338 / 2000]: Train Loss=4.8361, Val Cor=0.1047, Time=0.1597 sec\n",
      "Epoch [1339 / 2000]: Train Loss=4.9759, Val Cor=0.4024, Time=0.1603 sec\n",
      "Epoch [1340 / 2000]: Train Loss=4.8823, Val Cor=0.5395, Time=0.1605 sec\n",
      "Epoch [1341 / 2000]: Train Loss=4.7979, Val Cor=0.5324, Time=0.1590 sec\n",
      "Epoch [1342 / 2000]: Train Loss=4.7469, Val Cor=0.6663, Time=0.1586 sec\n",
      "Epoch [1343 / 2000]: Train Loss=4.7033, Val Cor=0.3159, Time=0.1603 sec\n",
      "Epoch [1344 / 2000]: Train Loss=4.8748, Val Cor=0.6242, Time=0.1606 sec\n",
      "Epoch [1345 / 2000]: Train Loss=4.9652, Val Cor=0.5846, Time=0.1607 sec\n",
      "Epoch [1346 / 2000]: Train Loss=5.1737, Val Cor=-0.6036, Time=0.1602 sec\n",
      "Epoch [1347 / 2000]: Train Loss=4.9456, Val Cor=0.6412, Time=0.1611 sec\n",
      "Epoch [1348 / 2000]: Train Loss=4.7689, Val Cor=-0.2651, Time=0.1618 sec\n",
      "Epoch [1349 / 2000]: Train Loss=4.7578, Val Cor=0.6505, Time=0.1621 sec\n",
      "Epoch [1350 / 2000]: Train Loss=4.7467, Val Cor=0.2807, Time=0.1622 sec\n",
      "Epoch [1351 / 2000]: Train Loss=4.7810, Val Cor=0.6687, Time=0.1627 sec\n",
      "Epoch [1352 / 2000]: Train Loss=4.8672, Val Cor=0.4244, Time=0.1621 sec\n",
      "Epoch [1353 / 2000]: Train Loss=4.8194, Val Cor=0.4535, Time=0.1618 sec\n",
      "Epoch [1354 / 2000]: Train Loss=4.9363, Val Cor=0.3219, Time=0.1609 sec\n",
      "Early stopping triggered.\n",
      "Training RNN model 3:\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1 / 2000]: Train Loss=8.4421, Val Cor=-0.0307, Time=0.1613 sec\n",
      "Epoch [2 / 2000]: Train Loss=8.2304, Val Cor=0.3810, Time=0.1608 sec\n",
      "Epoch [3 / 2000]: Train Loss=8.2289, Val Cor=0.5197, Time=0.1600 sec\n",
      "Epoch [4 / 2000]: Train Loss=8.1098, Val Cor=0.5351, Time=0.1599 sec\n",
      "Epoch [5 / 2000]: Train Loss=8.2339, Val Cor=0.5140, Time=0.1604 sec\n",
      "Epoch [6 / 2000]: Train Loss=8.1843, Val Cor=0.4934, Time=0.1601 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7 / 2000]: Train Loss=8.1228, Val Cor=0.4787, Time=0.1598 sec\n",
      "Epoch [8 / 2000]: Train Loss=8.2847, Val Cor=0.4699, Time=0.1601 sec\n",
      "Epoch [9 / 2000]: Train Loss=8.1392, Val Cor=0.5294, Time=0.1614 sec\n",
      "Epoch [10 / 2000]: Train Loss=8.1827, Val Cor=0.5142, Time=0.1609 sec\n",
      "Epoch [11 / 2000]: Train Loss=8.1206, Val Cor=0.5244, Time=0.1612 sec\n",
      "Epoch [12 / 2000]: Train Loss=8.0906, Val Cor=0.4865, Time=0.1616 sec\n",
      "Epoch [13 / 2000]: Train Loss=8.1210, Val Cor=0.4799, Time=0.1614 sec\n",
      "Epoch [14 / 2000]: Train Loss=8.1051, Val Cor=0.5165, Time=0.1596 sec\n",
      "Epoch [15 / 2000]: Train Loss=8.0706, Val Cor=0.5283, Time=0.1596 sec\n",
      "Epoch [16 / 2000]: Train Loss=8.0574, Val Cor=0.5529, Time=0.1595 sec\n",
      "Epoch [17 / 2000]: Train Loss=7.9711, Val Cor=0.5529, Time=0.1594 sec\n",
      "Epoch [18 / 2000]: Train Loss=8.0985, Val Cor=0.5632, Time=0.1593 sec\n",
      "Epoch [19 / 2000]: Train Loss=7.9425, Val Cor=0.5821, Time=0.1594 sec\n",
      "Epoch [20 / 2000]: Train Loss=7.9089, Val Cor=0.5926, Time=0.1596 sec\n",
      "Epoch [21 / 2000]: Train Loss=7.8430, Val Cor=0.6063, Time=0.1604 sec\n",
      "Epoch [22 / 2000]: Train Loss=8.0055, Val Cor=0.6259, Time=0.1599 sec\n",
      "Epoch [23 / 2000]: Train Loss=7.8077, Val Cor=0.6338, Time=0.1602 sec\n",
      "Epoch [24 / 2000]: Train Loss=7.7636, Val Cor=0.6397, Time=0.1601 sec\n",
      "Epoch [25 / 2000]: Train Loss=7.5843, Val Cor=0.6413, Time=0.1603 sec\n",
      "Epoch [26 / 2000]: Train Loss=7.6099, Val Cor=0.6516, Time=0.1601 sec\n",
      "Epoch [27 / 2000]: Train Loss=7.2786, Val Cor=0.6595, Time=0.1606 sec\n",
      "Epoch [28 / 2000]: Train Loss=7.5510, Val Cor=0.6687, Time=0.1599 sec\n",
      "Epoch [29 / 2000]: Train Loss=7.5841, Val Cor=0.6736, Time=0.1602 sec\n",
      "Epoch [30 / 2000]: Train Loss=7.4876, Val Cor=0.6759, Time=0.1599 sec\n",
      "Epoch [31 / 2000]: Train Loss=7.3181, Val Cor=0.6812, Time=0.1601 sec\n",
      "Epoch [32 / 2000]: Train Loss=7.3519, Val Cor=0.6851, Time=0.1598 sec\n",
      "Epoch [33 / 2000]: Train Loss=7.5140, Val Cor=0.6906, Time=0.1602 sec\n",
      "Epoch [34 / 2000]: Train Loss=7.6184, Val Cor=0.6907, Time=0.1597 sec\n",
      "Epoch [35 / 2000]: Train Loss=7.8621, Val Cor=0.6909, Time=0.1598 sec\n",
      "Epoch [36 / 2000]: Train Loss=7.4924, Val Cor=0.6904, Time=0.1598 sec\n",
      "Epoch [37 / 2000]: Train Loss=7.4718, Val Cor=0.6927, Time=0.1604 sec\n",
      "Epoch [38 / 2000]: Train Loss=7.2055, Val Cor=0.6923, Time=0.1593 sec\n",
      "Epoch [39 / 2000]: Train Loss=7.3015, Val Cor=0.6942, Time=0.1610 sec\n",
      "Epoch [40 / 2000]: Train Loss=7.4993, Val Cor=0.6975, Time=0.1615 sec\n",
      "Epoch [41 / 2000]: Train Loss=7.2485, Val Cor=0.6980, Time=0.1614 sec\n",
      "Epoch [42 / 2000]: Train Loss=7.5530, Val Cor=0.6980, Time=0.1614 sec\n",
      "Epoch [43 / 2000]: Train Loss=7.4072, Val Cor=0.6967, Time=0.1621 sec\n",
      "Epoch [44 / 2000]: Train Loss=7.1178, Val Cor=0.6986, Time=0.1617 sec\n",
      "Epoch [45 / 2000]: Train Loss=7.4384, Val Cor=0.7003, Time=0.1619 sec\n",
      "Epoch [46 / 2000]: Train Loss=7.1864, Val Cor=0.7006, Time=0.1613 sec\n",
      "Epoch [47 / 2000]: Train Loss=7.2767, Val Cor=0.7005, Time=0.1617 sec\n",
      "Epoch [48 / 2000]: Train Loss=7.2080, Val Cor=0.7022, Time=0.1616 sec\n",
      "Epoch [49 / 2000]: Train Loss=7.3671, Val Cor=0.7013, Time=0.1614 sec\n",
      "Epoch [50 / 2000]: Train Loss=7.0618, Val Cor=0.7013, Time=0.1614 sec\n",
      "Epoch [51 / 2000]: Train Loss=6.9800, Val Cor=0.7028, Time=0.1598 sec\n",
      "Epoch [52 / 2000]: Train Loss=7.1925, Val Cor=0.7059, Time=0.1596 sec\n",
      "Epoch [53 / 2000]: Train Loss=7.0019, Val Cor=0.7063, Time=0.1597 sec\n",
      "Epoch [54 / 2000]: Train Loss=6.8327, Val Cor=0.7060, Time=0.1589 sec\n",
      "Epoch [55 / 2000]: Train Loss=7.0876, Val Cor=0.7062, Time=0.1596 sec\n",
      "Epoch [56 / 2000]: Train Loss=6.9677, Val Cor=0.7065, Time=0.1596 sec\n",
      "Epoch [57 / 2000]: Train Loss=7.2646, Val Cor=0.7073, Time=0.1593 sec\n",
      "Epoch [58 / 2000]: Train Loss=6.8487, Val Cor=0.7064, Time=0.1591 sec\n",
      "Epoch [59 / 2000]: Train Loss=7.4996, Val Cor=0.7052, Time=0.1597 sec\n",
      "Epoch [60 / 2000]: Train Loss=7.3285, Val Cor=0.7050, Time=0.1612 sec\n",
      "Epoch [61 / 2000]: Train Loss=7.3857, Val Cor=0.7069, Time=0.1612 sec\n",
      "Epoch [62 / 2000]: Train Loss=6.9328, Val Cor=0.7071, Time=0.1614 sec\n",
      "Epoch [63 / 2000]: Train Loss=7.3399, Val Cor=0.7103, Time=0.1616 sec\n",
      "Epoch [64 / 2000]: Train Loss=7.3790, Val Cor=0.7101, Time=0.1612 sec\n",
      "Epoch [65 / 2000]: Train Loss=7.0589, Val Cor=0.7101, Time=0.1616 sec\n",
      "Epoch [66 / 2000]: Train Loss=7.2988, Val Cor=0.7100, Time=0.1604 sec\n",
      "Epoch [67 / 2000]: Train Loss=7.0359, Val Cor=0.7093, Time=0.1604 sec\n",
      "Epoch [68 / 2000]: Train Loss=6.8881, Val Cor=0.7094, Time=0.1590 sec\n",
      "Epoch [69 / 2000]: Train Loss=7.2432, Val Cor=0.7094, Time=0.1600 sec\n",
      "Epoch [70 / 2000]: Train Loss=6.9906, Val Cor=0.7120, Time=0.1598 sec\n",
      "Epoch [71 / 2000]: Train Loss=7.0151, Val Cor=0.7108, Time=0.1593 sec\n",
      "Epoch [72 / 2000]: Train Loss=6.8629, Val Cor=0.7115, Time=0.1598 sec\n",
      "Epoch [73 / 2000]: Train Loss=6.8286, Val Cor=0.7100, Time=0.1596 sec\n",
      "Epoch [74 / 2000]: Train Loss=6.9370, Val Cor=0.7092, Time=0.1598 sec\n",
      "Epoch [75 / 2000]: Train Loss=6.8454, Val Cor=0.7103, Time=0.1602 sec\n",
      "Epoch [76 / 2000]: Train Loss=6.8938, Val Cor=0.7088, Time=0.1609 sec\n",
      "Epoch [77 / 2000]: Train Loss=7.0591, Val Cor=0.7093, Time=0.1612 sec\n",
      "Epoch [78 / 2000]: Train Loss=6.8260, Val Cor=0.7091, Time=0.1607 sec\n",
      "Epoch [79 / 2000]: Train Loss=7.0512, Val Cor=0.7092, Time=0.1617 sec\n",
      "Epoch [80 / 2000]: Train Loss=7.0315, Val Cor=0.7106, Time=0.1613 sec\n",
      "Epoch [81 / 2000]: Train Loss=6.9723, Val Cor=0.7105, Time=0.1603 sec\n",
      "Epoch [82 / 2000]: Train Loss=6.9844, Val Cor=0.7103, Time=0.1598 sec\n",
      "Epoch [83 / 2000]: Train Loss=7.1280, Val Cor=0.7093, Time=0.1598 sec\n",
      "Epoch [84 / 2000]: Train Loss=6.8884, Val Cor=0.7082, Time=0.1599 sec\n",
      "Epoch [85 / 2000]: Train Loss=6.9373, Val Cor=0.7076, Time=0.1603 sec\n",
      "Epoch [86 / 2000]: Train Loss=6.7368, Val Cor=0.7094, Time=0.1600 sec\n",
      "Epoch [87 / 2000]: Train Loss=7.0292, Val Cor=0.7098, Time=0.1598 sec\n",
      "Epoch [88 / 2000]: Train Loss=6.7738, Val Cor=0.7097, Time=0.1601 sec\n",
      "Epoch [89 / 2000]: Train Loss=6.6220, Val Cor=0.7092, Time=0.1617 sec\n",
      "Epoch [90 / 2000]: Train Loss=6.6166, Val Cor=0.7086, Time=0.1611 sec\n",
      "Epoch [91 / 2000]: Train Loss=6.7163, Val Cor=0.7100, Time=0.1619 sec\n",
      "Epoch [92 / 2000]: Train Loss=6.5704, Val Cor=0.7100, Time=0.1621 sec\n",
      "Epoch [93 / 2000]: Train Loss=6.5657, Val Cor=0.7099, Time=0.1620 sec\n",
      "Epoch [94 / 2000]: Train Loss=6.6305, Val Cor=0.7116, Time=0.1609 sec\n",
      "Epoch [95 / 2000]: Train Loss=6.7527, Val Cor=0.7092, Time=0.1607 sec\n",
      "Epoch [96 / 2000]: Train Loss=6.6876, Val Cor=0.7081, Time=0.1603 sec\n",
      "Epoch [97 / 2000]: Train Loss=6.5494, Val Cor=0.7072, Time=0.1606 sec\n",
      "Epoch [98 / 2000]: Train Loss=6.5089, Val Cor=0.7082, Time=0.1599 sec\n",
      "Epoch [99 / 2000]: Train Loss=6.3864, Val Cor=0.7104, Time=0.1598 sec\n",
      "Epoch [100 / 2000]: Train Loss=6.4877, Val Cor=0.7057, Time=0.1604 sec\n",
      "Epoch [101 / 2000]: Train Loss=6.6929, Val Cor=0.7068, Time=0.1617 sec\n",
      "Epoch [102 / 2000]: Train Loss=6.5732, Val Cor=0.7083, Time=0.1612 sec\n",
      "Epoch [103 / 2000]: Train Loss=6.4911, Val Cor=0.7052, Time=0.1615 sec\n",
      "Epoch [104 / 2000]: Train Loss=6.4147, Val Cor=0.7056, Time=0.1617 sec\n",
      "Epoch [105 / 2000]: Train Loss=6.4026, Val Cor=0.7045, Time=0.1614 sec\n",
      "Epoch [106 / 2000]: Train Loss=6.4315, Val Cor=0.7042, Time=0.1607 sec\n",
      "Epoch [107 / 2000]: Train Loss=6.3354, Val Cor=0.7040, Time=0.1600 sec\n",
      "Epoch [108 / 2000]: Train Loss=6.5360, Val Cor=0.7025, Time=0.1595 sec\n",
      "Epoch [109 / 2000]: Train Loss=6.2940, Val Cor=0.7069, Time=0.1603 sec\n",
      "Epoch [110 / 2000]: Train Loss=6.6084, Val Cor=0.7061, Time=0.1598 sec\n",
      "Epoch [111 / 2000]: Train Loss=6.5911, Val Cor=0.7124, Time=0.1613 sec\n",
      "Epoch [112 / 2000]: Train Loss=6.3857, Val Cor=0.7116, Time=0.1611 sec\n",
      "Epoch [113 / 2000]: Train Loss=6.4949, Val Cor=0.7040, Time=0.1614 sec\n",
      "Epoch [114 / 2000]: Train Loss=6.7612, Val Cor=0.7089, Time=0.1601 sec\n",
      "Epoch [115 / 2000]: Train Loss=6.5281, Val Cor=0.7110, Time=0.1619 sec\n",
      "Epoch [116 / 2000]: Train Loss=6.3451, Val Cor=0.7097, Time=0.1616 sec\n",
      "Epoch [117 / 2000]: Train Loss=6.7002, Val Cor=0.7136, Time=0.1619 sec\n",
      "Epoch [118 / 2000]: Train Loss=6.3622, Val Cor=0.7036, Time=0.1668 sec\n",
      "Epoch [119 / 2000]: Train Loss=6.2134, Val Cor=0.7061, Time=0.1620 sec\n",
      "Epoch [120 / 2000]: Train Loss=6.2669, Val Cor=0.7112, Time=0.1654 sec\n",
      "Epoch [121 / 2000]: Train Loss=6.6181, Val Cor=0.7051, Time=0.1626 sec\n",
      "Epoch [122 / 2000]: Train Loss=6.4162, Val Cor=0.7136, Time=0.1616 sec\n",
      "Epoch [123 / 2000]: Train Loss=6.3335, Val Cor=0.7101, Time=0.1615 sec\n",
      "Epoch [124 / 2000]: Train Loss=6.2284, Val Cor=0.7089, Time=0.1597 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [125 / 2000]: Train Loss=6.0732, Val Cor=0.7111, Time=0.1599 sec\n",
      "Epoch [126 / 2000]: Train Loss=6.6421, Val Cor=0.7061, Time=0.1602 sec\n",
      "Epoch [127 / 2000]: Train Loss=6.6090, Val Cor=0.7130, Time=0.1600 sec\n",
      "Epoch [128 / 2000]: Train Loss=6.3339, Val Cor=0.7068, Time=0.1595 sec\n",
      "Epoch [129 / 2000]: Train Loss=6.5487, Val Cor=0.7125, Time=0.1611 sec\n",
      "Epoch [130 / 2000]: Train Loss=6.6166, Val Cor=0.7087, Time=0.1609 sec\n",
      "Epoch [131 / 2000]: Train Loss=6.3580, Val Cor=0.7081, Time=0.1605 sec\n",
      "Epoch [132 / 2000]: Train Loss=6.1867, Val Cor=0.7077, Time=0.1608 sec\n",
      "Epoch [133 / 2000]: Train Loss=6.3794, Val Cor=0.7089, Time=0.1614 sec\n",
      "Epoch [134 / 2000]: Train Loss=6.4862, Val Cor=0.7078, Time=0.1609 sec\n",
      "Epoch [135 / 2000]: Train Loss=6.4457, Val Cor=0.7090, Time=0.1597 sec\n",
      "Epoch [136 / 2000]: Train Loss=6.4888, Val Cor=0.7071, Time=0.1594 sec\n",
      "Epoch [137 / 2000]: Train Loss=6.4094, Val Cor=0.7078, Time=0.1600 sec\n",
      "Epoch [138 / 2000]: Train Loss=6.4208, Val Cor=0.7082, Time=0.1591 sec\n",
      "Epoch [139 / 2000]: Train Loss=6.0860, Val Cor=0.7090, Time=0.1599 sec\n",
      "Epoch [140 / 2000]: Train Loss=6.2933, Val Cor=0.7028, Time=0.1610 sec\n",
      "Epoch [141 / 2000]: Train Loss=6.2932, Val Cor=0.7084, Time=0.1609 sec\n",
      "Epoch [142 / 2000]: Train Loss=6.1533, Val Cor=0.7083, Time=0.1610 sec\n",
      "Epoch [143 / 2000]: Train Loss=6.5301, Val Cor=0.7019, Time=0.1614 sec\n",
      "Epoch [144 / 2000]: Train Loss=6.6993, Val Cor=0.7077, Time=0.1612 sec\n",
      "Epoch [145 / 2000]: Train Loss=6.7009, Val Cor=0.7088, Time=0.1611 sec\n",
      "Epoch [146 / 2000]: Train Loss=6.4886, Val Cor=0.7012, Time=0.1592 sec\n",
      "Epoch [147 / 2000]: Train Loss=6.2627, Val Cor=0.7033, Time=0.1595 sec\n",
      "Epoch [148 / 2000]: Train Loss=6.2563, Val Cor=0.7047, Time=0.1583 sec\n",
      "Epoch [149 / 2000]: Train Loss=6.6780, Val Cor=0.7023, Time=0.1587 sec\n",
      "Epoch [150 / 2000]: Train Loss=6.2382, Val Cor=0.7036, Time=0.1584 sec\n",
      "Epoch [151 / 2000]: Train Loss=6.0474, Val Cor=0.7057, Time=0.1590 sec\n",
      "Epoch [152 / 2000]: Train Loss=6.3842, Val Cor=0.7009, Time=0.1599 sec\n",
      "Epoch [153 / 2000]: Train Loss=6.4884, Val Cor=0.7021, Time=0.1598 sec\n",
      "Epoch [154 / 2000]: Train Loss=6.1050, Val Cor=0.7045, Time=0.1602 sec\n",
      "Epoch [155 / 2000]: Train Loss=6.3279, Val Cor=0.7001, Time=0.1594 sec\n",
      "Epoch [156 / 2000]: Train Loss=6.2088, Val Cor=0.7052, Time=0.1583 sec\n",
      "Epoch [157 / 2000]: Train Loss=6.2289, Val Cor=0.7038, Time=0.1582 sec\n",
      "Epoch [158 / 2000]: Train Loss=6.3133, Val Cor=0.7047, Time=0.1582 sec\n",
      "Epoch [159 / 2000]: Train Loss=6.2784, Val Cor=0.7017, Time=0.1597 sec\n",
      "Epoch [160 / 2000]: Train Loss=6.3675, Val Cor=0.6956, Time=0.1598 sec\n",
      "Epoch [161 / 2000]: Train Loss=6.7141, Val Cor=0.7013, Time=0.1603 sec\n",
      "Epoch [162 / 2000]: Train Loss=6.4594, Val Cor=0.7066, Time=0.1615 sec\n",
      "Epoch [163 / 2000]: Train Loss=5.9600, Val Cor=0.7029, Time=0.1612 sec\n",
      "Epoch [164 / 2000]: Train Loss=6.1756, Val Cor=0.7018, Time=0.1611 sec\n",
      "Epoch [165 / 2000]: Train Loss=6.5914, Val Cor=0.7024, Time=0.1623 sec\n",
      "Epoch [166 / 2000]: Train Loss=6.2065, Val Cor=0.7027, Time=0.1619 sec\n",
      "Epoch [167 / 2000]: Train Loss=6.4437, Val Cor=0.6954, Time=0.1602 sec\n",
      "Epoch [168 / 2000]: Train Loss=6.4010, Val Cor=0.7016, Time=0.1600 sec\n",
      "Epoch [169 / 2000]: Train Loss=6.0211, Val Cor=0.7021, Time=0.1606 sec\n",
      "Epoch [170 / 2000]: Train Loss=6.0830, Val Cor=0.6986, Time=0.1605 sec\n",
      "Epoch [171 / 2000]: Train Loss=6.0335, Val Cor=0.7026, Time=0.1609 sec\n",
      "Epoch [172 / 2000]: Train Loss=6.1828, Val Cor=0.7002, Time=0.1610 sec\n",
      "Epoch [173 / 2000]: Train Loss=6.1137, Val Cor=0.7024, Time=0.1618 sec\n",
      "Epoch [174 / 2000]: Train Loss=6.7498, Val Cor=0.5403, Time=0.1618 sec\n",
      "Epoch [175 / 2000]: Train Loss=6.8731, Val Cor=0.7006, Time=0.1632 sec\n",
      "Epoch [176 / 2000]: Train Loss=6.4072, Val Cor=0.6991, Time=0.1631 sec\n",
      "Epoch [177 / 2000]: Train Loss=5.9756, Val Cor=0.7007, Time=0.1621 sec\n",
      "Epoch [178 / 2000]: Train Loss=6.0976, Val Cor=0.7015, Time=0.1598 sec\n",
      "Epoch [179 / 2000]: Train Loss=5.9735, Val Cor=0.6996, Time=0.1600 sec\n",
      "Epoch [180 / 2000]: Train Loss=5.9891, Val Cor=0.6933, Time=0.1602 sec\n",
      "Epoch [181 / 2000]: Train Loss=6.2010, Val Cor=0.7000, Time=0.1605 sec\n",
      "Epoch [182 / 2000]: Train Loss=6.2408, Val Cor=0.7029, Time=0.1598 sec\n",
      "Epoch [183 / 2000]: Train Loss=6.3414, Val Cor=0.7009, Time=0.1602 sec\n",
      "Epoch [184 / 2000]: Train Loss=6.2349, Val Cor=0.7030, Time=0.1614 sec\n",
      "Epoch [185 / 2000]: Train Loss=6.3530, Val Cor=0.7018, Time=0.1616 sec\n",
      "Epoch [186 / 2000]: Train Loss=6.3393, Val Cor=0.7013, Time=0.1608 sec\n",
      "Epoch [187 / 2000]: Train Loss=6.1861, Val Cor=0.6994, Time=0.1613 sec\n",
      "Epoch [188 / 2000]: Train Loss=6.0507, Val Cor=0.6965, Time=0.1614 sec\n",
      "Epoch [189 / 2000]: Train Loss=6.0477, Val Cor=0.6974, Time=0.1615 sec\n",
      "Epoch [190 / 2000]: Train Loss=6.0834, Val Cor=0.7021, Time=0.1618 sec\n",
      "Epoch [191 / 2000]: Train Loss=6.0917, Val Cor=0.6984, Time=0.1618 sec\n",
      "Epoch [192 / 2000]: Train Loss=6.1865, Val Cor=0.7011, Time=0.1603 sec\n",
      "Epoch [193 / 2000]: Train Loss=6.1359, Val Cor=0.6998, Time=0.1598 sec\n",
      "Epoch [194 / 2000]: Train Loss=6.2414, Val Cor=0.7030, Time=0.1598 sec\n",
      "Epoch [195 / 2000]: Train Loss=6.4869, Val Cor=0.6973, Time=0.1605 sec\n",
      "Epoch [196 / 2000]: Train Loss=6.3184, Val Cor=0.6992, Time=0.1616 sec\n",
      "Epoch [197 / 2000]: Train Loss=6.2689, Val Cor=0.7003, Time=0.1619 sec\n",
      "Epoch [198 / 2000]: Train Loss=6.2552, Val Cor=0.6924, Time=0.1614 sec\n",
      "Epoch [199 / 2000]: Train Loss=6.2417, Val Cor=0.6914, Time=0.1614 sec\n",
      "Epoch [200 / 2000]: Train Loss=6.0479, Val Cor=0.6966, Time=0.1614 sec\n",
      "Epoch [201 / 2000]: Train Loss=6.2481, Val Cor=0.6902, Time=0.1616 sec\n",
      "Epoch [202 / 2000]: Train Loss=6.0454, Val Cor=0.7001, Time=0.1612 sec\n",
      "Epoch [203 / 2000]: Train Loss=6.2523, Val Cor=0.7017, Time=0.1600 sec\n",
      "Epoch [204 / 2000]: Train Loss=5.8797, Val Cor=0.6954, Time=0.1596 sec\n",
      "Epoch [205 / 2000]: Train Loss=6.0387, Val Cor=0.7020, Time=0.1602 sec\n",
      "Epoch [206 / 2000]: Train Loss=6.0980, Val Cor=0.7011, Time=0.1603 sec\n",
      "Epoch [207 / 2000]: Train Loss=5.9611, Val Cor=0.7019, Time=0.1618 sec\n",
      "Epoch [208 / 2000]: Train Loss=6.2078, Val Cor=0.6965, Time=0.1612 sec\n",
      "Epoch [209 / 2000]: Train Loss=5.7854, Val Cor=0.7007, Time=0.1618 sec\n",
      "Epoch [210 / 2000]: Train Loss=5.6811, Val Cor=0.6933, Time=0.1613 sec\n",
      "Epoch [211 / 2000]: Train Loss=6.1271, Val Cor=0.6978, Time=0.1603 sec\n",
      "Epoch [212 / 2000]: Train Loss=5.9700, Val Cor=0.7057, Time=0.1603 sec\n",
      "Epoch [213 / 2000]: Train Loss=5.9954, Val Cor=0.7029, Time=0.1601 sec\n",
      "Epoch [214 / 2000]: Train Loss=5.8011, Val Cor=0.6958, Time=0.1596 sec\n",
      "Epoch [215 / 2000]: Train Loss=5.8389, Val Cor=0.7035, Time=0.1598 sec\n",
      "Epoch [216 / 2000]: Train Loss=6.0556, Val Cor=0.6974, Time=0.1595 sec\n",
      "Epoch [217 / 2000]: Train Loss=6.3058, Val Cor=0.6940, Time=0.1611 sec\n",
      "Epoch [218 / 2000]: Train Loss=6.6154, Val Cor=0.7044, Time=0.1610 sec\n",
      "Epoch [219 / 2000]: Train Loss=6.0620, Val Cor=0.6915, Time=0.1608 sec\n",
      "Epoch [220 / 2000]: Train Loss=6.0698, Val Cor=0.7041, Time=0.1611 sec\n",
      "Epoch [221 / 2000]: Train Loss=6.0169, Val Cor=0.7019, Time=0.1611 sec\n",
      "Epoch [222 / 2000]: Train Loss=5.9606, Val Cor=0.6998, Time=0.1596 sec\n",
      "Epoch [223 / 2000]: Train Loss=5.7822, Val Cor=0.7018, Time=0.1594 sec\n",
      "Epoch [224 / 2000]: Train Loss=5.9235, Val Cor=0.6976, Time=0.1597 sec\n",
      "Epoch [225 / 2000]: Train Loss=6.1346, Val Cor=0.6972, Time=0.1602 sec\n",
      "Epoch [226 / 2000]: Train Loss=6.1153, Val Cor=0.7003, Time=0.1605 sec\n",
      "Epoch [227 / 2000]: Train Loss=5.9895, Val Cor=0.7015, Time=0.1608 sec\n",
      "Epoch [228 / 2000]: Train Loss=5.8162, Val Cor=0.7040, Time=0.1605 sec\n",
      "Epoch [229 / 2000]: Train Loss=6.0227, Val Cor=0.6982, Time=0.1618 sec\n",
      "Epoch [230 / 2000]: Train Loss=6.0719, Val Cor=0.6960, Time=0.1615 sec\n",
      "Epoch [231 / 2000]: Train Loss=6.4559, Val Cor=0.6961, Time=0.1612 sec\n",
      "Epoch [232 / 2000]: Train Loss=6.1066, Val Cor=0.6986, Time=0.1616 sec\n",
      "Epoch [233 / 2000]: Train Loss=6.0171, Val Cor=0.6996, Time=0.1611 sec\n",
      "Epoch [234 / 2000]: Train Loss=5.8575, Val Cor=0.7030, Time=0.1598 sec\n",
      "Epoch [235 / 2000]: Train Loss=6.3914, Val Cor=0.7020, Time=0.1599 sec\n",
      "Epoch [236 / 2000]: Train Loss=6.0993, Val Cor=0.6871, Time=0.1601 sec\n",
      "Epoch [237 / 2000]: Train Loss=6.2256, Val Cor=0.6990, Time=0.1611 sec\n",
      "Epoch [238 / 2000]: Train Loss=5.9467, Val Cor=0.6989, Time=0.1599 sec\n",
      "Epoch [239 / 2000]: Train Loss=6.0720, Val Cor=0.6977, Time=0.1599 sec\n",
      "Epoch [240 / 2000]: Train Loss=6.1517, Val Cor=0.6932, Time=0.1601 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [241 / 2000]: Train Loss=6.2065, Val Cor=0.6901, Time=0.1607 sec\n",
      "Epoch [242 / 2000]: Train Loss=5.9939, Val Cor=0.6873, Time=0.1614 sec\n",
      "Epoch [243 / 2000]: Train Loss=6.2595, Val Cor=0.6977, Time=0.1612 sec\n",
      "Epoch [244 / 2000]: Train Loss=5.8508, Val Cor=0.6967, Time=0.1614 sec\n",
      "Epoch [245 / 2000]: Train Loss=6.0944, Val Cor=0.6990, Time=0.1613 sec\n",
      "Epoch [246 / 2000]: Train Loss=6.1656, Val Cor=0.6984, Time=0.1607 sec\n",
      "Epoch [247 / 2000]: Train Loss=6.0771, Val Cor=0.6976, Time=0.1600 sec\n",
      "Epoch [248 / 2000]: Train Loss=6.0408, Val Cor=0.6999, Time=0.1596 sec\n",
      "Epoch [249 / 2000]: Train Loss=5.9455, Val Cor=0.6994, Time=0.1598 sec\n",
      "Epoch [250 / 2000]: Train Loss=5.8389, Val Cor=0.6935, Time=0.1601 sec\n",
      "Epoch [251 / 2000]: Train Loss=5.8933, Val Cor=0.6897, Time=0.1605 sec\n",
      "Epoch [252 / 2000]: Train Loss=5.7616, Val Cor=0.7008, Time=0.1607 sec\n",
      "Epoch [253 / 2000]: Train Loss=6.4310, Val Cor=0.7007, Time=0.1615 sec\n",
      "Epoch [254 / 2000]: Train Loss=5.9531, Val Cor=0.6984, Time=0.1614 sec\n",
      "Epoch [255 / 2000]: Train Loss=6.0489, Val Cor=0.7002, Time=0.1619 sec\n",
      "Epoch [256 / 2000]: Train Loss=5.9248, Val Cor=0.6927, Time=0.1617 sec\n",
      "Epoch [257 / 2000]: Train Loss=5.7136, Val Cor=0.7017, Time=0.1606 sec\n",
      "Epoch [258 / 2000]: Train Loss=5.6960, Val Cor=0.6819, Time=0.1597 sec\n",
      "Epoch [259 / 2000]: Train Loss=5.8070, Val Cor=0.7049, Time=0.1600 sec\n",
      "Epoch [260 / 2000]: Train Loss=5.7414, Val Cor=0.7032, Time=0.1601 sec\n",
      "Epoch [261 / 2000]: Train Loss=6.1762, Val Cor=0.7066, Time=0.1601 sec\n",
      "Epoch [262 / 2000]: Train Loss=5.9700, Val Cor=0.7017, Time=0.1613 sec\n",
      "Epoch [263 / 2000]: Train Loss=5.7465, Val Cor=0.7030, Time=0.1615 sec\n",
      "Epoch [264 / 2000]: Train Loss=5.9394, Val Cor=0.6982, Time=0.1619 sec\n",
      "Epoch [265 / 2000]: Train Loss=5.9858, Val Cor=0.7005, Time=0.1615 sec\n",
      "Epoch [266 / 2000]: Train Loss=6.0067, Val Cor=0.6985, Time=0.1599 sec\n",
      "Epoch [267 / 2000]: Train Loss=6.2967, Val Cor=0.7045, Time=0.1598 sec\n",
      "Epoch [268 / 2000]: Train Loss=6.0522, Val Cor=0.6993, Time=0.1602 sec\n",
      "Epoch [269 / 2000]: Train Loss=5.9698, Val Cor=0.6995, Time=0.1599 sec\n",
      "Epoch [270 / 2000]: Train Loss=6.1227, Val Cor=0.6980, Time=0.1599 sec\n",
      "Epoch [271 / 2000]: Train Loss=6.0833, Val Cor=0.6863, Time=0.1604 sec\n",
      "Epoch [272 / 2000]: Train Loss=5.8426, Val Cor=0.7014, Time=0.1605 sec\n",
      "Epoch [273 / 2000]: Train Loss=5.7916, Val Cor=0.6914, Time=0.1596 sec\n",
      "Epoch [274 / 2000]: Train Loss=5.9145, Val Cor=0.7030, Time=0.1595 sec\n",
      "Epoch [275 / 2000]: Train Loss=6.1984, Val Cor=0.6853, Time=0.1601 sec\n",
      "Epoch [276 / 2000]: Train Loss=6.2271, Val Cor=0.6939, Time=0.1606 sec\n",
      "Epoch [277 / 2000]: Train Loss=6.1166, Val Cor=0.7007, Time=0.1607 sec\n",
      "Epoch [278 / 2000]: Train Loss=6.4141, Val Cor=0.6887, Time=0.1606 sec\n",
      "Epoch [279 / 2000]: Train Loss=6.0799, Val Cor=0.6848, Time=0.1623 sec\n",
      "Epoch [280 / 2000]: Train Loss=6.2650, Val Cor=0.6909, Time=0.1610 sec\n",
      "Epoch [281 / 2000]: Train Loss=6.2743, Val Cor=0.6877, Time=0.1614 sec\n",
      "Epoch [282 / 2000]: Train Loss=5.8457, Val Cor=0.7069, Time=0.1621 sec\n",
      "Epoch [283 / 2000]: Train Loss=5.9874, Val Cor=0.7018, Time=0.1622 sec\n",
      "Epoch [284 / 2000]: Train Loss=6.1618, Val Cor=0.6910, Time=0.1615 sec\n",
      "Epoch [285 / 2000]: Train Loss=5.9495, Val Cor=0.7045, Time=0.1620 sec\n",
      "Epoch [286 / 2000]: Train Loss=5.9048, Val Cor=0.6779, Time=0.1617 sec\n",
      "Epoch [287 / 2000]: Train Loss=6.3322, Val Cor=0.6972, Time=0.1620 sec\n",
      "Epoch [288 / 2000]: Train Loss=6.0382, Val Cor=0.7003, Time=0.1600 sec\n",
      "Epoch [289 / 2000]: Train Loss=5.8664, Val Cor=0.7010, Time=0.1599 sec\n",
      "Epoch [290 / 2000]: Train Loss=5.9693, Val Cor=0.7025, Time=0.1599 sec\n",
      "Epoch [291 / 2000]: Train Loss=5.6321, Val Cor=0.7031, Time=0.1602 sec\n",
      "Epoch [292 / 2000]: Train Loss=5.6629, Val Cor=0.6833, Time=0.1613 sec\n",
      "Epoch [293 / 2000]: Train Loss=5.8825, Val Cor=0.7029, Time=0.1615 sec\n",
      "Epoch [294 / 2000]: Train Loss=5.8837, Val Cor=0.7047, Time=0.1615 sec\n",
      "Epoch [295 / 2000]: Train Loss=5.9942, Val Cor=0.7008, Time=0.1613 sec\n",
      "Epoch [296 / 2000]: Train Loss=6.0547, Val Cor=0.6844, Time=0.1603 sec\n",
      "Epoch [297 / 2000]: Train Loss=6.0598, Val Cor=0.7024, Time=0.1601 sec\n",
      "Epoch [298 / 2000]: Train Loss=5.8979, Val Cor=0.6793, Time=0.1595 sec\n",
      "Epoch [299 / 2000]: Train Loss=5.8626, Val Cor=0.7037, Time=0.1601 sec\n",
      "Epoch [300 / 2000]: Train Loss=5.5724, Val Cor=0.6870, Time=0.1599 sec\n",
      "Epoch [301 / 2000]: Train Loss=5.7619, Val Cor=0.7014, Time=0.1599 sec\n",
      "Epoch [302 / 2000]: Train Loss=5.8559, Val Cor=0.6493, Time=0.1601 sec\n",
      "Epoch [303 / 2000]: Train Loss=6.1273, Val Cor=0.7028, Time=0.1614 sec\n",
      "Epoch [304 / 2000]: Train Loss=5.8802, Val Cor=0.6979, Time=0.1612 sec\n",
      "Epoch [305 / 2000]: Train Loss=6.0274, Val Cor=0.6978, Time=0.1620 sec\n",
      "Epoch [306 / 2000]: Train Loss=5.9953, Val Cor=0.7057, Time=0.1615 sec\n",
      "Epoch [307 / 2000]: Train Loss=5.7961, Val Cor=0.7053, Time=0.1618 sec\n",
      "Epoch [308 / 2000]: Train Loss=5.7660, Val Cor=0.7023, Time=0.1617 sec\n",
      "Epoch [309 / 2000]: Train Loss=5.8437, Val Cor=0.7036, Time=0.1606 sec\n",
      "Epoch [310 / 2000]: Train Loss=5.9524, Val Cor=0.6857, Time=0.1605 sec\n",
      "Epoch [311 / 2000]: Train Loss=5.8550, Val Cor=0.6965, Time=0.1610 sec\n",
      "Epoch [312 / 2000]: Train Loss=5.9118, Val Cor=0.6824, Time=0.1605 sec\n",
      "Epoch [313 / 2000]: Train Loss=5.8946, Val Cor=0.6857, Time=0.1606 sec\n",
      "Epoch [314 / 2000]: Train Loss=5.6026, Val Cor=0.7003, Time=0.1616 sec\n",
      "Epoch [315 / 2000]: Train Loss=5.6965, Val Cor=0.6847, Time=0.1612 sec\n",
      "Epoch [316 / 2000]: Train Loss=5.7930, Val Cor=0.6973, Time=0.1612 sec\n",
      "Epoch [317 / 2000]: Train Loss=5.9917, Val Cor=0.6861, Time=0.1618 sec\n",
      "Epoch [318 / 2000]: Train Loss=6.0393, Val Cor=0.6739, Time=0.1615 sec\n",
      "Epoch [319 / 2000]: Train Loss=5.7095, Val Cor=0.7018, Time=0.1619 sec\n",
      "Epoch [320 / 2000]: Train Loss=5.8226, Val Cor=0.5251, Time=0.1595 sec\n",
      "Epoch [321 / 2000]: Train Loss=6.1167, Val Cor=0.6969, Time=0.1603 sec\n",
      "Epoch [322 / 2000]: Train Loss=5.6847, Val Cor=0.7041, Time=0.1598 sec\n",
      "Early stopping triggered.\n",
      "Training RNN model 4:\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [1 / 2000]: Train Loss=8.3140, Val Cor=0.3377, Time=0.1606 sec\n",
      "Epoch [2 / 2000]: Train Loss=8.3375, Val Cor=0.4034, Time=0.1595 sec\n",
      "Epoch [3 / 2000]: Train Loss=8.2156, Val Cor=0.4257, Time=0.1596 sec\n",
      "Epoch [4 / 2000]: Train Loss=8.2207, Val Cor=0.4377, Time=0.1596 sec\n",
      "Epoch [5 / 2000]: Train Loss=8.3047, Val Cor=0.4480, Time=0.1600 sec\n",
      "Epoch [6 / 2000]: Train Loss=8.2825, Val Cor=0.4019, Time=0.1589 sec\n",
      "Epoch [7 / 2000]: Train Loss=8.2513, Val Cor=0.4434, Time=0.1598 sec\n",
      "Epoch [8 / 2000]: Train Loss=8.2304, Val Cor=0.4420, Time=0.1609 sec\n",
      "Epoch [9 / 2000]: Train Loss=8.2100, Val Cor=0.4211, Time=0.1609 sec\n",
      "Epoch [10 / 2000]: Train Loss=8.1802, Val Cor=0.4236, Time=0.1609 sec\n",
      "Epoch [11 / 2000]: Train Loss=8.1828, Val Cor=0.4386, Time=0.1607 sec\n",
      "Epoch [12 / 2000]: Train Loss=8.2256, Val Cor=0.4597, Time=0.1591 sec\n",
      "Epoch [13 / 2000]: Train Loss=8.2109, Val Cor=0.4720, Time=0.1590 sec\n",
      "Epoch [14 / 2000]: Train Loss=8.1866, Val Cor=0.4733, Time=0.1589 sec\n",
      "Epoch [15 / 2000]: Train Loss=8.1569, Val Cor=0.4880, Time=0.1591 sec\n",
      "Epoch [16 / 2000]: Train Loss=8.0581, Val Cor=0.4966, Time=0.1593 sec\n",
      "Epoch [17 / 2000]: Train Loss=8.1641, Val Cor=0.4956, Time=0.1595 sec\n",
      "Epoch [18 / 2000]: Train Loss=7.9490, Val Cor=0.5064, Time=0.1597 sec\n",
      "Epoch [19 / 2000]: Train Loss=7.8983, Val Cor=0.5114, Time=0.1596 sec\n",
      "Epoch [20 / 2000]: Train Loss=7.7311, Val Cor=0.5251, Time=0.1595 sec\n",
      "Epoch [21 / 2000]: Train Loss=7.6521, Val Cor=0.5282, Time=0.1599 sec\n",
      "Epoch [22 / 2000]: Train Loss=7.6635, Val Cor=0.5332, Time=0.1595 sec\n",
      "Epoch [23 / 2000]: Train Loss=7.7031, Val Cor=0.5303, Time=0.1600 sec\n",
      "Epoch [24 / 2000]: Train Loss=7.6439, Val Cor=0.5411, Time=0.1602 sec\n",
      "Epoch [25 / 2000]: Train Loss=7.4718, Val Cor=0.5383, Time=0.1604 sec\n",
      "Epoch [26 / 2000]: Train Loss=7.5191, Val Cor=0.5449, Time=0.1604 sec\n",
      "Epoch [27 / 2000]: Train Loss=7.5353, Val Cor=0.5473, Time=0.1606 sec\n",
      "Epoch [28 / 2000]: Train Loss=7.5547, Val Cor=0.5447, Time=0.1603 sec\n",
      "Epoch [29 / 2000]: Train Loss=7.4181, Val Cor=0.5453, Time=0.1617 sec\n",
      "Epoch [30 / 2000]: Train Loss=7.2247, Val Cor=0.5477, Time=0.1615 sec\n",
      "Epoch [31 / 2000]: Train Loss=7.6721, Val Cor=0.5511, Time=0.1616 sec\n",
      "Epoch [32 / 2000]: Train Loss=7.1885, Val Cor=0.5520, Time=0.1612 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33 / 2000]: Train Loss=7.3274, Val Cor=0.5490, Time=0.1622 sec\n",
      "Epoch [34 / 2000]: Train Loss=7.5116, Val Cor=0.5444, Time=0.1633 sec\n",
      "Epoch [35 / 2000]: Train Loss=7.2703, Val Cor=0.5488, Time=0.1633 sec\n",
      "Epoch [36 / 2000]: Train Loss=7.3167, Val Cor=0.5572, Time=0.1604 sec\n",
      "Epoch [37 / 2000]: Train Loss=7.3002, Val Cor=0.5530, Time=0.1603 sec\n",
      "Epoch [38 / 2000]: Train Loss=7.1094, Val Cor=0.5610, Time=0.1605 sec\n",
      "Epoch [39 / 2000]: Train Loss=7.1161, Val Cor=0.5607, Time=0.1607 sec\n",
      "Epoch [40 / 2000]: Train Loss=7.1419, Val Cor=0.5588, Time=0.1605 sec\n",
      "Epoch [41 / 2000]: Train Loss=7.4177, Val Cor=0.5630, Time=0.1598 sec\n",
      "Epoch [42 / 2000]: Train Loss=7.1985, Val Cor=0.5660, Time=0.1596 sec\n",
      "Epoch [43 / 2000]: Train Loss=7.4003, Val Cor=0.5571, Time=0.1599 sec\n",
      "Epoch [44 / 2000]: Train Loss=7.4538, Val Cor=0.5669, Time=0.1603 sec\n",
      "Epoch [45 / 2000]: Train Loss=7.0226, Val Cor=0.5609, Time=0.1600 sec\n",
      "Epoch [46 / 2000]: Train Loss=7.0878, Val Cor=0.5629, Time=0.1611 sec\n",
      "Epoch [47 / 2000]: Train Loss=7.0131, Val Cor=0.5653, Time=0.1610 sec\n",
      "Epoch [48 / 2000]: Train Loss=7.0555, Val Cor=0.5674, Time=0.1608 sec\n",
      "Epoch [49 / 2000]: Train Loss=6.9601, Val Cor=0.5620, Time=0.1611 sec\n",
      "Epoch [50 / 2000]: Train Loss=7.1080, Val Cor=0.5712, Time=0.1613 sec\n",
      "Epoch [51 / 2000]: Train Loss=6.7548, Val Cor=0.5795, Time=0.1617 sec\n",
      "Epoch [52 / 2000]: Train Loss=7.4934, Val Cor=0.5774, Time=0.1612 sec\n",
      "Epoch [53 / 2000]: Train Loss=7.0991, Val Cor=0.5776, Time=0.1612 sec\n",
      "Epoch [54 / 2000]: Train Loss=7.3661, Val Cor=0.5769, Time=0.1599 sec\n",
      "Epoch [55 / 2000]: Train Loss=6.9381, Val Cor=0.5826, Time=0.1600 sec\n",
      "Epoch [56 / 2000]: Train Loss=7.2395, Val Cor=0.5798, Time=0.1591 sec\n",
      "Epoch [57 / 2000]: Train Loss=6.8875, Val Cor=0.5831, Time=0.1596 sec\n",
      "Epoch [58 / 2000]: Train Loss=6.7798, Val Cor=0.5827, Time=0.1594 sec\n",
      "Epoch [59 / 2000]: Train Loss=7.1044, Val Cor=0.5834, Time=0.1600 sec\n",
      "Epoch [60 / 2000]: Train Loss=6.9801, Val Cor=0.5801, Time=0.1593 sec\n",
      "Epoch [61 / 2000]: Train Loss=6.8473, Val Cor=0.5801, Time=0.1599 sec\n",
      "Epoch [62 / 2000]: Train Loss=6.9295, Val Cor=0.5863, Time=0.1609 sec\n",
      "Epoch [63 / 2000]: Train Loss=6.9505, Val Cor=0.5879, Time=0.1612 sec\n",
      "Epoch [64 / 2000]: Train Loss=7.0577, Val Cor=0.5855, Time=0.1611 sec\n",
      "Epoch [65 / 2000]: Train Loss=7.0860, Val Cor=0.5847, Time=0.1617 sec\n",
      "Epoch [66 / 2000]: Train Loss=7.0286, Val Cor=0.5819, Time=0.1607 sec\n",
      "Epoch [67 / 2000]: Train Loss=7.1299, Val Cor=0.5833, Time=0.1614 sec\n",
      "Epoch [68 / 2000]: Train Loss=6.9383, Val Cor=0.5894, Time=0.1611 sec\n",
      "Epoch [69 / 2000]: Train Loss=6.9997, Val Cor=0.5877, Time=0.1613 sec\n",
      "Epoch [70 / 2000]: Train Loss=6.6172, Val Cor=0.5850, Time=0.1601 sec\n",
      "Epoch [71 / 2000]: Train Loss=6.9691, Val Cor=0.5867, Time=0.1594 sec\n",
      "Epoch [72 / 2000]: Train Loss=7.1595, Val Cor=0.5923, Time=0.1594 sec\n",
      "Epoch [73 / 2000]: Train Loss=6.7804, Val Cor=0.5965, Time=0.1597 sec\n",
      "Epoch [74 / 2000]: Train Loss=7.0644, Val Cor=0.5954, Time=0.1594 sec\n",
      "Epoch [75 / 2000]: Train Loss=7.0072, Val Cor=0.5968, Time=0.1597 sec\n",
      "Epoch [76 / 2000]: Train Loss=6.7330, Val Cor=0.5980, Time=0.1593 sec\n",
      "Epoch [77 / 2000]: Train Loss=6.6312, Val Cor=0.6007, Time=0.1594 sec\n",
      "Epoch [78 / 2000]: Train Loss=6.9968, Val Cor=0.6003, Time=0.1593 sec\n",
      "Epoch [79 / 2000]: Train Loss=6.9454, Val Cor=0.6016, Time=0.1600 sec\n",
      "Epoch [80 / 2000]: Train Loss=6.9816, Val Cor=0.6012, Time=0.1598 sec\n",
      "Epoch [81 / 2000]: Train Loss=6.7415, Val Cor=0.6003, Time=0.1611 sec\n",
      "Epoch [82 / 2000]: Train Loss=6.6791, Val Cor=0.6072, Time=0.1615 sec\n",
      "Epoch [83 / 2000]: Train Loss=6.9036, Val Cor=0.5993, Time=0.1620 sec\n",
      "Epoch [84 / 2000]: Train Loss=6.6823, Val Cor=0.6040, Time=0.1616 sec\n",
      "Epoch [85 / 2000]: Train Loss=6.8905, Val Cor=0.6052, Time=0.1614 sec\n",
      "Epoch [86 / 2000]: Train Loss=6.5496, Val Cor=0.6086, Time=0.1620 sec\n",
      "Epoch [87 / 2000]: Train Loss=6.7228, Val Cor=0.6030, Time=0.1622 sec\n",
      "Epoch [88 / 2000]: Train Loss=6.9502, Val Cor=0.6073, Time=0.1616 sec\n",
      "Epoch [89 / 2000]: Train Loss=6.6281, Val Cor=0.6112, Time=0.1602 sec\n",
      "Epoch [90 / 2000]: Train Loss=6.8163, Val Cor=0.6106, Time=0.1597 sec\n",
      "Epoch [91 / 2000]: Train Loss=6.5892, Val Cor=0.6095, Time=0.1603 sec\n",
      "Epoch [92 / 2000]: Train Loss=6.8062, Val Cor=0.6136, Time=0.1599 sec\n",
      "Epoch [93 / 2000]: Train Loss=6.7705, Val Cor=0.6104, Time=0.1603 sec\n",
      "Epoch [94 / 2000]: Train Loss=6.5733, Val Cor=0.6154, Time=0.1602 sec\n",
      "Epoch [95 / 2000]: Train Loss=6.6437, Val Cor=0.6116, Time=0.1598 sec\n",
      "Epoch [96 / 2000]: Train Loss=7.0014, Val Cor=0.6155, Time=0.1600 sec\n",
      "Epoch [97 / 2000]: Train Loss=6.5124, Val Cor=0.6173, Time=0.1601 sec\n",
      "Epoch [98 / 2000]: Train Loss=6.7606, Val Cor=0.6167, Time=0.1597 sec\n",
      "Epoch [99 / 2000]: Train Loss=6.7621, Val Cor=0.6225, Time=0.1614 sec\n",
      "Epoch [100 / 2000]: Train Loss=6.6322, Val Cor=0.6220, Time=0.1607 sec\n",
      "Epoch [101 / 2000]: Train Loss=6.7566, Val Cor=0.6286, Time=0.1616 sec\n",
      "Epoch [102 / 2000]: Train Loss=6.6551, Val Cor=0.6292, Time=0.1609 sec\n",
      "Epoch [103 / 2000]: Train Loss=6.6988, Val Cor=0.6225, Time=0.1616 sec\n",
      "Epoch [104 / 2000]: Train Loss=6.4016, Val Cor=0.6242, Time=0.1618 sec\n",
      "Epoch [105 / 2000]: Train Loss=6.6787, Val Cor=0.6250, Time=0.1617 sec\n",
      "Epoch [106 / 2000]: Train Loss=6.4985, Val Cor=0.6287, Time=0.1596 sec\n",
      "Epoch [107 / 2000]: Train Loss=6.8859, Val Cor=0.6267, Time=0.1598 sec\n",
      "Epoch [108 / 2000]: Train Loss=6.4666, Val Cor=0.6301, Time=0.1598 sec\n",
      "Epoch [109 / 2000]: Train Loss=6.7003, Val Cor=0.6293, Time=0.1594 sec\n",
      "Epoch [110 / 2000]: Train Loss=6.2232, Val Cor=0.6284, Time=0.1599 sec\n",
      "Epoch [111 / 2000]: Train Loss=6.5502, Val Cor=0.6279, Time=0.1602 sec\n",
      "Epoch [112 / 2000]: Train Loss=6.9102, Val Cor=0.6319, Time=0.1605 sec\n",
      "Epoch [113 / 2000]: Train Loss=6.6578, Val Cor=0.6365, Time=0.1605 sec\n",
      "Epoch [114 / 2000]: Train Loss=6.6159, Val Cor=0.6386, Time=0.1601 sec\n",
      "Epoch [115 / 2000]: Train Loss=6.6335, Val Cor=0.6425, Time=0.1604 sec\n",
      "Epoch [116 / 2000]: Train Loss=6.9356, Val Cor=0.6365, Time=0.1603 sec\n",
      "Epoch [117 / 2000]: Train Loss=6.8024, Val Cor=0.6362, Time=0.1614 sec\n",
      "Epoch [118 / 2000]: Train Loss=6.4955, Val Cor=0.6375, Time=0.1615 sec\n",
      "Epoch [119 / 2000]: Train Loss=6.4610, Val Cor=0.6363, Time=0.1609 sec\n",
      "Epoch [120 / 2000]: Train Loss=6.4539, Val Cor=0.6329, Time=0.1601 sec\n",
      "Epoch [121 / 2000]: Train Loss=6.5265, Val Cor=0.6303, Time=0.1601 sec\n",
      "Epoch [122 / 2000]: Train Loss=6.4082, Val Cor=0.6298, Time=0.1598 sec\n",
      "Epoch [123 / 2000]: Train Loss=6.6147, Val Cor=0.6335, Time=0.1599 sec\n",
      "Epoch [124 / 2000]: Train Loss=6.9170, Val Cor=0.6395, Time=0.1610 sec\n",
      "Epoch [125 / 2000]: Train Loss=6.4760, Val Cor=0.6405, Time=0.1613 sec\n",
      "Epoch [126 / 2000]: Train Loss=6.4956, Val Cor=0.6405, Time=0.1614 sec\n",
      "Epoch [127 / 2000]: Train Loss=6.6888, Val Cor=0.6385, Time=0.1608 sec\n",
      "Epoch [128 / 2000]: Train Loss=6.5673, Val Cor=0.6403, Time=0.1613 sec\n",
      "Epoch [129 / 2000]: Train Loss=6.5308, Val Cor=0.6399, Time=0.1613 sec\n",
      "Epoch [130 / 2000]: Train Loss=6.2688, Val Cor=0.6371, Time=0.1611 sec\n",
      "Epoch [131 / 2000]: Train Loss=7.0283, Val Cor=0.6401, Time=0.1604 sec\n",
      "Epoch [132 / 2000]: Train Loss=6.7624, Val Cor=0.6362, Time=0.1595 sec\n",
      "Epoch [133 / 2000]: Train Loss=6.7611, Val Cor=0.6423, Time=0.1597 sec\n",
      "Epoch [134 / 2000]: Train Loss=6.7551, Val Cor=0.6401, Time=0.1600 sec\n",
      "Epoch [135 / 2000]: Train Loss=6.6590, Val Cor=0.6398, Time=0.1604 sec\n",
      "Epoch [136 / 2000]: Train Loss=6.4880, Val Cor=0.6422, Time=0.1602 sec\n",
      "Epoch [137 / 2000]: Train Loss=6.6155, Val Cor=0.6372, Time=0.1607 sec\n",
      "Epoch [138 / 2000]: Train Loss=6.4811, Val Cor=0.6422, Time=0.1614 sec\n",
      "Epoch [139 / 2000]: Train Loss=6.7246, Val Cor=0.6414, Time=0.1617 sec\n",
      "Epoch [140 / 2000]: Train Loss=6.4744, Val Cor=0.6355, Time=0.1619 sec\n",
      "Epoch [141 / 2000]: Train Loss=6.5678, Val Cor=0.6381, Time=0.1617 sec\n",
      "Epoch [142 / 2000]: Train Loss=6.9017, Val Cor=0.6421, Time=0.1605 sec\n",
      "Epoch [143 / 2000]: Train Loss=6.4883, Val Cor=0.6397, Time=0.1600 sec\n",
      "Epoch [144 / 2000]: Train Loss=6.5060, Val Cor=0.6364, Time=0.1601 sec\n",
      "Epoch [145 / 2000]: Train Loss=6.8234, Val Cor=0.6356, Time=0.1603 sec\n",
      "Epoch [146 / 2000]: Train Loss=6.2785, Val Cor=0.6394, Time=0.1597 sec\n",
      "Epoch [147 / 2000]: Train Loss=6.6617, Val Cor=0.6312, Time=0.1602 sec\n",
      "Epoch [148 / 2000]: Train Loss=6.3479, Val Cor=0.6321, Time=0.1611 sec\n",
      "Epoch [149 / 2000]: Train Loss=6.3258, Val Cor=0.6304, Time=0.1612 sec\n",
      "Epoch [150 / 2000]: Train Loss=6.4568, Val Cor=0.6328, Time=0.1612 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151 / 2000]: Train Loss=6.7057, Val Cor=0.6375, Time=0.1609 sec\n",
      "Epoch [152 / 2000]: Train Loss=6.6791, Val Cor=0.6374, Time=0.1612 sec\n",
      "Epoch [153 / 2000]: Train Loss=6.7499, Val Cor=0.6388, Time=0.1611 sec\n",
      "Epoch [154 / 2000]: Train Loss=6.6450, Val Cor=0.6343, Time=0.1610 sec\n",
      "Epoch [155 / 2000]: Train Loss=6.4180, Val Cor=0.6368, Time=0.1596 sec\n",
      "Epoch [156 / 2000]: Train Loss=6.7158, Val Cor=0.6363, Time=0.1593 sec\n",
      "Epoch [157 / 2000]: Train Loss=6.6535, Val Cor=0.6384, Time=0.1599 sec\n",
      "Epoch [158 / 2000]: Train Loss=6.5848, Val Cor=0.6379, Time=0.1594 sec\n",
      "Epoch [159 / 2000]: Train Loss=6.4353, Val Cor=0.6412, Time=0.1601 sec\n",
      "Epoch [160 / 2000]: Train Loss=6.3872, Val Cor=0.6389, Time=0.1597 sec\n",
      "Epoch [161 / 2000]: Train Loss=6.7373, Val Cor=0.6401, Time=0.1611 sec\n",
      "Epoch [162 / 2000]: Train Loss=6.4014, Val Cor=0.6417, Time=0.1612 sec\n",
      "Epoch [163 / 2000]: Train Loss=6.1430, Val Cor=0.6426, Time=0.1618 sec\n",
      "Epoch [164 / 2000]: Train Loss=6.1742, Val Cor=0.6390, Time=0.1613 sec\n",
      "Epoch [165 / 2000]: Train Loss=6.3358, Val Cor=0.6373, Time=0.1617 sec\n",
      "Epoch [166 / 2000]: Train Loss=6.5595, Val Cor=0.6385, Time=0.1604 sec\n",
      "Epoch [167 / 2000]: Train Loss=6.4086, Val Cor=0.6390, Time=0.1602 sec\n",
      "Epoch [168 / 2000]: Train Loss=6.3266, Val Cor=0.6393, Time=0.1601 sec\n",
      "Epoch [169 / 2000]: Train Loss=6.4257, Val Cor=0.6426, Time=0.1603 sec\n",
      "Epoch [170 / 2000]: Train Loss=6.3151, Val Cor=0.6412, Time=0.1600 sec\n",
      "Epoch [171 / 2000]: Train Loss=6.3702, Val Cor=0.6395, Time=0.1608 sec\n",
      "Epoch [172 / 2000]: Train Loss=6.3867, Val Cor=0.6407, Time=0.1608 sec\n",
      "Epoch [173 / 2000]: Train Loss=6.2736, Val Cor=0.6364, Time=0.1618 sec\n",
      "Epoch [174 / 2000]: Train Loss=6.2259, Val Cor=0.6350, Time=0.1615 sec\n",
      "Epoch [175 / 2000]: Train Loss=6.5618, Val Cor=0.6370, Time=0.1617 sec\n",
      "Epoch [176 / 2000]: Train Loss=6.2975, Val Cor=0.6373, Time=0.1613 sec\n",
      "Epoch [177 / 2000]: Train Loss=6.2504, Val Cor=0.6384, Time=0.1602 sec\n",
      "Epoch [178 / 2000]: Train Loss=6.2811, Val Cor=0.6382, Time=0.1600 sec\n",
      "Epoch [179 / 2000]: Train Loss=6.3191, Val Cor=0.6442, Time=0.1602 sec\n",
      "Epoch [180 / 2000]: Train Loss=6.5690, Val Cor=0.6437, Time=0.1597 sec\n",
      "Epoch [181 / 2000]: Train Loss=6.2456, Val Cor=0.6473, Time=0.1601 sec\n",
      "Epoch [182 / 2000]: Train Loss=6.7274, Val Cor=0.6476, Time=0.1602 sec\n",
      "Epoch [183 / 2000]: Train Loss=6.5158, Val Cor=0.6470, Time=0.1603 sec\n",
      "Epoch [184 / 2000]: Train Loss=6.3029, Val Cor=0.6437, Time=0.1613 sec\n",
      "Epoch [185 / 2000]: Train Loss=6.3113, Val Cor=0.6473, Time=0.1620 sec\n",
      "Epoch [186 / 2000]: Train Loss=6.3018, Val Cor=0.6434, Time=0.1614 sec\n",
      "Epoch [187 / 2000]: Train Loss=6.2103, Val Cor=0.6505, Time=0.1617 sec\n",
      "Epoch [188 / 2000]: Train Loss=6.2186, Val Cor=0.6488, Time=0.1613 sec\n",
      "Epoch [189 / 2000]: Train Loss=6.5765, Val Cor=0.6438, Time=0.1621 sec\n",
      "Epoch [190 / 2000]: Train Loss=6.6892, Val Cor=0.6475, Time=0.1616 sec\n",
      "Epoch [191 / 2000]: Train Loss=6.3671, Val Cor=0.6451, Time=0.1599 sec\n",
      "Epoch [192 / 2000]: Train Loss=6.5334, Val Cor=0.6533, Time=0.1594 sec\n",
      "Epoch [193 / 2000]: Train Loss=6.2055, Val Cor=0.6479, Time=0.1596 sec\n",
      "Epoch [194 / 2000]: Train Loss=6.1349, Val Cor=0.6492, Time=0.1597 sec\n",
      "Epoch [195 / 2000]: Train Loss=6.3186, Val Cor=0.6506, Time=0.1599 sec\n",
      "Epoch [196 / 2000]: Train Loss=6.4587, Val Cor=0.6542, Time=0.1601 sec\n",
      "Epoch [197 / 2000]: Train Loss=6.3963, Val Cor=0.6511, Time=0.1601 sec\n",
      "Epoch [198 / 2000]: Train Loss=6.7182, Val Cor=0.6501, Time=0.1611 sec\n",
      "Epoch [199 / 2000]: Train Loss=6.2683, Val Cor=0.6430, Time=0.1615 sec\n",
      "Epoch [200 / 2000]: Train Loss=6.3962, Val Cor=0.6532, Time=0.1614 sec\n",
      "Epoch [201 / 2000]: Train Loss=6.1445, Val Cor=0.6517, Time=0.1610 sec\n",
      "Epoch [202 / 2000]: Train Loss=6.0859, Val Cor=0.6480, Time=0.1597 sec\n",
      "Epoch [203 / 2000]: Train Loss=6.1280, Val Cor=0.6489, Time=0.1595 sec\n",
      "Epoch [204 / 2000]: Train Loss=6.8584, Val Cor=0.6537, Time=0.1601 sec\n",
      "Epoch [205 / 2000]: Train Loss=6.4019, Val Cor=0.6524, Time=0.1603 sec\n",
      "Epoch [206 / 2000]: Train Loss=6.3240, Val Cor=0.6518, Time=0.1598 sec\n",
      "Epoch [207 / 2000]: Train Loss=6.3416, Val Cor=0.6542, Time=0.1610 sec\n",
      "Epoch [208 / 2000]: Train Loss=6.2739, Val Cor=0.6529, Time=0.1607 sec\n",
      "Epoch [209 / 2000]: Train Loss=6.4384, Val Cor=0.6594, Time=0.1611 sec\n",
      "Epoch [210 / 2000]: Train Loss=6.0575, Val Cor=0.6517, Time=0.1605 sec\n",
      "Epoch [211 / 2000]: Train Loss=6.3051, Val Cor=0.6587, Time=0.1614 sec\n",
      "Epoch [212 / 2000]: Train Loss=6.0434, Val Cor=0.6608, Time=0.1611 sec\n",
      "Epoch [213 / 2000]: Train Loss=6.3843, Val Cor=0.6536, Time=0.1614 sec\n",
      "Epoch [214 / 2000]: Train Loss=6.0582, Val Cor=0.6599, Time=0.1611 sec\n",
      "Epoch [215 / 2000]: Train Loss=6.4204, Val Cor=0.6460, Time=0.1597 sec\n",
      "Epoch [216 / 2000]: Train Loss=6.2203, Val Cor=0.6514, Time=0.1594 sec\n",
      "Epoch [217 / 2000]: Train Loss=6.2015, Val Cor=0.6508, Time=0.1601 sec\n",
      "Epoch [218 / 2000]: Train Loss=6.0737, Val Cor=0.6536, Time=0.1599 sec\n",
      "Epoch [219 / 2000]: Train Loss=6.0670, Val Cor=0.6568, Time=0.1600 sec\n",
      "Epoch [220 / 2000]: Train Loss=6.2015, Val Cor=0.6570, Time=0.1611 sec\n",
      "Epoch [221 / 2000]: Train Loss=6.4573, Val Cor=0.6606, Time=0.1611 sec\n",
      "Epoch [222 / 2000]: Train Loss=6.4726, Val Cor=0.6586, Time=0.1614 sec\n",
      "Epoch [223 / 2000]: Train Loss=6.1680, Val Cor=0.6572, Time=0.1616 sec\n",
      "Epoch [224 / 2000]: Train Loss=6.1974, Val Cor=0.6630, Time=0.1601 sec\n",
      "Epoch [225 / 2000]: Train Loss=6.0433, Val Cor=0.6611, Time=0.1603 sec\n",
      "Epoch [226 / 2000]: Train Loss=6.2699, Val Cor=0.6619, Time=0.1600 sec\n",
      "Epoch [227 / 2000]: Train Loss=6.0536, Val Cor=0.6591, Time=0.1603 sec\n",
      "Epoch [228 / 2000]: Train Loss=5.8472, Val Cor=0.6586, Time=0.1607 sec\n",
      "Epoch [229 / 2000]: Train Loss=6.1722, Val Cor=0.6643, Time=0.1612 sec\n",
      "Epoch [230 / 2000]: Train Loss=6.1832, Val Cor=0.6576, Time=0.1606 sec\n",
      "Epoch [231 / 2000]: Train Loss=6.2763, Val Cor=0.6566, Time=0.1617 sec\n",
      "Epoch [232 / 2000]: Train Loss=6.4675, Val Cor=0.6649, Time=0.1615 sec\n",
      "Epoch [233 / 2000]: Train Loss=6.1239, Val Cor=0.6635, Time=0.1616 sec\n",
      "Epoch [234 / 2000]: Train Loss=5.9440, Val Cor=0.6635, Time=0.1618 sec\n",
      "Epoch [235 / 2000]: Train Loss=6.1730, Val Cor=0.6614, Time=0.1620 sec\n",
      "Epoch [236 / 2000]: Train Loss=5.9644, Val Cor=0.6701, Time=0.1614 sec\n",
      "Epoch [237 / 2000]: Train Loss=6.0161, Val Cor=0.6582, Time=0.1611 sec\n",
      "Epoch [238 / 2000]: Train Loss=6.0031, Val Cor=0.6658, Time=0.1597 sec\n",
      "Epoch [239 / 2000]: Train Loss=6.2046, Val Cor=0.6658, Time=0.1598 sec\n",
      "Epoch [240 / 2000]: Train Loss=6.2686, Val Cor=0.6566, Time=0.1599 sec\n",
      "Epoch [241 / 2000]: Train Loss=6.2311, Val Cor=0.6542, Time=0.1591 sec\n",
      "Epoch [242 / 2000]: Train Loss=6.0990, Val Cor=0.6667, Time=0.1591 sec\n",
      "Epoch [243 / 2000]: Train Loss=5.8934, Val Cor=0.6653, Time=0.1595 sec\n",
      "Epoch [244 / 2000]: Train Loss=6.1651, Val Cor=0.6497, Time=0.1608 sec\n",
      "Epoch [245 / 2000]: Train Loss=6.0632, Val Cor=0.6704, Time=0.1607 sec\n",
      "Epoch [246 / 2000]: Train Loss=6.4869, Val Cor=0.6593, Time=0.1604 sec\n",
      "Epoch [247 / 2000]: Train Loss=6.0220, Val Cor=0.6671, Time=0.1611 sec\n",
      "Epoch [248 / 2000]: Train Loss=5.9806, Val Cor=0.6651, Time=0.1607 sec\n",
      "Epoch [249 / 2000]: Train Loss=6.1318, Val Cor=0.6664, Time=0.1596 sec\n",
      "Epoch [250 / 2000]: Train Loss=6.0384, Val Cor=0.6668, Time=0.1595 sec\n",
      "Epoch [251 / 2000]: Train Loss=5.8936, Val Cor=0.6646, Time=0.1595 sec\n",
      "Epoch [252 / 2000]: Train Loss=6.0139, Val Cor=0.6662, Time=0.1597 sec\n",
      "Epoch [253 / 2000]: Train Loss=6.0370, Val Cor=0.6661, Time=0.1601 sec\n",
      "Epoch [254 / 2000]: Train Loss=6.0292, Val Cor=0.6689, Time=0.1598 sec\n",
      "Epoch [255 / 2000]: Train Loss=6.1990, Val Cor=0.6609, Time=0.1598 sec\n",
      "Epoch [256 / 2000]: Train Loss=6.0203, Val Cor=0.6689, Time=0.1598 sec\n",
      "Epoch [257 / 2000]: Train Loss=6.1329, Val Cor=0.6654, Time=0.1599 sec\n",
      "Epoch [258 / 2000]: Train Loss=5.8681, Val Cor=0.6685, Time=0.1599 sec\n",
      "Epoch [259 / 2000]: Train Loss=5.9475, Val Cor=0.6669, Time=0.1615 sec\n",
      "Epoch [260 / 2000]: Train Loss=5.8947, Val Cor=0.6687, Time=0.1615 sec\n",
      "Epoch [261 / 2000]: Train Loss=5.9312, Val Cor=0.6696, Time=0.1613 sec\n",
      "Epoch [262 / 2000]: Train Loss=5.8852, Val Cor=0.6700, Time=0.1618 sec\n",
      "Epoch [263 / 2000]: Train Loss=5.8376, Val Cor=0.6717, Time=0.1618 sec\n",
      "Epoch [264 / 2000]: Train Loss=5.7110, Val Cor=0.6689, Time=0.1615 sec\n",
      "Epoch [265 / 2000]: Train Loss=6.2051, Val Cor=0.6624, Time=0.1619 sec\n",
      "Epoch [266 / 2000]: Train Loss=5.7576, Val Cor=0.6716, Time=0.1605 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [267 / 2000]: Train Loss=6.0956, Val Cor=0.6723, Time=0.1605 sec\n",
      "Epoch [268 / 2000]: Train Loss=6.3954, Val Cor=0.6636, Time=0.1607 sec\n",
      "Epoch [269 / 2000]: Train Loss=6.0178, Val Cor=0.6678, Time=0.1618 sec\n",
      "Epoch [270 / 2000]: Train Loss=6.1109, Val Cor=0.6690, Time=0.1618 sec\n",
      "Epoch [271 / 2000]: Train Loss=6.0216, Val Cor=0.6660, Time=0.1603 sec\n",
      "Epoch [272 / 2000]: Train Loss=5.9140, Val Cor=0.6655, Time=0.1598 sec\n",
      "Epoch [273 / 2000]: Train Loss=5.8027, Val Cor=0.6724, Time=0.1610 sec\n",
      "Epoch [274 / 2000]: Train Loss=5.8056, Val Cor=0.6735, Time=0.1612 sec\n",
      "Epoch [275 / 2000]: Train Loss=5.6803, Val Cor=0.6682, Time=0.1621 sec\n",
      "Epoch [276 / 2000]: Train Loss=6.5529, Val Cor=0.5574, Time=0.1611 sec\n",
      "Epoch [277 / 2000]: Train Loss=6.1949, Val Cor=0.6601, Time=0.1629 sec\n",
      "Epoch [278 / 2000]: Train Loss=5.9610, Val Cor=0.6693, Time=0.1614 sec\n",
      "Epoch [279 / 2000]: Train Loss=5.9701, Val Cor=0.6696, Time=0.1607 sec\n",
      "Epoch [280 / 2000]: Train Loss=5.9264, Val Cor=0.6727, Time=0.1601 sec\n",
      "Epoch [281 / 2000]: Train Loss=5.9799, Val Cor=0.6600, Time=0.1606 sec\n",
      "Epoch [282 / 2000]: Train Loss=5.9147, Val Cor=0.6610, Time=0.1605 sec\n",
      "Epoch [283 / 2000]: Train Loss=5.9655, Val Cor=0.6699, Time=0.1610 sec\n",
      "Epoch [284 / 2000]: Train Loss=5.9393, Val Cor=0.6697, Time=0.1601 sec\n",
      "Epoch [285 / 2000]: Train Loss=6.1079, Val Cor=0.6697, Time=0.1606 sec\n",
      "Epoch [286 / 2000]: Train Loss=6.1026, Val Cor=0.6601, Time=0.1605 sec\n",
      "Epoch [287 / 2000]: Train Loss=5.9939, Val Cor=0.6658, Time=0.1601 sec\n",
      "Epoch [288 / 2000]: Train Loss=6.0045, Val Cor=0.6576, Time=0.1590 sec\n",
      "Epoch [289 / 2000]: Train Loss=5.8707, Val Cor=0.6591, Time=0.1597 sec\n",
      "Epoch [290 / 2000]: Train Loss=6.0391, Val Cor=0.6693, Time=0.1595 sec\n",
      "Epoch [291 / 2000]: Train Loss=6.1099, Val Cor=0.6739, Time=0.1596 sec\n",
      "Epoch [292 / 2000]: Train Loss=6.0399, Val Cor=0.6715, Time=0.1595 sec\n",
      "Epoch [293 / 2000]: Train Loss=5.7430, Val Cor=0.6707, Time=0.1611 sec\n",
      "Epoch [294 / 2000]: Train Loss=5.8541, Val Cor=0.6694, Time=0.1609 sec\n",
      "Epoch [295 / 2000]: Train Loss=5.7891, Val Cor=0.6649, Time=0.1611 sec\n",
      "Epoch [296 / 2000]: Train Loss=5.8960, Val Cor=0.6677, Time=0.1611 sec\n",
      "Epoch [297 / 2000]: Train Loss=6.0945, Val Cor=0.6705, Time=0.1609 sec\n",
      "Epoch [298 / 2000]: Train Loss=6.5294, Val Cor=0.3371, Time=0.1611 sec\n",
      "Epoch [299 / 2000]: Train Loss=6.2436, Val Cor=0.5681, Time=0.1605 sec\n",
      "Epoch [300 / 2000]: Train Loss=6.3268, Val Cor=0.6761, Time=0.1601 sec\n",
      "Epoch [301 / 2000]: Train Loss=5.9895, Val Cor=0.6730, Time=0.1602 sec\n",
      "Epoch [302 / 2000]: Train Loss=5.8450, Val Cor=0.6724, Time=0.1598 sec\n",
      "Epoch [303 / 2000]: Train Loss=5.9771, Val Cor=0.6677, Time=0.1602 sec\n",
      "Epoch [304 / 2000]: Train Loss=6.1377, Val Cor=0.6805, Time=0.1607 sec\n",
      "Epoch [305 / 2000]: Train Loss=5.8873, Val Cor=0.6738, Time=0.1603 sec\n",
      "Epoch [306 / 2000]: Train Loss=5.7523, Val Cor=0.6820, Time=0.1612 sec\n",
      "Epoch [307 / 2000]: Train Loss=5.7117, Val Cor=0.6751, Time=0.1615 sec\n",
      "Epoch [308 / 2000]: Train Loss=6.0868, Val Cor=0.6321, Time=0.1614 sec\n",
      "Epoch [309 / 2000]: Train Loss=6.0381, Val Cor=0.6741, Time=0.1614 sec\n",
      "Epoch [310 / 2000]: Train Loss=5.8590, Val Cor=0.6737, Time=0.1618 sec\n",
      "Epoch [311 / 2000]: Train Loss=5.9878, Val Cor=0.6740, Time=0.1617 sec\n",
      "Epoch [312 / 2000]: Train Loss=5.8990, Val Cor=0.6741, Time=0.1602 sec\n",
      "Epoch [313 / 2000]: Train Loss=5.8132, Val Cor=0.6776, Time=0.1598 sec\n",
      "Epoch [314 / 2000]: Train Loss=5.9502, Val Cor=0.6733, Time=0.1601 sec\n",
      "Epoch [315 / 2000]: Train Loss=6.0358, Val Cor=0.6762, Time=0.1606 sec\n",
      "Epoch [316 / 2000]: Train Loss=6.1413, Val Cor=0.6656, Time=0.1613 sec\n",
      "Epoch [317 / 2000]: Train Loss=5.7661, Val Cor=0.6726, Time=0.1614 sec\n",
      "Epoch [318 / 2000]: Train Loss=5.8329, Val Cor=0.6747, Time=0.1612 sec\n",
      "Epoch [319 / 2000]: Train Loss=5.7373, Val Cor=0.6728, Time=0.1618 sec\n",
      "Epoch [320 / 2000]: Train Loss=5.8346, Val Cor=0.6796, Time=0.1615 sec\n",
      "Epoch [321 / 2000]: Train Loss=5.6757, Val Cor=0.6731, Time=0.1596 sec\n",
      "Epoch [322 / 2000]: Train Loss=5.9868, Val Cor=0.6624, Time=0.1593 sec\n",
      "Epoch [323 / 2000]: Train Loss=6.2244, Val Cor=0.6772, Time=0.1598 sec\n",
      "Epoch [324 / 2000]: Train Loss=5.9626, Val Cor=0.6834, Time=0.1599 sec\n",
      "Epoch [325 / 2000]: Train Loss=5.9186, Val Cor=0.6805, Time=0.1596 sec\n",
      "Epoch [326 / 2000]: Train Loss=5.7054, Val Cor=0.6825, Time=0.1610 sec\n",
      "Epoch [327 / 2000]: Train Loss=5.7295, Val Cor=0.6706, Time=0.1609 sec\n",
      "Epoch [328 / 2000]: Train Loss=6.0295, Val Cor=0.6716, Time=0.1613 sec\n",
      "Epoch [329 / 2000]: Train Loss=6.3285, Val Cor=0.6617, Time=0.1610 sec\n",
      "Epoch [330 / 2000]: Train Loss=5.8137, Val Cor=0.6731, Time=0.1604 sec\n",
      "Epoch [331 / 2000]: Train Loss=5.9021, Val Cor=0.6700, Time=0.1600 sec\n",
      "Epoch [332 / 2000]: Train Loss=6.0475, Val Cor=0.6689, Time=0.1598 sec\n",
      "Epoch [333 / 2000]: Train Loss=5.7644, Val Cor=0.6720, Time=0.1601 sec\n",
      "Epoch [334 / 2000]: Train Loss=5.9187, Val Cor=0.6619, Time=0.1600 sec\n",
      "Epoch [335 / 2000]: Train Loss=5.8713, Val Cor=0.6746, Time=0.1602 sec\n",
      "Epoch [336 / 2000]: Train Loss=5.6356, Val Cor=0.6696, Time=0.1604 sec\n",
      "Epoch [337 / 2000]: Train Loss=5.9072, Val Cor=0.6736, Time=0.1610 sec\n",
      "Epoch [338 / 2000]: Train Loss=6.0631, Val Cor=0.6772, Time=0.1613 sec\n",
      "Epoch [339 / 2000]: Train Loss=5.9489, Val Cor=0.5628, Time=0.1619 sec\n",
      "Epoch [340 / 2000]: Train Loss=5.9309, Val Cor=0.6763, Time=0.1612 sec\n",
      "Epoch [341 / 2000]: Train Loss=5.9596, Val Cor=0.6705, Time=0.1601 sec\n",
      "Epoch [342 / 2000]: Train Loss=5.9096, Val Cor=0.6839, Time=0.1602 sec\n",
      "Epoch [343 / 2000]: Train Loss=5.8379, Val Cor=0.6711, Time=0.1604 sec\n",
      "Epoch [344 / 2000]: Train Loss=6.0212, Val Cor=0.6835, Time=0.1604 sec\n",
      "Epoch [345 / 2000]: Train Loss=5.9984, Val Cor=0.6768, Time=0.1607 sec\n",
      "Epoch [346 / 2000]: Train Loss=5.7288, Val Cor=0.6722, Time=0.1616 sec\n",
      "Epoch [347 / 2000]: Train Loss=5.7004, Val Cor=0.6718, Time=0.1618 sec\n",
      "Epoch [348 / 2000]: Train Loss=5.8189, Val Cor=0.6748, Time=0.1613 sec\n",
      "Epoch [349 / 2000]: Train Loss=5.8882, Val Cor=0.6731, Time=0.1624 sec\n",
      "Epoch [350 / 2000]: Train Loss=5.9664, Val Cor=0.6760, Time=0.1629 sec\n",
      "Epoch [351 / 2000]: Train Loss=6.0408, Val Cor=0.6754, Time=0.1604 sec\n",
      "Epoch [352 / 2000]: Train Loss=5.8528, Val Cor=0.6727, Time=0.1603 sec\n",
      "Epoch [353 / 2000]: Train Loss=5.8055, Val Cor=0.6747, Time=0.1602 sec\n",
      "Epoch [354 / 2000]: Train Loss=5.8579, Val Cor=0.6757, Time=0.1601 sec\n",
      "Epoch [355 / 2000]: Train Loss=5.8350, Val Cor=0.6735, Time=0.1606 sec\n",
      "Epoch [356 / 2000]: Train Loss=5.9198, Val Cor=0.6725, Time=0.1608 sec\n",
      "Epoch [357 / 2000]: Train Loss=5.6843, Val Cor=0.6487, Time=0.1618 sec\n",
      "Epoch [358 / 2000]: Train Loss=5.7373, Val Cor=0.6619, Time=0.1613 sec\n",
      "Epoch [359 / 2000]: Train Loss=5.8333, Val Cor=0.6765, Time=0.1615 sec\n",
      "Epoch [360 / 2000]: Train Loss=5.9155, Val Cor=0.6728, Time=0.1619 sec\n",
      "Epoch [361 / 2000]: Train Loss=5.8597, Val Cor=0.6709, Time=0.1613 sec\n",
      "Epoch [362 / 2000]: Train Loss=6.0311, Val Cor=0.4654, Time=0.1599 sec\n",
      "Epoch [363 / 2000]: Train Loss=6.0010, Val Cor=0.6708, Time=0.1595 sec\n",
      "Epoch [364 / 2000]: Train Loss=5.7804, Val Cor=0.6691, Time=0.1601 sec\n",
      "Epoch [365 / 2000]: Train Loss=5.7739, Val Cor=0.6756, Time=0.1596 sec\n",
      "Epoch [366 / 2000]: Train Loss=5.8025, Val Cor=0.6738, Time=0.1597 sec\n",
      "Epoch [367 / 2000]: Train Loss=5.9690, Val Cor=0.6764, Time=0.1601 sec\n",
      "Epoch [368 / 2000]: Train Loss=5.7056, Val Cor=0.6784, Time=0.1603 sec\n",
      "Epoch [369 / 2000]: Train Loss=5.7406, Val Cor=0.6755, Time=0.1613 sec\n",
      "Epoch [370 / 2000]: Train Loss=5.6788, Val Cor=0.6756, Time=0.1612 sec\n",
      "Epoch [371 / 2000]: Train Loss=5.8529, Val Cor=0.6701, Time=0.1617 sec\n",
      "Epoch [372 / 2000]: Train Loss=5.9156, Val Cor=0.6675, Time=0.1612 sec\n",
      "Epoch [373 / 2000]: Train Loss=5.9265, Val Cor=0.6696, Time=0.1603 sec\n",
      "Epoch [374 / 2000]: Train Loss=5.7981, Val Cor=0.6735, Time=0.1603 sec\n",
      "Epoch [375 / 2000]: Train Loss=5.8652, Val Cor=0.6723, Time=0.1606 sec\n",
      "Epoch [376 / 2000]: Train Loss=6.0437, Val Cor=0.6420, Time=0.1603 sec\n",
      "Epoch [377 / 2000]: Train Loss=5.9077, Val Cor=0.6476, Time=0.1608 sec\n",
      "Epoch [378 / 2000]: Train Loss=6.2334, Val Cor=0.6288, Time=0.1618 sec\n",
      "Epoch [379 / 2000]: Train Loss=5.9002, Val Cor=0.2071, Time=0.1618 sec\n",
      "Epoch [380 / 2000]: Train Loss=6.1044, Val Cor=0.4256, Time=0.1618 sec\n",
      "Epoch [381 / 2000]: Train Loss=5.8547, Val Cor=0.6808, Time=0.1617 sec\n",
      "Epoch [382 / 2000]: Train Loss=5.7931, Val Cor=0.6693, Time=0.1603 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [383 / 2000]: Train Loss=5.9589, Val Cor=0.5689, Time=0.1602 sec\n",
      "Epoch [384 / 2000]: Train Loss=5.9242, Val Cor=0.6193, Time=0.1604 sec\n",
      "Epoch [385 / 2000]: Train Loss=6.0324, Val Cor=0.6693, Time=0.1607 sec\n",
      "Epoch [386 / 2000]: Train Loss=5.9801, Val Cor=0.5867, Time=0.1603 sec\n",
      "Epoch [387 / 2000]: Train Loss=5.9821, Val Cor=0.6689, Time=0.1615 sec\n",
      "Epoch [388 / 2000]: Train Loss=6.0762, Val Cor=0.6685, Time=0.1608 sec\n",
      "Epoch [389 / 2000]: Train Loss=5.9512, Val Cor=0.6733, Time=0.1611 sec\n",
      "Epoch [390 / 2000]: Train Loss=6.1270, Val Cor=0.6752, Time=0.1612 sec\n",
      "Epoch [391 / 2000]: Train Loss=5.8947, Val Cor=0.6802, Time=0.1611 sec\n",
      "Epoch [392 / 2000]: Train Loss=5.8058, Val Cor=0.6654, Time=0.1597 sec\n",
      "Epoch [393 / 2000]: Train Loss=5.5484, Val Cor=0.6605, Time=0.1598 sec\n",
      "Epoch [394 / 2000]: Train Loss=6.0811, Val Cor=0.6720, Time=0.1595 sec\n",
      "Epoch [395 / 2000]: Train Loss=5.9382, Val Cor=0.6752, Time=0.1599 sec\n",
      "Epoch [396 / 2000]: Train Loss=5.7853, Val Cor=0.6635, Time=0.1595 sec\n",
      "Epoch [397 / 2000]: Train Loss=6.1879, Val Cor=-0.0382, Time=0.1601 sec\n",
      "Epoch [398 / 2000]: Train Loss=6.0493, Val Cor=0.6760, Time=0.1610 sec\n",
      "Epoch [399 / 2000]: Train Loss=5.8714, Val Cor=0.6705, Time=0.1611 sec\n",
      "Epoch [400 / 2000]: Train Loss=6.0350, Val Cor=0.6091, Time=0.1618 sec\n",
      "Epoch [401 / 2000]: Train Loss=6.0217, Val Cor=0.6316, Time=0.1616 sec\n",
      "Epoch [402 / 2000]: Train Loss=5.7455, Val Cor=0.6761, Time=0.1608 sec\n",
      "Epoch [403 / 2000]: Train Loss=5.6855, Val Cor=0.6669, Time=0.1606 sec\n",
      "Epoch [404 / 2000]: Train Loss=5.5747, Val Cor=0.6739, Time=0.1602 sec\n",
      "Epoch [405 / 2000]: Train Loss=5.5871, Val Cor=0.6759, Time=0.1601 sec\n",
      "Epoch [406 / 2000]: Train Loss=5.7881, Val Cor=0.6236, Time=0.1600 sec\n",
      "Epoch [407 / 2000]: Train Loss=6.2950, Val Cor=-0.2797, Time=0.1612 sec\n",
      "Epoch [408 / 2000]: Train Loss=6.0774, Val Cor=0.6798, Time=0.1609 sec\n",
      "Epoch [409 / 2000]: Train Loss=5.9526, Val Cor=0.6842, Time=0.1611 sec\n",
      "Epoch [410 / 2000]: Train Loss=5.8511, Val Cor=0.6507, Time=0.1606 sec\n",
      "Epoch [411 / 2000]: Train Loss=5.9335, Val Cor=0.6656, Time=0.1614 sec\n",
      "Epoch [412 / 2000]: Train Loss=5.6602, Val Cor=0.6819, Time=0.1609 sec\n",
      "Epoch [413 / 2000]: Train Loss=5.6955, Val Cor=0.6740, Time=0.1599 sec\n",
      "Epoch [414 / 2000]: Train Loss=5.6995, Val Cor=0.6707, Time=0.1597 sec\n",
      "Epoch [415 / 2000]: Train Loss=5.8467, Val Cor=0.6700, Time=0.1601 sec\n",
      "Epoch [416 / 2000]: Train Loss=5.8321, Val Cor=0.6566, Time=0.1599 sec\n",
      "Epoch [417 / 2000]: Train Loss=5.8604, Val Cor=0.6787, Time=0.1612 sec\n",
      "Epoch [418 / 2000]: Train Loss=5.6000, Val Cor=0.6752, Time=0.1612 sec\n",
      "Epoch [419 / 2000]: Train Loss=5.5749, Val Cor=0.6733, Time=0.1618 sec\n",
      "Epoch [420 / 2000]: Train Loss=6.1509, Val Cor=0.6757, Time=0.1614 sec\n",
      "Epoch [421 / 2000]: Train Loss=5.6148, Val Cor=0.6730, Time=0.1604 sec\n",
      "Epoch [422 / 2000]: Train Loss=5.4944, Val Cor=0.6725, Time=0.1601 sec\n",
      "Epoch [423 / 2000]: Train Loss=5.7284, Val Cor=0.6756, Time=0.1606 sec\n",
      "Epoch [424 / 2000]: Train Loss=5.5723, Val Cor=0.6343, Time=0.1604 sec\n",
      "Epoch [425 / 2000]: Train Loss=6.3407, Val Cor=-0.5419, Time=0.1606 sec\n",
      "Epoch [426 / 2000]: Train Loss=6.1213, Val Cor=0.6769, Time=0.1607 sec\n",
      "Epoch [427 / 2000]: Train Loss=5.8263, Val Cor=0.6760, Time=0.1620 sec\n",
      "Epoch [428 / 2000]: Train Loss=5.9058, Val Cor=0.6809, Time=0.1615 sec\n",
      "Epoch [429 / 2000]: Train Loss=5.8073, Val Cor=0.6720, Time=0.1618 sec\n",
      "Epoch [430 / 2000]: Train Loss=5.6497, Val Cor=0.6664, Time=0.1623 sec\n",
      "Epoch [431 / 2000]: Train Loss=6.0238, Val Cor=0.6675, Time=0.1617 sec\n",
      "Epoch [432 / 2000]: Train Loss=5.9840, Val Cor=0.6748, Time=0.1617 sec\n",
      "Epoch [433 / 2000]: Train Loss=5.8713, Val Cor=0.6743, Time=0.1605 sec\n",
      "Epoch [434 / 2000]: Train Loss=5.6275, Val Cor=0.6752, Time=0.1602 sec\n",
      "Epoch [435 / 2000]: Train Loss=5.5767, Val Cor=0.6781, Time=0.1599 sec\n",
      "Epoch [436 / 2000]: Train Loss=5.6050, Val Cor=0.6766, Time=0.1599 sec\n",
      "Epoch [437 / 2000]: Train Loss=5.7575, Val Cor=0.6240, Time=0.1598 sec\n",
      "Epoch [438 / 2000]: Train Loss=5.7115, Val Cor=0.6748, Time=0.1600 sec\n",
      "Epoch [439 / 2000]: Train Loss=5.7430, Val Cor=0.6808, Time=0.1606 sec\n",
      "Epoch [440 / 2000]: Train Loss=5.9529, Val Cor=0.6741, Time=0.1610 sec\n",
      "Epoch [441 / 2000]: Train Loss=5.7843, Val Cor=0.6761, Time=0.1616 sec\n",
      "Epoch [442 / 2000]: Train Loss=5.8471, Val Cor=0.4310, Time=0.1611 sec\n",
      "Epoch [443 / 2000]: Train Loss=5.7546, Val Cor=0.6693, Time=0.1615 sec\n",
      "Epoch [444 / 2000]: Train Loss=5.5745, Val Cor=0.6770, Time=0.1607 sec\n",
      "Epoch [445 / 2000]: Train Loss=5.7412, Val Cor=0.6713, Time=0.1601 sec\n",
      "Epoch [446 / 2000]: Train Loss=5.8144, Val Cor=0.6747, Time=0.1598 sec\n",
      "Epoch [447 / 2000]: Train Loss=5.4985, Val Cor=0.6799, Time=0.1600 sec\n",
      "Epoch [448 / 2000]: Train Loss=5.4950, Val Cor=0.6715, Time=0.1596 sec\n",
      "Epoch [449 / 2000]: Train Loss=5.8155, Val Cor=0.6739, Time=0.1598 sec\n",
      "Epoch [450 / 2000]: Train Loss=5.7666, Val Cor=0.6795, Time=0.1601 sec\n",
      "Epoch [451 / 2000]: Train Loss=5.7210, Val Cor=0.6785, Time=0.1610 sec\n",
      "Epoch [452 / 2000]: Train Loss=5.9850, Val Cor=0.6794, Time=0.1612 sec\n",
      "Epoch [453 / 2000]: Train Loss=5.8049, Val Cor=0.6751, Time=0.1622 sec\n",
      "Epoch [454 / 2000]: Train Loss=5.5707, Val Cor=0.6787, Time=0.1616 sec\n",
      "Epoch [455 / 2000]: Train Loss=5.8304, Val Cor=0.6824, Time=0.1616 sec\n",
      "Epoch [456 / 2000]: Train Loss=5.6689, Val Cor=0.6819, Time=0.1608 sec\n",
      "Epoch [457 / 2000]: Train Loss=5.5967, Val Cor=0.5973, Time=0.1605 sec\n",
      "Epoch [458 / 2000]: Train Loss=5.6122, Val Cor=0.6724, Time=0.1604 sec\n",
      "Epoch [459 / 2000]: Train Loss=5.7299, Val Cor=0.6745, Time=0.1606 sec\n",
      "Epoch [460 / 2000]: Train Loss=5.9256, Val Cor=0.6781, Time=0.1606 sec\n",
      "Epoch [461 / 2000]: Train Loss=5.5857, Val Cor=0.6722, Time=0.1605 sec\n",
      "Epoch [462 / 2000]: Train Loss=5.6003, Val Cor=0.6689, Time=0.1609 sec\n",
      "Epoch [463 / 2000]: Train Loss=5.8204, Val Cor=0.6756, Time=0.1612 sec\n",
      "Epoch [464 / 2000]: Train Loss=5.7277, Val Cor=0.6771, Time=0.1617 sec\n",
      "Epoch [465 / 2000]: Train Loss=5.7115, Val Cor=0.6536, Time=0.1614 sec\n",
      "Epoch [466 / 2000]: Train Loss=5.6109, Val Cor=0.6740, Time=0.1598 sec\n",
      "Epoch [467 / 2000]: Train Loss=5.7350, Val Cor=0.6726, Time=0.1598 sec\n",
      "Epoch [468 / 2000]: Train Loss=6.2400, Val Cor=0.6576, Time=0.1596 sec\n",
      "Epoch [469 / 2000]: Train Loss=5.9564, Val Cor=0.6672, Time=0.1599 sec\n",
      "Epoch [470 / 2000]: Train Loss=5.5424, Val Cor=0.6803, Time=0.1597 sec\n",
      "Epoch [471 / 2000]: Train Loss=5.5295, Val Cor=0.6793, Time=0.1597 sec\n",
      "Epoch [472 / 2000]: Train Loss=5.8898, Val Cor=0.6790, Time=0.1609 sec\n",
      "Epoch [473 / 2000]: Train Loss=6.0977, Val Cor=0.6459, Time=0.1611 sec\n",
      "Epoch [474 / 2000]: Train Loss=5.8533, Val Cor=0.6808, Time=0.1597 sec\n",
      "Epoch [475 / 2000]: Train Loss=5.5137, Val Cor=0.6695, Time=0.1594 sec\n",
      "Epoch [476 / 2000]: Train Loss=5.6256, Val Cor=0.6754, Time=0.1597 sec\n",
      "Epoch [477 / 2000]: Train Loss=5.6285, Val Cor=0.6748, Time=0.1599 sec\n",
      "Epoch [478 / 2000]: Train Loss=5.6443, Val Cor=0.6736, Time=0.1595 sec\n",
      "Epoch [479 / 2000]: Train Loss=5.9244, Val Cor=-0.5967, Time=0.1601 sec\n",
      "Epoch [480 / 2000]: Train Loss=5.6955, Val Cor=0.5126, Time=0.1612 sec\n",
      "Epoch [481 / 2000]: Train Loss=5.6947, Val Cor=0.6733, Time=0.1609 sec\n",
      "Epoch [482 / 2000]: Train Loss=5.4929, Val Cor=0.6774, Time=0.1617 sec\n",
      "Epoch [483 / 2000]: Train Loss=5.6721, Val Cor=0.6779, Time=0.1615 sec\n",
      "Epoch [484 / 2000]: Train Loss=5.5055, Val Cor=0.6767, Time=0.1601 sec\n",
      "Epoch [485 / 2000]: Train Loss=5.6085, Val Cor=0.6788, Time=0.1598 sec\n",
      "Epoch [486 / 2000]: Train Loss=5.5326, Val Cor=0.6730, Time=0.1602 sec\n",
      "Epoch [487 / 2000]: Train Loss=5.5887, Val Cor=0.5691, Time=0.1604 sec\n",
      "Epoch [488 / 2000]: Train Loss=5.4982, Val Cor=0.6748, Time=0.1607 sec\n",
      "Epoch [489 / 2000]: Train Loss=5.4731, Val Cor=0.6752, Time=0.1615 sec\n",
      "Epoch [490 / 2000]: Train Loss=5.6890, Val Cor=0.6755, Time=0.1613 sec\n",
      "Epoch [491 / 2000]: Train Loss=5.4162, Val Cor=0.6776, Time=0.1615 sec\n",
      "Epoch [492 / 2000]: Train Loss=5.5022, Val Cor=0.6726, Time=0.1617 sec\n",
      "Epoch [493 / 2000]: Train Loss=5.7820, Val Cor=-0.5953, Time=0.1622 sec\n",
      "Epoch [494 / 2000]: Train Loss=6.2292, Val Cor=-0.6037, Time=0.1620 sec\n",
      "Epoch [495 / 2000]: Train Loss=6.1804, Val Cor=-0.4939, Time=0.1624 sec\n",
      "Epoch [496 / 2000]: Train Loss=5.6561, Val Cor=0.6573, Time=0.1602 sec\n",
      "Epoch [497 / 2000]: Train Loss=5.9329, Val Cor=0.6716, Time=0.1599 sec\n",
      "Epoch [498 / 2000]: Train Loss=5.5227, Val Cor=0.6694, Time=0.1598 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [499 / 2000]: Train Loss=5.7748, Val Cor=0.4250, Time=0.1602 sec\n",
      "Epoch [500 / 2000]: Train Loss=5.7304, Val Cor=0.6139, Time=0.1612 sec\n",
      "Epoch [501 / 2000]: Train Loss=5.6474, Val Cor=0.6737, Time=0.1611 sec\n",
      "Epoch [502 / 2000]: Train Loss=5.6117, Val Cor=0.6770, Time=0.1613 sec\n",
      "Epoch [503 / 2000]: Train Loss=5.8042, Val Cor=0.6123, Time=0.1610 sec\n",
      "Epoch [504 / 2000]: Train Loss=5.7504, Val Cor=0.6729, Time=0.1610 sec\n",
      "Epoch [505 / 2000]: Train Loss=5.5027, Val Cor=0.6747, Time=0.1603 sec\n",
      "Epoch [506 / 2000]: Train Loss=5.4645, Val Cor=0.6212, Time=0.1597 sec\n",
      "Epoch [507 / 2000]: Train Loss=5.6866, Val Cor=0.6715, Time=0.1601 sec\n",
      "Epoch [508 / 2000]: Train Loss=5.5661, Val Cor=0.6695, Time=0.1611 sec\n",
      "Epoch [509 / 2000]: Train Loss=5.6135, Val Cor=-0.0718, Time=0.1612 sec\n",
      "Epoch [510 / 2000]: Train Loss=5.6400, Val Cor=0.4727, Time=0.1616 sec\n",
      "Epoch [511 / 2000]: Train Loss=5.8023, Val Cor=0.6705, Time=0.1618 sec\n",
      "Epoch [512 / 2000]: Train Loss=5.7384, Val Cor=0.6743, Time=0.1615 sec\n",
      "Epoch [513 / 2000]: Train Loss=5.6700, Val Cor=-0.4361, Time=0.1611 sec\n",
      "Epoch [514 / 2000]: Train Loss=6.0909, Val Cor=0.6799, Time=0.1604 sec\n",
      "Epoch [515 / 2000]: Train Loss=5.6099, Val Cor=0.6687, Time=0.1603 sec\n",
      "Epoch [516 / 2000]: Train Loss=5.7164, Val Cor=0.6753, Time=0.1593 sec\n",
      "Epoch [517 / 2000]: Train Loss=5.8269, Val Cor=0.6786, Time=0.1602 sec\n",
      "Epoch [518 / 2000]: Train Loss=5.9072, Val Cor=0.6783, Time=0.1599 sec\n",
      "Epoch [519 / 2000]: Train Loss=5.5708, Val Cor=0.4319, Time=0.1599 sec\n",
      "Epoch [520 / 2000]: Train Loss=5.5353, Val Cor=0.6627, Time=0.1602 sec\n",
      "Epoch [521 / 2000]: Train Loss=5.5394, Val Cor=0.6729, Time=0.1612 sec\n",
      "Epoch [522 / 2000]: Train Loss=5.4600, Val Cor=0.6728, Time=0.1610 sec\n",
      "Epoch [523 / 2000]: Train Loss=5.4921, Val Cor=0.6783, Time=0.1609 sec\n",
      "Epoch [524 / 2000]: Train Loss=5.4774, Val Cor=0.6278, Time=0.1615 sec\n",
      "Epoch [525 / 2000]: Train Loss=5.9589, Val Cor=-0.5358, Time=0.1609 sec\n",
      "Epoch [526 / 2000]: Train Loss=5.6996, Val Cor=0.6782, Time=0.1606 sec\n",
      "Epoch [527 / 2000]: Train Loss=5.8695, Val Cor=0.3723, Time=0.1599 sec\n",
      "Epoch [528 / 2000]: Train Loss=5.8394, Val Cor=0.6813, Time=0.1596 sec\n",
      "Epoch [529 / 2000]: Train Loss=5.4540, Val Cor=0.6810, Time=0.1602 sec\n",
      "Epoch [530 / 2000]: Train Loss=5.4311, Val Cor=0.6758, Time=0.1603 sec\n",
      "Epoch [531 / 2000]: Train Loss=5.6370, Val Cor=0.6428, Time=0.1602 sec\n",
      "Epoch [532 / 2000]: Train Loss=5.4835, Val Cor=0.6762, Time=0.1605 sec\n",
      "Epoch [533 / 2000]: Train Loss=5.5670, Val Cor=0.6579, Time=0.1618 sec\n",
      "Epoch [534 / 2000]: Train Loss=5.7530, Val Cor=0.6178, Time=0.1612 sec\n",
      "Epoch [535 / 2000]: Train Loss=5.7469, Val Cor=0.6743, Time=0.1621 sec\n",
      "Epoch [536 / 2000]: Train Loss=5.6336, Val Cor=0.6717, Time=0.1614 sec\n",
      "Epoch [537 / 2000]: Train Loss=5.7547, Val Cor=0.6744, Time=0.1613 sec\n",
      "Epoch [538 / 2000]: Train Loss=5.6679, Val Cor=0.6698, Time=0.1604 sec\n",
      "Epoch [539 / 2000]: Train Loss=5.5850, Val Cor=0.6755, Time=0.1604 sec\n",
      "Epoch [540 / 2000]: Train Loss=5.4302, Val Cor=0.6706, Time=0.1603 sec\n",
      "Epoch [541 / 2000]: Train Loss=5.3683, Val Cor=0.6646, Time=0.1596 sec\n",
      "Epoch [542 / 2000]: Train Loss=5.3425, Val Cor=0.6702, Time=0.1602 sec\n",
      "Epoch [543 / 2000]: Train Loss=5.4314, Val Cor=0.6717, Time=0.1614 sec\n",
      "Epoch [544 / 2000]: Train Loss=5.7415, Val Cor=0.6648, Time=0.1613 sec\n",
      "Epoch [545 / 2000]: Train Loss=6.1551, Val Cor=0.6705, Time=0.1612 sec\n",
      "Epoch [546 / 2000]: Train Loss=5.6259, Val Cor=0.6717, Time=0.1616 sec\n",
      "Epoch [547 / 2000]: Train Loss=5.6482, Val Cor=0.6690, Time=0.1613 sec\n",
      "Epoch [548 / 2000]: Train Loss=5.6576, Val Cor=0.6606, Time=0.1608 sec\n",
      "Epoch [549 / 2000]: Train Loss=5.6299, Val Cor=0.6693, Time=0.1597 sec\n",
      "Epoch [550 / 2000]: Train Loss=6.3995, Val Cor=-0.5978, Time=0.1596 sec\n",
      "Epoch [551 / 2000]: Train Loss=6.2411, Val Cor=0.6761, Time=0.1599 sec\n",
      "Epoch [552 / 2000]: Train Loss=5.7369, Val Cor=0.6109, Time=0.1598 sec\n",
      "Epoch [553 / 2000]: Train Loss=5.7182, Val Cor=-0.5727, Time=0.1600 sec\n",
      "Epoch [554 / 2000]: Train Loss=5.5755, Val Cor=-0.5664, Time=0.1603 sec\n",
      "Epoch [555 / 2000]: Train Loss=5.5944, Val Cor=0.6667, Time=0.1614 sec\n",
      "Epoch [556 / 2000]: Train Loss=5.6682, Val Cor=0.6675, Time=0.1613 sec\n",
      "Epoch [557 / 2000]: Train Loss=5.4257, Val Cor=0.6595, Time=0.1617 sec\n",
      "Epoch [558 / 2000]: Train Loss=5.7365, Val Cor=-0.2325, Time=0.1614 sec\n",
      "Epoch [559 / 2000]: Train Loss=5.3770, Val Cor=0.6772, Time=0.1618 sec\n",
      "Epoch [560 / 2000]: Train Loss=5.4220, Val Cor=0.6739, Time=0.1616 sec\n",
      "Epoch [561 / 2000]: Train Loss=5.4716, Val Cor=0.6747, Time=0.1618 sec\n",
      "Epoch [562 / 2000]: Train Loss=5.6309, Val Cor=0.6688, Time=0.1604 sec\n",
      "Epoch [563 / 2000]: Train Loss=5.3429, Val Cor=0.6703, Time=0.1601 sec\n",
      "Epoch [564 / 2000]: Train Loss=5.1567, Val Cor=0.6689, Time=0.1606 sec\n",
      "Epoch [565 / 2000]: Train Loss=5.8895, Val Cor=-0.6298, Time=0.1618 sec\n",
      "Epoch [566 / 2000]: Train Loss=5.5706, Val Cor=0.4419, Time=0.1613 sec\n",
      "Epoch [567 / 2000]: Train Loss=5.5227, Val Cor=-0.5486, Time=0.1612 sec\n",
      "Epoch [568 / 2000]: Train Loss=5.5921, Val Cor=-0.6182, Time=0.1615 sec\n",
      "Epoch [569 / 2000]: Train Loss=5.8685, Val Cor=0.6706, Time=0.1615 sec\n",
      "Epoch [570 / 2000]: Train Loss=5.4325, Val Cor=0.6722, Time=0.1602 sec\n",
      "Epoch [571 / 2000]: Train Loss=5.6592, Val Cor=-0.6386, Time=0.1594 sec\n",
      "Epoch [572 / 2000]: Train Loss=5.7559, Val Cor=0.6682, Time=0.1598 sec\n",
      "Epoch [573 / 2000]: Train Loss=5.7190, Val Cor=0.6738, Time=0.1599 sec\n",
      "Epoch [574 / 2000]: Train Loss=5.6149, Val Cor=0.6372, Time=0.1600 sec\n",
      "Epoch [575 / 2000]: Train Loss=5.6385, Val Cor=0.6779, Time=0.1608 sec\n",
      "Epoch [576 / 2000]: Train Loss=5.4667, Val Cor=-0.5938, Time=0.1609 sec\n",
      "Epoch [577 / 2000]: Train Loss=5.4258, Val Cor=0.6755, Time=0.1610 sec\n",
      "Epoch [578 / 2000]: Train Loss=6.1240, Val Cor=0.6699, Time=0.1615 sec\n",
      "Epoch [579 / 2000]: Train Loss=5.7334, Val Cor=0.6726, Time=0.1611 sec\n",
      "Epoch [580 / 2000]: Train Loss=5.5421, Val Cor=0.6751, Time=0.1604 sec\n",
      "Epoch [581 / 2000]: Train Loss=5.4246, Val Cor=0.6783, Time=0.1600 sec\n",
      "Epoch [582 / 2000]: Train Loss=5.3678, Val Cor=0.6696, Time=0.1599 sec\n",
      "Epoch [583 / 2000]: Train Loss=5.7450, Val Cor=-0.6758, Time=0.1599 sec\n",
      "Epoch [584 / 2000]: Train Loss=5.9014, Val Cor=-0.6668, Time=0.1600 sec\n",
      "Epoch [585 / 2000]: Train Loss=5.7336, Val Cor=-0.6391, Time=0.1605 sec\n",
      "Epoch [586 / 2000]: Train Loss=5.7039, Val Cor=-0.5776, Time=0.1610 sec\n",
      "Epoch [587 / 2000]: Train Loss=5.5638, Val Cor=-0.5915, Time=0.1611 sec\n",
      "Epoch [588 / 2000]: Train Loss=6.3073, Val Cor=0.6472, Time=0.1614 sec\n",
      "Epoch [589 / 2000]: Train Loss=5.7446, Val Cor=-0.6160, Time=0.1614 sec\n",
      "Epoch [590 / 2000]: Train Loss=5.4389, Val Cor=0.6791, Time=0.1603 sec\n",
      "Epoch [591 / 2000]: Train Loss=5.4330, Val Cor=0.6823, Time=0.1599 sec\n",
      "Epoch [592 / 2000]: Train Loss=5.6643, Val Cor=0.6761, Time=0.1598 sec\n",
      "Epoch [593 / 2000]: Train Loss=5.5685, Val Cor=-0.5648, Time=0.1598 sec\n",
      "Epoch [594 / 2000]: Train Loss=5.6919, Val Cor=0.6751, Time=0.1598 sec\n",
      "Epoch [595 / 2000]: Train Loss=5.4775, Val Cor=0.6739, Time=0.1609 sec\n",
      "Epoch [596 / 2000]: Train Loss=5.6535, Val Cor=-0.3967, Time=0.1613 sec\n",
      "Epoch [597 / 2000]: Train Loss=6.0204, Val Cor=0.6659, Time=0.1618 sec\n",
      "Epoch [598 / 2000]: Train Loss=5.7493, Val Cor=0.6834, Time=0.1610 sec\n",
      "Epoch [599 / 2000]: Train Loss=5.5206, Val Cor=0.6651, Time=0.1600 sec\n",
      "Epoch [600 / 2000]: Train Loss=5.5431, Val Cor=0.6743, Time=0.1596 sec\n",
      "Epoch [601 / 2000]: Train Loss=5.5716, Val Cor=0.6799, Time=0.1602 sec\n",
      "Epoch [602 / 2000]: Train Loss=5.4520, Val Cor=0.6384, Time=0.1600 sec\n",
      "Epoch [603 / 2000]: Train Loss=5.3356, Val Cor=0.6743, Time=0.1598 sec\n",
      "Epoch [604 / 2000]: Train Loss=5.3602, Val Cor=0.6013, Time=0.1613 sec\n",
      "Epoch [605 / 2000]: Train Loss=5.4407, Val Cor=0.6714, Time=0.1615 sec\n",
      "Epoch [606 / 2000]: Train Loss=5.3733, Val Cor=0.6677, Time=0.1612 sec\n",
      "Epoch [607 / 2000]: Train Loss=5.4182, Val Cor=0.6704, Time=0.1612 sec\n",
      "Epoch [608 / 2000]: Train Loss=5.2840, Val Cor=0.6708, Time=0.1618 sec\n",
      "Epoch [609 / 2000]: Train Loss=5.3056, Val Cor=0.6694, Time=0.1618 sec\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "RNN_ensemble = []\n",
    "for i in range(5):\n",
    "    print(f\"Training RNN model {i}:\")\n",
    "    print(\"-\"*127)\n",
    "    RNN = train_RNN_ensemble(1280, 128, 2000, 32, 5e-4, 200, train_sets[i], val_sets[i], i)\n",
    "    RNN_ensemble.append(RNN)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    torch.save(RNN_ensemble[i].state_dict(), f'Models/Revised_RNN_ensemble_5_model_{i}_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>DMS_score</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S8P</td>\n",
       "      <td>0.805727</td>\n",
       "      <td>MVNEARGNPSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S20E</td>\n",
       "      <td>0.777952</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASEGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S20P</td>\n",
       "      <td>0.473713</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASPGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D27G</td>\n",
       "      <td>0.428925</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKGSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S28W</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDWSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mutant  DMS_score                                           sequence\n",
       "0    S8P   0.805727  MVNEARGNPSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1   S20E   0.777952  MVNEARGNSSLNPCLEGSASEGSESSKDSSRCSTPGLDPERHERLR...\n",
       "2   S20P   0.473713  MVNEARGNSSLNPCLEGSASPGSESSKDSSRCSTPGLDPERHERLR...\n",
       "3   D27G   0.428925  MVNEARGNSSLNPCLEGSASSGSESSKGSSRCSTPGLDPERHERLR...\n",
       "4   S28W   0.862100  MVNEARGNSSLNPCLEGSASSGSESSKDWSRCSTPGLDPERHERLR..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = pd.read_csv(\"query1.csv\")\n",
    "query1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S8P', 'S20E', 'S20P', 'D27G', 'S28W', 'S29V', 'S29T', 'R30L', 'R40K', 'R40C', 'R49A', 'R51G', 'D56S', 'K57M', 'S60N', 'S60P', 'R67G', 'S77V', 'S77G', 'D103K', 'K104Q', 'S107P', 'C129S', 'C130Q', 'R131M', 'R131V', 'Q132T', 'H142Q', 'K143V', 'K145P', 'Q146S', 'K150P', 'H180E', 'S183K', 'S183Y', 'D190W', 'C192M', 'H200V', 'H200M', 'S205T', 'H212A', 'H212R', 'K214H', 'S218G', 'S218Y', 'Q259P', 'H262F', 'S263Q', 'R294I', 'Q306V', 'R324G', 'R324I', 'M326S', 'M337L', 'S351R', 'H353K', 'R356G', 'R357M', 'Q378T', 'Q378V', 'S391G', 'S393Y', 'S393Q', 'K400Q', 'K410A', 'S411R', 'K413P', 'K418M', 'M419T', 'M419E', 'S426R', 'C451F', 'K467Q', 'R475L', 'Q476R', 'S483M', 'S483R', 'Q484W', 'S502V', 'Q508F', 'K509S', 'Q526M', 'Q570G', 'Q570A', 'D575W', 'R593Y', 'R593D', 'R605C', 'R605I', 'R605L', 'H612L', 'H612T', 'D628H', 'C630V', 'Q633Y', 'Q633D', 'R644S', 'Q647A', 'R650I', 'R650C']\n"
     ]
    }
   ],
   "source": [
    "query1_mutants = list(query1[\"mutant\"])\n",
    "print(query1_mutants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1D', 'V1Y', 'V1C', 'V1A', 'V1E', 'V1W', 'V1T', 'V1R', 'V1Q', 'V1S', 'V1N', 'V1M', 'V1L', 'V1K', 'V1I', 'V1H', 'V1G', 'V1F', 'V1P', 'N2R', 'N2Y', 'N2W', 'N2V', 'N2T', 'N2S', 'N2Q', 'N2I', 'N2M', 'N2A', 'N2P', 'N2D', 'N2E', 'N2F', 'N2C', 'N2H', 'N2K', 'N2L', 'N2G', 'E3K', 'E3C', 'E3D', 'E3F', 'E3G', 'E3H', 'E3I', 'E3L', 'E3A', 'E3N', 'E3M', 'E3W', 'E3V', 'E3T', 'E3Y', 'E3R', 'E3Q', 'E3P', 'E3S', 'A4M', 'A4F', 'A4G', 'A4I', 'A4K', 'A4L', 'A4N', 'A4S', 'A4Q', 'A4R', 'A4E', 'A4T', 'A4V', 'A4W', 'A4Y', 'A4P', 'A4D', 'A4H', 'A4C', 'R5W', 'R5A', 'R5C', 'R5D', 'R5E', 'R5F', 'R5G', 'R5H', 'R5I', 'R5Y', 'R5L', 'R5M', 'R5N', 'R5P', 'R5Q', 'R5S', 'R5T', 'R5V', 'R5K', 'G6V', 'G6T', 'G6S', 'G6R', 'G6Q', 'G6P', 'G6N', 'G6H', 'G6L', 'G6K', 'G6I', 'G6F', 'G6E', 'G6D', 'G6C', 'G6M', 'G6A', 'G6Y', 'G6W', 'N7A', 'N7W', 'N7V', 'N7T', 'N7S', 'N7R', 'N7Q', 'N7P', 'N7M', 'N7Y', 'N7K', 'N7C', 'N7D', 'N7E', 'N7L', 'N7G', 'N7H', 'N7I', 'N7F', 'S8D', 'S8E', 'S8F', 'S8G', 'S8H', 'S8I', 'S8K', 'S8L', 'S8N', 'S8P', 'S8C', 'S8R', 'S8T', 'S8V', 'S8W', 'S8Y', 'S8M', 'S8Q', 'S8A', 'S9C', 'S9D', 'S9E', 'S9F', 'S9G', 'S9H', 'S9I', 'S9K', 'S9L', 'S9M', 'S9N', 'S9P', 'S9Q', 'S9R', 'S9T', 'S9V', 'S9A', 'S9W', 'S9Y', 'L10V', 'L10A', 'L10C', 'L10D', 'L10E', 'L10F', 'L10G', 'L10H', 'L10K', 'L10M', 'L10N', 'L10P', 'L10Q', 'L10R', 'L10S', 'L10T', 'L10W', 'L10Y', 'L10I', 'N11Y', 'N11C', 'N11D', 'N11E', 'N11F', 'N11G', 'N11H', 'N11A', 'N11I', 'N11L', 'N11M', 'N11P', 'N11Q', 'N11R', 'N11S', 'N11K', 'N11W', 'N11V', 'N11T', 'P12W', 'P12D', 'P12E', 'P12F', 'P12G', 'P12H', 'P12I', 'P12K', 'P12L', 'P12N', 'P12Q', 'P12R', 'P12S', 'P12T', 'P12V', 'P12M', 'P12Y', 'P12A', 'P12C', 'C13A', 'C13D', 'C13E', 'C13F', 'C13G', 'C13H', 'C13K', 'C13L', 'C13M', 'C13I', 'C13P', 'C13N', 'C13W', 'C13V', 'C13T', 'C13Y', 'C13R', 'C13Q', 'C13S', 'L14K', 'L14D', 'L14E', 'L14F', 'L14G', 'L14H', 'L14I', 'L14M', 'L14C', 'L14P', 'L14Q', 'L14R', 'L14S', 'L14T', 'L14V', 'L14W', 'L14Y', 'L14N', 'L14A', 'E15C', 'E15T', 'E15A', 'E15Y', 'E15W', 'E15V', 'E15D', 'E15F', 'E15G', 'E15H', 'E15S', 'E15K', 'E15L', 'E15M', 'E15N', 'E15P', 'E15Q', 'E15R', 'E15I', 'G16D', 'G16E', 'G16F', 'G16H', 'G16I', 'G16K', 'G16L', 'G16R', 'G16P', 'G16Q', 'G16S', 'G16T', 'G16V', 'G16C', 'G16Y', 'G16N', 'G16W', 'G16M', 'G16A', 'S17A', 'S17Y', 'S17D', 'S17V', 'S17T', 'S17R', 'S17Q', 'S17P', 'S17N', 'S17W', 'S17M', 'S17E', 'S17F', 'S17G', 'S17C', 'S17I', 'S17K', 'S17L', 'S17H', 'A18D', 'A18E', 'A18F', 'A18G', 'A18H', 'A18I', 'A18K', 'A18M', 'A18N', 'A18V', 'A18Q', 'A18R', 'A18S', 'A18T', 'A18L', 'A18P', 'A18C', 'A18Y', 'A18W', 'S19R', 'S19Q', 'S19P', 'S19N', 'S19M', 'S19L', 'S19W', 'S19I', 'S19G', 'S19F', 'S19E', 'S19D', 'S19C', 'S19A', 'S19Y', 'S19H', 'S19K', 'S19V', 'S19T', 'S20R', 'S20A', 'S20C', 'S20D', 'S20E', 'S20F', 'S20G', 'S20H', 'S20I', 'S20Y', 'S20K', 'S20L', 'S20M', 'S20N', 'S20P', 'S20Q', 'S20T', 'S20V', 'S20W', 'G21A', 'G21W', 'G21N', 'G21Y', 'G21V', 'G21T', 'G21S', 'G21R', 'G21P', 'G21M', 'G21L', 'G21Q', 'G21I', 'G21H', 'G21F', 'G21E', 'G21D', 'G21C', 'G21K', 'S22I', 'S22C', 'S22A', 'S22D', 'S22E', 'S22F', 'S22G', 'S22H', 'S22W', 'S22K', 'S22M', 'S22L', 'S22V', 'S22T', 'S22Y', 'S22Q', 'S22P', 'S22N', 'S22R', 'E23A', 'E23I', 'E23C', 'E23D', 'E23F', 'E23G', 'E23H', 'E23K', 'E23Q', 'E23M', 'E23N', 'E23P', 'E23R', 'E23S', 'E23T', 'E23V', 'E23W', 'E23L', 'E23Y', 'S24Y', 'S24H', 'S24D', 'S24E', 'S24F', 'S24G', 'S24I', 'S24K', 'S24A', 'S24L', 'S24N', 'S24P', 'S24Q', 'S24R', 'S24T', 'S24V', 'S24M', 'S24W', 'S24C', 'S25E', 'S25A', 'S25C', 'S25Y', 'S25W', 'S25V', 'S25T', 'S25F', 'S25Q', 'S25P', 'S25R', 'S25M', 'S25L', 'S25K', 'S25I', 'S25H', 'S25D', 'S25G', 'S25N', 'K26G', 'K26M', 'K26F', 'K26E', 'K26H', 'K26I', 'K26L', 'K26N', 'K26C', 'K26Q', 'K26R', 'K26S', 'K26T', 'K26V', 'K26D', 'K26Y', 'K26W', 'K26P', 'K26A', 'D27A', 'D27Y', 'D27C', 'D27E', 'D27F', 'D27G', 'D27I', 'D27K', 'D27L', 'D27M', 'D27H', 'D27P', 'D27Q', 'D27R', 'D27S', 'D27T', 'D27V', 'D27W', 'D27N', 'S28H', 'S28G', 'S28F', 'S28A', 'S28D', 'S28C', 'S28I', 'S28E', 'S28K', 'S28L', 'S28M', 'S28W', 'S28V', 'S28T', 'S28Y', 'S28Q', 'S28P', 'S28N', 'S28R', 'S29M', 'S29V', 'S29T', 'S29R', 'S29Q', 'S29P', 'S29N', 'S29L', 'S29F', 'S29I', 'S29H', 'S29G', 'S29W', 'S29E', 'S29D', 'S29C', 'S29A', 'S29K', 'S29Y', 'R30D', 'R30W', 'R30Y', 'R30C', 'R30E', 'R30F', 'R30G', 'R30I', 'R30K', 'R30L', 'R30H', 'R30N', 'R30A', 'R30P', 'R30Q', 'R30S', 'R30T', 'R30V', 'R30M', 'C31P', 'C31W', 'C31V', 'C31T', 'C31S', 'C31R', 'C31Q', 'C31N', 'C31E', 'C31L', 'C31K', 'C31I', 'C31H', 'C31G', 'C31F', 'C31D', 'C31A', 'C31M', 'C31Y', 'S32W', 'S32Y', 'S32I', 'S32R', 'S32Q', 'S32P', 'S32N', 'S32M', 'S32L', 'S32K', 'S32T', 'S32H', 'S32F', 'S32E', 'S32D', 'S32C', 'S32A', 'S32G', 'S32V', 'T33W', 'T33A', 'T33Y', 'T33C', 'T33R', 'T33Q', 'T33P', 'T33M', 'T33L', 'T33K', 'T33N', 'T33I', 'T33H', 'T33G', 'T33F', 'T33E', 'T33D', 'T33V', 'T33S', 'P34K', 'P34D', 'P34E', 'P34F', 'P34G', 'P34H', 'P34I', 'P34T', 'P34M', 'P34N', 'P34Q', 'P34R', 'P34S', 'P34A', 'P34W', 'P34V', 'P34C', 'P34L', 'P34Y', 'G35K', 'G35C', 'G35Y', 'G35A', 'G35W', 'G35V', 'G35T', 'G35S', 'G35Q', 'G35P', 'G35R', 'G35M', 'G35L', 'G35I', 'G35H', 'G35F', 'G35E', 'G35D', 'G35N', 'L36K', 'L36S', 'L36R', 'L36Q', 'L36P', 'L36N', 'L36M', 'L36I', 'L36W', 'L36G', 'L36F', 'L36E', 'L36D', 'L36C', 'L36A', 'L36Y', 'L36T', 'L36H', 'L36V', 'D37M', 'D37C', 'D37A', 'D37Y', 'D37W', 'D37V', 'D37T', 'D37R', 'D37Q', 'D37P', 'D37S', 'D37L', 'D37K', 'D37I', 'D37H', 'D37G', 'D37F', 'D37E', 'D37N', 'P38N', 'P38W', 'P38V', 'P38T', 'P38S', 'P38R', 'P38Q', 'P38M', 'P38C', 'P38K', 'P38I', 'P38H', 'P38G', 'P38F', 'P38E', 'P38D', 'P38L', 'P38A', 'P38Y', 'E39G', 'E39Q', 'E39V', 'E39T', 'E39A', 'E39C', 'E39D', 'E39F', 'E39S', 'E39H', 'E39I', 'E39K', 'E39L', 'E39M', 'E39N', 'E39P', 'E39W', 'E39Y', 'E39R', 'R40Y', 'R40M', 'R40L', 'R40K', 'R40I', 'R40H', 'R40G', 'R40Q', 'R40N', 'R40S', 'R40E', 'R40D', 'R40C', 'R40W', 'R40A', 'R40T', 'R40V', 'R40F', 'R40P', 'H41W', 'H41V', 'H41R', 'H41S', 'H41Y', 'H41Q', 'H41P', 'H41N', 'H41M', 'H41L', 'H41K', 'H41I', 'H41F', 'H41E', 'H41D', 'H41C', 'H41A', 'H41T', 'H41G', 'E42H', 'E42Y', 'E42A', 'E42C', 'E42D', 'E42F', 'E42I', 'E42K', 'E42L', 'E42M', 'E42G', 'E42P', 'E42Q', 'E42R', 'E42S', 'E42T', 'E42V', 'E42W', 'E42N', 'R43M', 'R43N', 'R43P', 'R43V', 'R43S', 'R43T', 'R43L', 'R43Q', 'R43K', 'R43A', 'R43H', 'R43G', 'R43F', 'R43E', 'R43D', 'R43C', 'R43W', 'R43I', 'R43Y', 'L44A', 'L44E', 'L44V', 'L44G', 'L44H', 'L44I', 'L44K', 'L44M', 'L44N', 'L44P', 'L44D', 'L44Q', 'L44S', 'L44T', 'L44W', 'L44Y', 'L44C', 'L44R', 'L44F', 'R45Y', 'R45W', 'R45V', 'R45A', 'R45Q', 'R45S', 'R45P', 'R45N', 'R45M', 'R45L', 'R45K', 'R45T', 'R45H', 'R45G', 'R45F', 'R45E', 'R45D', 'R45C', 'R45I', 'E46R', 'E46A', 'E46Y', 'E46W', 'E46V', 'E46T', 'E46S', 'E46Q', 'E46M', 'E46N', 'E46P', 'E46F', 'E46G', 'E46H', 'E46D', 'E46K', 'E46L', 'E46C', 'E46I', 'K47W', 'K47M', 'K47Y', 'K47S', 'K47R', 'K47Q', 'K47P', 'K47N', 'K47L', 'K47V', 'K47H', 'K47G', 'K47F', 'K47E', 'K47D', 'K47C', 'K47A', 'K47T', 'K47I', 'R49Q', 'R49S', 'R49Y', 'R49V', 'R49W', 'R49P', 'R49T', 'R49N', 'R49I', 'R49L', 'R49K', 'R49G', 'R49F', 'R49E', 'R49D', 'R49C', 'R49A', 'R49M', 'R49H', 'R50A', 'R50Y', 'R50C', 'R50D', 'R50E', 'R50G', 'R50H', 'R50I', 'R50K', 'R50F', 'R50M', 'R50V', 'R50L', 'R50T', 'R50S', 'R50W', 'R50Q', 'R50P', 'R50N', 'R51A', 'R51C', 'R51E', 'R51F', 'R51G', 'R51H', 'R51I', 'R51K', 'R51M', 'R51D', 'R51L', 'R51Q', 'R51P', 'R51Y', 'R51W', 'R51N', 'R51T', 'R51S', 'R51V', 'L52A', 'L52C', 'L52D', 'L52E', 'L52F', 'L52G', 'L52H', 'L52K', 'L52M', 'L52P', 'L52R', 'L52S', 'L52T', 'L52V', 'L52W', 'L52Y', 'L52N', 'L52Q', 'L52I', 'E53R', 'E53Y', 'E53W', 'E53T', 'E53S', 'E53A', 'E53C', 'E53D', 'E53F', 'E53V', 'E53Q', 'E53H', 'E53I', 'E53K', 'E53G', 'E53L', 'E53M', 'E53P', 'E53N', 'S54L', 'S54M', 'S54P', 'S54K', 'S54Q', 'S54R', 'S54N', 'S54I', 'S54D', 'S54G', 'S54F', 'S54E', 'S54W', 'S54Y', 'S54T', 'S54A', 'S54C', 'S54H', 'S54V', 'G55C', 'G55A', 'G55D', 'G55E', 'G55F', 'G55H', 'G55I', 'G55K', 'G55L', 'G55M', 'G55N', 'G55P', 'G55Q', 'G55R', 'G55S', 'G55T', 'G55V', 'G55W', 'G55Y', 'D56V', 'D56S', 'D56R', 'D56Q', 'D56P', 'D56N', 'D56M', 'D56L', 'D56K', 'D56H', 'D56G', 'D56F', 'D56E', 'D56C', 'D56A', 'D56I', 'D56T', 'D56Y', 'D56W', 'K57Y', 'K57W', 'K57V', 'K57T', 'K57S', 'K57Q', 'K57P', 'K57N', 'K57M', 'K57R', 'K57I', 'K57L', 'K57C', 'K57D', 'K57E', 'K57A', 'K57G', 'K57H', 'K57F', 'W58P', 'W58A', 'W58Y', 'W58V', 'W58T', 'W58S', 'W58R', 'W58Q', 'W58N', 'W58C', 'W58L', 'W58K', 'W58I', 'W58H', 'W58G', 'W58F', 'W58E', 'W58D', 'W58M', 'F59Y', 'F59T', 'F59V', 'F59A', 'F59C', 'F59D', 'F59E', 'F59G', 'F59H', 'F59I', 'F59K', 'F59L', 'F59M', 'F59N', 'F59P', 'F59Q', 'F59R', 'F59W', 'F59S', 'S60V', 'S60Y', 'S60C', 'S60E', 'S60F', 'S60G', 'S60H', 'S60I', 'S60K', 'S60L', 'S60M', 'S60N', 'S60P', 'S60Q', 'S60R', 'S60T', 'S60W', 'S60A', 'S60D', 'L61F', 'L61C', 'L61E', 'L61G', 'L61H', 'L61I', 'L61A', 'L61M', 'L61N', 'L61K', 'L61Q', 'L61D', 'L61R', 'L61S', 'L61T', 'L61V', 'L61W', 'L61Y', 'L61P', 'E62N', 'E62H', 'E62I', 'E62K', 'E62L', 'E62M', 'E62C', 'E62D', 'E62R', 'E62S', 'E62T', 'E62V', 'E62W', 'E62G', 'E62Q', 'E62A', 'E62P', 'E62F', 'E62Y', 'F63S', 'F63K', 'F63I', 'F63H', 'F63G', 'F63E', 'F63D', 'F63C', 'F63M', 'F63N', 'F63P', 'F63Q', 'F63R', 'F63T', 'F63V', 'F63W', 'F63Y', 'F63A', 'F63L', 'F64I', 'F64A', 'F64D', 'F64H', 'F64E', 'F64V', 'F64T', 'F64S', 'F64R', 'F64Q', 'F64P', 'F64C', 'F64N', 'F64L', 'F64G', 'F64K', 'F64Y', 'F64M', 'F64W', 'P65E', 'P65D', 'P65F', 'P65H', 'P65I', 'P65K', 'P65L', 'P65M', 'P65N', 'P65Q', 'P65R', 'P65S', 'P65T', 'P65W', 'P65Y', 'P65G', 'P65V', 'P65C', 'P65A', 'P66C', 'P66Y', 'P66W', 'P66V', 'P66T', 'P66R', 'P66Q', 'P66N', 'P66S', 'P66L', 'P66D', 'P66M', 'P66E', 'P66F', 'P66A', 'P66H', 'P66I', 'P66K', 'P66G', 'R67K', 'R67A', 'R67E', 'R67F', 'R67G', 'R67H', 'R67I', 'R67D', 'R67L', 'R67N', 'R67M', 'R67Y', 'R67W', 'R67V', 'R67C', 'R67S', 'R67Q', 'R67P', 'R67T', 'T68I', 'T68C', 'T68D', 'T68E', 'T68F', 'T68G', 'T68H', 'T68K', 'T68Q', 'T68M', 'T68N', 'T68P', 'T68R', 'T68S', 'T68V', 'T68W', 'T68Y', 'T68L', 'T68A', 'A69E', 'A69D', 'A69Y', 'A69W', 'A69V', 'A69S', 'A69R', 'A69Q', 'A69P', 'A69N', 'A69M', 'A69L', 'A69K', 'A69I', 'A69H', 'A69G', 'A69F', 'A69T', 'A69C', 'E70S', 'E70R', 'E70W', 'E70A', 'E70Y', 'E70D', 'E70F', 'E70G', 'E70H', 'E70C', 'E70K', 'E70L', 'E70M', 'E70N', 'E70P', 'E70Q', 'E70V', 'E70I', 'E70T', 'G71Q', 'G71R', 'G71S', 'G71Y', 'G71V', 'G71W', 'G71P', 'G71T', 'G71N', 'G71F', 'G71L', 'G71K', 'G71I', 'G71H', 'G71E', 'G71D', 'G71C', 'G71A', 'G71M', 'A72Q', 'A72R', 'A72S', 'A72P', 'A72V', 'A72W', 'A72Y', 'A72T', 'A72N', 'A72C', 'A72L', 'A72K', 'A72I', 'A72H', 'A72G', 'A72F', 'A72E', 'A72D', 'A72M', 'V73Q', 'V73P', 'V73N', 'V73M', 'V73L', 'V73K', 'V73F', 'V73H', 'V73G', 'V73R', 'V73E', 'V73D', 'V73C', 'V73A', 'V73I', 'V73S', 'V73Y', 'V73W', 'V73T', 'N74T', 'N74S', 'N74Y', 'N74R', 'N74Q', 'N74P', 'N74M', 'N74L', 'N74I', 'N74H', 'N74A', 'N74G', 'N74F', 'N74E', 'N74D', 'N74K', 'N74C', 'N74W', 'N74V', 'L75D', 'L75E', 'L75F', 'L75G', 'L75H', 'L75I', 'L75K', 'L75M', 'L75N', 'L75P', 'L75Q', 'L75R', 'L75S', 'L75T', 'L75V', 'L75A', 'L75W', 'L75C', 'L75Y', 'I76C', 'I76Y', 'I76W', 'I76V', 'I76T', 'I76D', 'I76R', 'I76Q', 'I76P', 'I76S', 'I76M', 'I76N', 'I76F', 'I76A', 'I76E', 'I76H', 'I76K', 'I76L', 'I76G', 'S77Y', 'S77W', 'S77V', 'S77T', 'S77R', 'S77Q', 'S77P', 'S77N', 'S77M', 'S77K', 'S77I', 'S77H', 'S77G', 'S77F', 'S77E', 'S77D', 'S77C', 'S77A', 'S77L', 'R78A', 'R78C', 'R78D', 'R78E', 'R78F', 'R78G', 'R78Y', 'R78I', 'R78V', 'R78H', 'R78T', 'R78S', 'R78W', 'R78K', 'R78P', 'R78N', 'R78M', 'R78L', 'R78Q', 'F79D', 'F79E', 'F79G', 'F79H', 'F79I', 'F79K', 'F79L', 'F79N', 'F79P', 'F79Q', 'F79S', 'F79T', 'F79V', 'F79W', 'F79M', 'F79R', 'F79C', 'F79A', 'F79Y', 'D80Y', 'D80C', 'D80E', 'D80F', 'D80G', 'D80H', 'D80I', 'D80K', 'D80L', 'D80N', 'D80P', 'D80Q', 'D80R', 'D80S', 'D80T', 'D80V', 'D80W', 'D80M', 'D80A', 'R81I', 'R81W', 'R81A', 'R81C', 'R81D', 'R81E', 'R81F', 'R81G', 'R81H', 'R81K', 'R81Y', 'R81M', 'R81N', 'R81P', 'R81Q', 'R81S', 'R81T', 'R81V', 'R81L', 'M82A', 'M82C', 'M82D', 'M82E', 'M82G', 'M82H', 'M82I', 'M82K', 'M82F', 'M82N', 'M82V', 'M82T', 'M82S', 'M82W', 'M82R', 'M82Q', 'M82P', 'M82Y', 'M82L', 'A83Y', 'A83Q', 'A83W', 'A83V', 'A83T', 'A83S', 'A83R', 'A83P', 'A83M', 'A83N', 'A83C', 'A83D', 'A83F', 'A83E', 'A83H', 'A83I', 'A83K', 'A83L', 'A83G', 'A84C', 'A84D', 'A84E', 'A84F', 'A84G', 'A84H', 'A84I', 'A84K', 'A84L', 'A84N', 'A84Y', 'A84W', 'A84V', 'A84T', 'A84M', 'A84R', 'A84Q', 'A84P', 'A84S', 'G85N', 'G85Y', 'G85W', 'G85V', 'G85T', 'G85S', 'G85R', 'G85P', 'G85M', 'G85Q', 'G85K', 'G85I', 'G85H', 'G85F', 'G85E', 'G85D', 'G85C', 'G85L', 'G85A', 'G86E', 'G86F', 'G86H', 'G86I', 'G86K', 'G86L', 'G86N', 'G86S', 'G86Q', 'G86R', 'G86T', 'G86V', 'G86W', 'G86Y', 'G86D', 'G86P', 'G86C', 'G86M', 'G86A', 'I90Y', 'I90A', 'I90C', 'I90D', 'I90E', 'I90G', 'I90H', 'I90K', 'I90L', 'I90M', 'I90N', 'I90P', 'I90Q', 'I90R', 'I90S', 'I90T', 'I90V', 'I90W', 'I90F', 'D91L', 'D91T', 'D91V', 'D91S', 'D91R', 'D91Q', 'D91P', 'D91N', 'D91M', 'D91K', 'D91I', 'D91H', 'D91G', 'D91F', 'D91E', 'D91C', 'D91A', 'D91W', 'D91Y', 'V92W', 'V92Y', 'V92S', 'V92R', 'V92Q', 'V92P', 'V92T', 'V92M', 'V92L', 'V92K', 'V92N', 'V92H', 'V92G', 'V92F', 'V92E', 'V92D', 'V92C', 'V92A', 'V92I', 'T93N', 'T93P', 'T93Q', 'T93W', 'T93S', 'T93V', 'T93M', 'T93R', 'T93L', 'T93Y', 'T93I', 'T93A', 'T93C', 'T93D', 'T93K', 'T93F', 'T93G', 'T93H', 'T93E', 'W94C', 'W94D', 'W94E', 'W94F', 'W94G', 'W94H', 'W94I', 'W94L', 'W94M', 'W94N', 'W94A', 'W94Q', 'W94R', 'W94S', 'W94T', 'W94K', 'W94P', 'W94Y', 'W94V', 'H95I', 'H95W', 'H95R', 'H95Q', 'H95P', 'H95N', 'H95M', 'H95L', 'H95K', 'H95S', 'H95Y', 'H95G', 'H95F', 'H95E', 'H95V', 'H95D', 'H95C', 'H95A', 'H95T', 'P96Y', 'P96A', 'P96D', 'P96E', 'P96F', 'P96G', 'P96H', 'P96I', 'P96K', 'P96M', 'P96N', 'P96Q', 'P96S', 'P96T', 'P96V', 'P96W', 'P96C', 'P96R', 'P96L', 'A97N', 'A97V', 'A97T', 'A97S', 'A97R', 'A97Q', 'A97P', 'A97M', 'A97L', 'A97K', 'A97I', 'A97H', 'A97G', 'A97F', 'A97E', 'A97D', 'A97W', 'A97Y', 'A97C', 'G98C', 'G98F', 'G98H', 'G98I', 'G98K', 'G98L', 'G98M', 'G98E', 'G98N', 'G98Q', 'G98R', 'G98S', 'G98T', 'G98V', 'G98W', 'G98P', 'G98D', 'G98A', 'G98Y', 'D99E', 'D99F', 'D99G', 'D99H', 'D99I', 'D99K', 'D99L', 'D99W', 'D99A', 'D99N', 'D99P', 'D99Q', 'D99R', 'D99S', 'D99T', 'D99V', 'D99M', 'D99C', 'D99Y', 'P100K', 'P100D', 'P100E', 'P100F', 'P100G', 'P100H', 'P100I', 'P100Y', 'P100L', 'P100M', 'P100N', 'P100Q', 'P100R', 'P100S', 'P100T', 'P100V', 'P100C', 'P100A', 'P100W', 'G101A', 'G101C', 'G101D', 'G101E', 'G101F', 'G101H', 'G101K', 'G101L', 'G101M', 'G101I', 'G101P', 'G101N', 'G101W', 'G101V', 'G101T', 'G101Y', 'G101R', 'G101Q', 'G101S', 'S102M', 'S102E', 'S102F', 'S102G', 'S102H', 'S102I', 'S102K', 'S102L', 'S102A', 'S102C', 'S102P', 'S102Q', 'S102R', 'S102T', 'S102V', 'S102W', 'S102Y', 'S102D', 'S102N', 'D103C', 'D103A', 'D103W', 'D103E', 'D103F', 'D103G', 'D103H', 'D103I', 'D103K', 'D103L', 'D103Y', 'D103N', 'D103P', 'D103Q', 'D103R', 'D103S', 'D103T', 'D103V', 'D103M', 'K104W', 'K104V', 'K104T', 'K104S', 'K104R', 'K104Q', 'K104P', 'K104N', 'K104H', 'K104L', 'K104I', 'K104G', 'K104F', 'K104E', 'K104D', 'K104Y', 'K104A', 'K104M', 'K104C', 'E105N', 'E105P', 'E105F', 'E105G', 'E105H', 'E105I', 'E105K', 'E105L', 'E105D', 'E105M', 'E105R', 'E105S', 'E105T', 'E105V', 'E105W', 'E105Y', 'E105Q', 'E105C', 'E105A', 'T106Y', 'T106C', 'T106F', 'T106G', 'T106H', 'T106I', 'T106K', 'T106L', 'T106M', 'T106A', 'T106N', 'T106D', 'T106W', 'T106V', 'T106E', 'T106R', 'T106Q', 'T106P', 'T106S', 'S107T', 'S107R', 'S107Q', 'S107P', 'S107N', 'S107M', 'S107L', 'S107I', 'S107H', 'S107G', 'S107E', 'S107D', 'S107C', 'S107A', 'S107Y', 'S107K', 'S107F', 'S107W', 'S107V', 'S108D', 'S108E', 'S108F', 'S108G', 'S108H', 'S108I', 'S108K', 'S108C', 'S108A', 'S108N', 'S108P', 'S108Q', 'S108R', 'S108T', 'S108V', 'S108W', 'S108M', 'S108L', 'S108Y', 'M109C', 'M109Y', 'M109W', 'M109V', 'M109T', 'M109R', 'M109Q', 'M109P', 'M109N', 'M109S', 'M109K', 'M109D', 'M109L', 'M109E', 'M109F', 'M109A', 'M109H', 'M109I', 'M109G', 'M110L', 'M110W', 'M110T', 'M110S', 'M110R', 'M110Q', 'M110P', 'M110N', 'M110V', 'M110K', 'M110H', 'M110G', 'M110F', 'M110E', 'M110D', 'M110C', 'M110A', 'M110Y', 'M110I', 'I111P', 'I111Q', 'I111V', 'I111S', 'I111T', 'I111N', 'I111R', 'I111M', 'I111F', 'I111K', 'I111H', 'I111G', 'I111E', 'I111D', 'I111C', 'I111A', 'I111Y', 'I111L', 'I111W', 'S113L', 'S113M', 'S113D', 'S113E', 'S113F', 'S113G', 'S113H', 'S113I', 'S113C', 'S113K', 'S113A', 'S113N', 'S113P', 'S113Q', 'S113R', 'S113T', 'S113Y', 'S113V', 'S113W', 'T114D', 'T114V', 'T114S', 'T114R', 'T114Q', 'T114P', 'T114N', 'T114C', 'T114M', 'T114K', 'T114I', 'T114H', 'T114G', 'T114F', 'T114E', 'T114L', 'T114A', 'T114W', 'T114Y', 'A115D', 'A115W', 'A115V', 'A115T', 'A115S', 'A115R', 'A115Q', 'A115P', 'A115N', 'A115L', 'A115K', 'A115I', 'A115H', 'A115G', 'A115F', 'A115E', 'A115M', 'A115Y', 'A115C', 'V116C', 'V116A', 'V116Y', 'V116W', 'V116T', 'V116S', 'V116R', 'V116Q', 'V116P', 'V116D', 'V116M', 'V116L', 'V116K', 'V116I', 'V116H', 'V116G', 'V116F', 'V116E', 'V116N', 'N117P', 'N117Q', 'N117V', 'N117S', 'N117T', 'N117M', 'N117R', 'N117L', 'N117G', 'N117I', 'N117H', 'N117F', 'N117E', 'N117D', 'N117C', 'N117W', 'N117K', 'N117A', 'N117Y', 'Y118V', 'Y118W', 'Y118C', 'Y118D', 'Y118E', 'Y118F', 'Y118G', 'Y118H', 'Y118I', 'Y118A', 'Y118L', 'Y118K', 'Y118S', 'Y118R', 'Y118T', 'Y118P', 'Y118N', 'Y118M', 'Y118Q', 'C119Y', 'C119Q', 'C119W', 'C119V', 'C119T', 'C119S', 'C119R', 'C119P', 'C119I', 'C119L', 'C119K', 'C119H', 'C119G', 'C119F', 'C119E', 'C119A', 'C119N', 'C119D', 'C119M', 'G120C', 'G120A', 'G120D', 'G120E', 'G120F', 'G120H', 'G120K', 'G120L', 'G120M', 'G120I', 'G120P', 'G120Q', 'G120R', 'G120S', 'G120T', 'G120V', 'G120W', 'G120N', 'G120Y', 'L121P', 'L121Q', 'L121R', 'L121V', 'L121T', 'L121M', 'L121S', 'L121K', 'L121A', 'L121H', 'L121G', 'L121F', 'L121E', 'L121D', 'L121C', 'L121W', 'L121I', 'L121Y', 'L121N', 'E122C', 'E122Y', 'E122A', 'E122W', 'E122V', 'E122S', 'E122R', 'E122Q', 'E122P', 'E122T', 'E122N', 'E122M', 'E122L', 'E122K', 'E122I', 'E122H', 'E122G', 'E122F', 'E122D', 'T123N', 'T123P', 'T123Q', 'T123R', 'T123Y', 'T123V', 'T123W', 'T123M', 'T123S', 'T123L', 'T123K', 'T123I', 'T123H', 'T123G', 'T123F', 'T123E', 'T123D', 'T123C', 'T123A', 'I124S', 'I124R', 'I124Q', 'I124P', 'I124N', 'I124M', 'I124L', 'I124K', 'I124E', 'I124G', 'I124F', 'I124W', 'I124D', 'I124C', 'I124A', 'I124V', 'I124Y', 'I124H', 'I124T', 'L125Y', 'L125M', 'L125S', 'L125R', 'L125Q', 'L125P', 'L125N', 'L125K', 'L125I', 'L125H', 'L125G', 'L125F', 'L125E', 'L125D', 'L125C', 'L125W', 'L125T', 'L125V', 'L125A', 'M127F', 'M127D', 'M127A', 'M127K', 'M127L', 'M127N', 'M127P', 'M127Q', 'M127R', 'M127S', 'M127C', 'M127T', 'M127V', 'M127W', 'M127Y', 'M127E', 'M127G', 'M127I', 'M127H', 'T128W', 'T128D', 'T128E', 'T128F', 'T128G', 'T128H', 'T128I', 'T128K', 'T128L', 'T128C', 'T128M', 'T128P', 'T128Q', 'T128R', 'T128S', 'T128V', 'T128Y', 'T128N', 'T128A', 'C129W', 'C129R', 'C129Q', 'C129P', 'C129N', 'C129M', 'C129L', 'C129S', 'C129K', 'C129H', 'C129G', 'C129F', 'C129E', 'C129D', 'C129A', 'C129I', 'C129T', 'C129V', 'C129Y', 'C130V', 'C130A', 'C130D', 'C130E', 'C130F', 'C130G', 'C130H', 'C130T', 'C130I', 'C130L', 'C130M', 'C130N', 'C130P', 'C130Q', 'C130R', 'C130S', 'C130Y', 'C130K', 'C130W', 'R131Y', 'R131W', 'R131I', 'R131S', 'R131Q', 'R131P', 'R131N', 'R131M', 'R131L', 'R131K', 'R131T', 'R131V', 'R131G', 'R131F', 'R131E', 'R131D', 'R131C', 'R131A', 'R131H', 'Q132Y', 'Q132W', 'Q132T', 'Q132R', 'Q132S', 'Q132V', 'Q132A', 'Q132C', 'Q132E', 'Q132F', 'Q132G', 'Q132D', 'Q132I', 'Q132K', 'Q132L', 'Q132M', 'Q132N', 'Q132P', 'Q132H', 'R133N', 'R133Y', 'R133W', 'R133V', 'R133S', 'R133Q', 'R133P', 'R133M', 'R133T', 'R133K', 'R133A', 'R133C', 'R133D', 'R133E', 'R133L', 'R133F', 'R133G', 'R133H', 'R133I', 'L134E', 'L134F', 'L134G', 'L134H', 'L134I', 'L134K', 'L134M', 'L134N', 'L134Q', 'L134Y', 'L134R', 'L134D', 'L134T', 'L134V', 'L134W', 'L134C', 'L134A', 'L134P', 'L134S', 'E135W', 'E135V', 'E135C', 'E135D', 'E135Y', 'E135G', 'E135I', 'E135K', 'E135L', 'E135F', 'E135A', 'E135M', 'E135T', 'E135S', 'E135H', 'E135Q', 'E135P', 'E135N', 'E135R', 'E136L', 'E136C', 'E136D', 'E136F', 'E136G', 'E136H', 'E136I', 'E136K', 'E136A', 'E136M', 'E136P', 'E136Q', 'E136R', 'E136S', 'E136T', 'E136V', 'E136W', 'E136Y', 'E136N', 'I137G', 'I137F', 'I137A', 'I137D', 'I137C', 'I137H', 'I137E', 'I137K', 'I137Q', 'I137N', 'I137P', 'I137R', 'I137S', 'I137T', 'I137V', 'I137W', 'I137Y', 'I137L', 'I137M', 'T138A', 'T138C', 'T138I', 'T138E', 'T138F', 'T138G', 'T138H', 'T138K', 'T138L', 'T138M', 'T138D', 'T138P', 'T138Q', 'T138R', 'T138S', 'T138V', 'T138W', 'T138Y', 'T138N', 'G139P', 'G139W', 'G139V', 'G139T', 'G139S', 'G139R', 'G139Q', 'G139E', 'G139F', 'G139M', 'G139L', 'G139K', 'G139I', 'G139H', 'G139C', 'G139N', 'G139A', 'G139Y', 'G139D', 'H140C', 'H140W', 'H140V', 'H140T', 'H140S', 'H140R', 'H140Q', 'H140P', 'H140A', 'H140M', 'H140D', 'H140N', 'H140E', 'H140F', 'H140Y', 'H140I', 'H140L', 'H140K', 'H140G', 'H142Y', 'H142W', 'H142V', 'H142T', 'H142S', 'H142R', 'H142N', 'H142M', 'H142L', 'H142I', 'H142G', 'H142F', 'H142E', 'H142D', 'H142P', 'H142K', 'H142Q', 'H142A', 'H142C', 'K143W', 'K143Y', 'K143V', 'K143T', 'K143S', 'K143R', 'K143P', 'K143N', 'K143M', 'K143Q', 'K143I', 'K143L', 'K143C', 'K143D', 'K143E', 'K143A', 'K143G', 'K143H', 'K143F', 'A144Q', 'A144Y', 'A144W', 'A144V', 'A144T', 'A144S', 'A144R', 'A144P', 'A144G', 'A144M', 'A144C', 'A144N', 'A144E', 'A144F', 'A144D', 'A144H', 'A144I', 'A144K', 'A144L', 'K145A', 'K145C', 'K145D', 'K145E', 'K145F', 'K145G', 'K145H', 'K145I', 'K145Y', 'K145L', 'K145V', 'K145T', 'K145S', 'K145W', 'K145Q', 'K145P', 'K145N', 'K145M', 'K145R', 'Q146K', 'Q146V', 'Q146R', 'Q146P', 'Q146N', 'Q146M', 'Q146L', 'Q146I', 'Q146D', 'Q146G', 'Q146F', 'Q146E', 'Q146T', 'Q146C', 'Q146Y', 'Q146A', 'Q146W', 'Q146H', 'Q146S', 'L147A', 'L147C', 'L147E', 'L147Y', 'L147W', 'L147V', 'L147T', 'L147S', 'L147R', 'L147D', 'L147Q', 'L147N', 'L147M', 'L147K', 'L147I', 'L147H', 'L147F', 'L147P', 'L147G', 'L149N', 'L149G', 'L149V', 'L149T', 'L149S', 'L149R', 'L149Q', 'L149P', 'L149M', 'L149K', 'L149I', 'L149H', 'L149F', 'L149E', 'L149D', 'L149A', 'L149C', 'L149W', 'L149Y', 'K150V', 'K150W', 'K150Y', 'K150R', 'K150Q', 'K150T', 'K150S', 'K150A', 'K150C', 'K150P', 'K150D', 'K150F', 'K150E', 'K150N', 'K150M', 'K150L', 'K150I', 'K150H', 'K150G', 'M153Y', 'M153E', 'M153W', 'M153F', 'M153V', 'M153D', 'M153C', 'M153A', 'M153G', 'M153H', 'M153I', 'M153K', 'M153L', 'M153N', 'M153P', 'M153Q', 'M153R', 'M153S', 'M153T', 'A154R', 'A154S', 'A154T', 'A154Y', 'A154W', 'A154Q', 'A154V', 'A154P', 'A154I', 'A154M', 'A154L', 'A154K', 'A154H', 'A154G', 'A154F', 'A154E', 'A154D', 'A154C', 'A154N', 'L155S', 'L155R', 'L155Q', 'L155K', 'L155N', 'L155M', 'L155T', 'L155I', 'L155P', 'L155V', 'L155C', 'L155F', 'L155E', 'L155D', 'L155A', 'L155Y', 'L155G', 'L155W', 'L155H', 'R156C', 'R156D', 'R156A', 'R156M', 'R156F', 'R156E', 'R156Y', 'R156W', 'R156T', 'R156S', 'R156Q', 'R156V', 'R156N', 'R156L', 'R156K', 'R156I', 'R156H', 'R156G', 'R156P', 'P159H', 'P159C', 'P159A', 'P159D', 'P159E', 'P159F', 'P159G', 'P159I', 'P159K', 'P159V', 'P159M', 'P159N', 'P159Q', 'P159R', 'P159S', 'P159T', 'P159W', 'P159L', 'P159Y', 'I160K', 'I160H', 'I160E', 'I160L', 'I160F', 'I160G', 'I160M', 'I160T', 'I160P', 'I160Q', 'I160R', 'I160S', 'I160V', 'I160Y', 'I160N', 'I160D', 'I160W', 'I160A', 'I160C', 'G161A', 'G161Y', 'G161W', 'G161V', 'G161S', 'G161R', 'G161Q', 'G161P', 'G161N', 'G161K', 'G161I', 'G161H', 'G161F', 'G161E', 'G161D', 'G161C', 'G161L', 'G161T', 'G161M', 'D162W', 'D162T', 'D162S', 'D162R', 'D162Q', 'D162P', 'D162N', 'D162V', 'D162M', 'D162K', 'D162I', 'D162H', 'D162G', 'D162F', 'D162E', 'D162L', 'D162C', 'D162A', 'D162Y', 'Q163C', 'Q163V', 'Q163T', 'Q163S', 'Q163R', 'Q163P', 'Q163N', 'Q163M', 'Q163L', 'Q163I', 'Q163H', 'Q163G', 'Q163F', 'Q163E', 'Q163D', 'Q163W', 'Q163K', 'Q163A', 'Q163Y', 'W164L', 'W164V', 'W164T', 'W164S', 'W164R', 'W164Q', 'W164P', 'W164Y', 'W164N', 'W164C', 'W164K', 'W164I', 'W164H', 'W164G', 'W164F', 'W164M', 'W164E', 'W164A', 'W164D', 'E165D', 'E165F', 'E165G', 'E165H', 'E165I', 'E165K', 'E165Y', 'E165W', 'E165P', 'E165Q', 'E165R', 'E165S', 'E165T', 'E165V', 'E165M', 'E165L', 'E165N', 'E165A', 'E165C', 'E166Y', 'E166W', 'E166V', 'E166T', 'E166S', 'E166R', 'E166Q', 'E166P', 'E166C', 'E166K', 'E166I', 'E166H', 'E166G', 'E166F', 'E166D', 'E166A', 'E166L', 'E166M', 'E166N', 'E167M', 'E167C', 'E167D', 'E167F', 'E167G', 'E167H', 'E167I', 'E167K', 'E167L', 'E167Y', 'E167N', 'E167P', 'E167Q', 'E167R', 'E167S', 'E167T', 'E167V', 'E167W', 'E167A', 'E168C', 'E168A', 'E168D', 'E168H', 'E168I', 'E168K', 'E168L', 'E168M', 'E168N', 'E168G', 'E168P', 'E168R', 'E168S', 'E168T', 'E168V', 'E168W', 'E168Y', 'E168Q', 'E168F', 'G169E', 'G169A', 'G169F', 'G169H', 'G169I', 'G169K', 'G169L', 'G169M', 'G169C', 'G169N', 'G169P', 'G169Q', 'G169R', 'G169S', 'G169T', 'G169V', 'G169W', 'G169D', 'G169Y', 'G170V', 'G170K', 'G170C', 'G170D', 'G170E', 'G170F', 'G170H', 'G170I', 'G170Y', 'G170A', 'G170L', 'G170N', 'G170P', 'G170Q', 'G170R', 'G170S', 'G170T', 'G170M', 'G170W', 'F171Y', 'F171A', 'F171C', 'F171W', 'F171D', 'F171E', 'F171G', 'F171H', 'F171K', 'F171L', 'F171M', 'F171I', 'F171P', 'F171Q', 'F171R', 'F171S', 'F171T', 'F171V', 'F171N', 'N172A', 'N172H', 'N172C', 'N172D', 'N172E', 'N172F', 'N172G', 'N172I', 'N172Y', 'N172L', 'N172M', 'N172P', 'N172Q', 'N172R', 'N172S', 'N172T', 'N172V', 'N172K', 'N172W', 'Y173A', 'Y173R', 'Y173W', 'Y173V', 'Y173T', 'Y173S', 'Y173Q', 'Y173P', 'Y173M', 'Y173N', 'Y173K', 'Y173I', 'Y173H', 'Y173G', 'Y173F', 'Y173E', 'Y173D', 'Y173L', 'Y173C', 'A174W', 'A174S', 'A174V', 'A174T', 'A174C', 'A174D', 'A174R', 'A174Y', 'A174Q', 'A174K', 'A174N', 'A174E', 'A174P', 'A174G', 'A174H', 'A174F', 'A174L', 'A174M', 'A174I', 'V175M', 'V175Y', 'V175W', 'V175T', 'V175S', 'V175R', 'V175Q', 'V175P', 'V175N', 'V175L', 'V175H', 'V175I', 'V175G', 'V175F', 'V175E', 'V175D', 'V175C', 'V175A', 'V175K', 'D176V', 'D176W', 'D176Q', 'D176T', 'D176S', 'D176R', 'D176P', 'D176F', 'D176M', 'D176L', 'D176K', 'D176I', 'D176H', 'D176G', 'D176Y', 'D176N', 'D176A', 'D176C', 'D176E', 'L177W', 'L177D', 'L177E', 'L177F', 'L177G', 'L177H', 'L177I', 'L177K', 'L177M', 'L177P', 'L177Q', 'L177R', 'L177S', 'L177T', 'L177V', 'L177Y', 'L177N', 'L177C', 'L177A', 'V178T', 'V178S', 'V178A', 'V178C', 'V178D', 'V178E', 'V178F', 'V178G', 'V178H', 'V178K', 'V178W', 'V178L', 'V178M', 'V178N', 'V178P', 'V178Q', 'V178R', 'V178Y', 'V178I', 'K179R', 'K179S', 'K179T', 'K179H', 'K179W', 'K179Y', 'K179Q', 'K179V', 'K179P', 'K179A', 'K179M', 'K179L', 'K179I', 'K179G', 'K179F', 'K179E', 'K179D', 'K179C', 'K179N', 'H180Y', 'H180W', 'H180V', 'H180C', 'H180D', 'H180E', 'H180F', 'H180G', 'H180I', 'H180K', 'H180T', 'H180L', 'H180M', 'H180N', 'H180P', 'H180Q', 'H180R', 'H180S', 'H180A', 'I181Y', 'I181W', 'I181D', 'I181V', 'I181T', 'I181S', 'I181R', 'I181Q', 'I181P', 'I181C', 'I181N', 'I181L', 'I181K', 'I181H', 'I181G', 'I181F', 'I181E', 'I181M', 'I181A', 'S183K', 'S183C', 'S183V', 'S183T', 'S183R', 'S183Q', 'S183P', 'S183N', 'S183A', 'S183M', 'S183I', 'S183H', 'S183G', 'S183F', 'S183E', 'S183D', 'S183L', 'S183W', 'S183Y', 'E184Y', 'E184V', 'E184T', 'E184S', 'E184R', 'E184Q', 'E184P', 'E184N', 'E184M', 'E184W', 'E184K', 'E184I', 'E184H', 'E184G', 'E184F', 'E184D', 'E184C', 'E184A', 'E184L', 'G186I', 'G186H', 'G186C', 'G186E', 'G186D', 'G186K', 'G186F', 'G186L', 'G186Q', 'G186N', 'G186P', 'G186R', 'G186S', 'G186T', 'G186V', 'G186A', 'G186M', 'G186W', 'G186Y', 'D187T', 'D187A', 'D187V', 'D187W', 'D187Y', 'D187S', 'D187Q', 'D187P', 'D187N', 'D187R', 'D187L', 'D187M', 'D187E', 'D187F', 'D187G', 'D187C', 'D187I', 'D187K', 'D187H', 'Y188P', 'Y188W', 'Y188V', 'Y188T', 'Y188S', 'Y188R', 'Y188Q', 'Y188N', 'Y188A', 'Y188L', 'Y188K', 'Y188I', 'Y188H', 'Y188G', 'Y188F', 'Y188E', 'Y188D', 'Y188M', 'Y188C', 'D190Y', 'D190C', 'D190V', 'D190T', 'D190S', 'D190R', 'D190Q', 'D190P', 'D190A', 'D190M', 'D190K', 'D190W', 'D190H', 'D190G', 'D190F', 'D190E', 'D190L', 'D190I', 'D190N', 'C192D', 'C192V', 'C192T', 'C192S', 'C192R', 'C192Q', 'C192P', 'C192N', 'C192W', 'C192Y', 'C192L', 'C192K', 'C192I', 'C192H', 'C192G', 'C192F', 'C192E', 'C192M', 'C192A', 'A194Y', 'A194E', 'A194W', 'A194V', 'A194T', 'A194S', 'A194R', 'A194Q', 'A194P', 'A194N', 'A194M', 'A194L', 'A194K', 'A194I', 'A194H', 'A194G', 'A194F', 'A194D', 'A194C', 'P197M', 'P197W', 'P197D', 'P197E', 'P197F', 'P197G', 'P197H', 'P197I', 'P197Y', 'P197K', 'P197N', 'P197Q', 'P197R', 'P197S', 'P197T', 'P197V', 'P197L', 'P197C', 'P197A', 'K198Y', 'K198C', 'K198D', 'K198E', 'K198F', 'K198G', 'K198H', 'K198I', 'K198L', 'K198A', 'K198N', 'K198P', 'K198Q', 'K198R', 'K198S', 'K198T', 'K198V', 'K198W', 'K198M', 'G199N', 'G199P', 'G199T', 'G199R', 'G199S', 'G199M', 'G199Q', 'G199L', 'G199F', 'G199I', 'G199H', 'G199E', 'G199D', 'G199C', 'G199A', 'G199V', 'G199K', 'G199W', 'G199Y', 'H200C', 'H200W', 'H200V', 'H200T', 'H200S', 'H200R', 'H200Q', 'H200P', 'H200A', 'H200N', 'H200Y', 'H200K', 'H200I', 'H200G', 'H200F', 'H200E', 'H200D', 'H200M', 'H200L', 'P201L', 'P201C', 'P201Y', 'P201W', 'P201V', 'P201T', 'P201R', 'P201Q', 'P201N', 'P201M', 'P201S', 'P201K', 'P201D', 'P201E', 'P201F', 'P201A', 'P201H', 'P201I', 'P201G', 'E202N', 'E202Y', 'E202V', 'E202T', 'E202S', 'E202R', 'E202Q', 'E202P', 'E202W', 'E202M', 'E202K', 'E202I', 'E202H', 'E202G', 'E202F', 'E202D', 'E202C', 'E202A', 'E202L', 'A203Q', 'A203R', 'A203W', 'A203T', 'A203V', 'A203P', 'A203S', 'A203D', 'A203I', 'A203M', 'A203L', 'A203K', 'A203H', 'A203G', 'A203F', 'A203E', 'A203Y', 'A203N', 'A203C', 'G204A', 'G204V', 'G204W', 'G204T', 'G204S', 'G204R', 'G204Q', 'G204P', 'G204N', 'G204M', 'G204Y', 'G204K', 'G204I', 'G204H', 'G204F', 'G204E', 'G204D', 'G204C', 'G204L', 'S205D', 'S205W', 'S205V', 'S205T', 'S205Q', 'S205P', 'S205N', 'S205M', 'S205H', 'S205K', 'S205I', 'S205G', 'S205F', 'S205E', 'S205Y', 'S205L', 'S205R', 'S205A', 'S205C', 'F206D', 'F206Y', 'F206W', 'F206V', 'F206T', 'F206S', 'F206R', 'F206Q', 'F206P', 'F206N', 'F206L', 'F206K', 'F206A', 'F206I', 'F206H', 'F206G', 'F206E', 'F206C', 'F206M', 'E207M', 'E207A', 'E207Y', 'E207W', 'E207V', 'E207T', 'E207R', 'E207Q', 'E207P', 'E207N', 'E207S', 'E207K', 'E207I', 'E207H', 'E207G', 'E207F', 'E207D', 'E207C', 'E207L', 'A208I', 'A208H', 'A208G', 'A208C', 'A208E', 'A208D', 'A208K', 'A208F', 'A208L', 'A208W', 'A208N', 'A208M', 'A208V', 'A208T', 'A208Y', 'A208R', 'A208Q', 'A208P', 'A208S', 'D209K', 'D209A', 'D209C', 'D209E', 'D209F', 'D209H', 'D209I', 'D209L', 'D209G', 'D209N', 'D209M', 'D209Y', 'D209V', 'D209T', 'D209W', 'D209R', 'D209Q', 'D209P', 'D209S', 'L210D', 'L210I', 'L210E', 'L210F', 'L210G', 'L210H', 'L210K', 'L210Q', 'L210N', 'L210P', 'L210V', 'L210R', 'L210S', 'L210T', 'L210C', 'L210M', 'L210A', 'L210W', 'L210Y', 'K211A', 'K211Y', 'K211W', 'K211V', 'K211T', 'K211R', 'K211Q', 'K211P', 'K211N', 'K211M', 'K211L', 'K211I', 'K211H', 'K211G', 'K211F', 'K211E', 'K211D', 'K211C', 'K211S', 'H212N', 'H212A', 'H212Y', 'H212V', 'H212T', 'H212S', 'H212R', 'H212Q', 'H212P', 'H212M', 'H212W', 'H212K', 'H212I', 'H212G', 'H212F', 'H212E', 'H212D', 'H212C', 'H212L', 'L213D', 'L213V', 'L213T', 'L213S', 'L213R', 'L213Q', 'L213P', 'L213N', 'L213H', 'L213K', 'L213I', 'L213G', 'L213F', 'L213E', 'L213W', 'L213M', 'L213Y', 'L213A', 'L213C', 'K214W', 'K214Y', 'K214V', 'K214T', 'K214S', 'K214R', 'K214P', 'K214N', 'K214M', 'K214Q', 'K214I', 'K214H', 'K214G', 'K214F', 'K214E', 'K214D', 'K214C', 'K214A', 'K214L', 'E215R', 'E215Y', 'E215W', 'E215V', 'E215T', 'E215S', 'E215Q', 'E215D', 'E215N', 'E215L', 'E215K', 'E215I', 'E215H', 'E215G', 'E215F', 'E215C', 'E215P', 'E215A', 'E215M', 'V217G', 'V217Y', 'V217W', 'V217T', 'V217R', 'V217Q', 'V217P', 'V217N', 'V217M', 'V217L', 'V217K', 'V217S', 'V217H', 'V217F', 'V217E', 'V217D', 'V217C', 'V217A', 'V217I', 'S218E', 'S218F', 'S218G', 'S218H', 'S218I', 'S218K', 'S218L', 'S218M', 'S218N', 'S218R', 'S218Q', 'S218T', 'S218V', 'S218Y', 'S218W', 'S218C', 'S218A', 'S218P', 'S218D', 'A219C', 'A219D', 'A219S', 'A219Y', 'A219T', 'A219R', 'A219Q', 'A219P', 'A219N', 'A219M', 'A219W', 'A219L', 'A219I', 'A219H', 'A219G', 'A219F', 'A219E', 'A219K', 'A219V', 'D222V', 'D222W', 'D222Y', 'D222A', 'D222G', 'D222C', 'D222E', 'D222F', 'D222T', 'D222H', 'D222I', 'D222K', 'D222M', 'D222N', 'D222P', 'D222Q', 'D222R', 'D222S', 'D222L', 'F223S', 'F223R', 'F223T', 'F223Q', 'F223W', 'F223Y', 'F223V', 'F223N', 'F223P', 'F223L', 'F223M', 'F223A', 'F223D', 'F223E', 'F223C', 'F223H', 'F223I', 'F223K', 'F223G', 'I225Q', 'I225Y', 'I225A', 'I225V', 'I225W', 'I225S', 'I225R', 'I225P', 'I225T', 'I225M', 'I225L', 'I225K', 'I225H', 'I225G', 'I225F', 'I225E', 'I225D', 'I225C', 'I225N', 'T226N', 'T226P', 'T226W', 'T226R', 'T226S', 'T226M', 'T226Q', 'T226L', 'T226G', 'T226I', 'T226H', 'T226F', 'T226E', 'T226D', 'T226C', 'T226A', 'T226Y', 'T226K', 'T226V', 'F229Y', 'F229N', 'F229P', 'F229T', 'F229V', 'F229W', 'F229C', 'F229D', 'F229E', 'F229G', 'F229A', 'F229I', 'F229K', 'F229L', 'F229M', 'F229S', 'F229R', 'F229Q', 'F229H', 'F230P', 'F230R', 'F230S', 'F230W', 'F230V', 'F230Y', 'F230N', 'F230T', 'F230M', 'F230Q', 'F230K', 'F230I', 'F230H', 'F230G', 'F230E', 'F230D', 'F230C', 'F230A', 'F230L', 'E231I', 'E231H', 'E231G', 'E231A', 'E231D', 'E231C', 'E231Y', 'E231F', 'E231K', 'E231M', 'E231V', 'E231T', 'E231S', 'E231R', 'E231Q', 'E231N', 'E231P', 'E231L', 'E231W', 'A232E', 'A232Y', 'A232F', 'A232D', 'A232C', 'A232W', 'A232N', 'A232Q', 'A232P', 'A232M', 'A232L', 'A232K', 'A232I', 'A232H', 'A232G', 'A232T', 'A232R', 'A232V', 'A232S', 'D233F', 'D233G', 'D233Y', 'D233H', 'D233I', 'D233K', 'D233R', 'D233S', 'D233N', 'D233W', 'D233V', 'D233P', 'D233Q', 'D233T', 'D233M', 'D233L', 'D233C', 'D233A', 'D233E', 'T234S', 'T234A', 'T234C', 'T234D', 'T234E', 'T234F', 'T234G', 'T234H', 'T234I', 'T234L', 'T234M', 'T234N', 'T234P', 'T234Q', 'T234R', 'T234K', 'T234Y', 'T234W', 'T234V', 'F235P', 'F235W', 'F235V', 'F235T', 'F235Y', 'F235S', 'F235R', 'F235Q', 'F235N', 'F235M', 'F235K', 'F235I', 'F235H', 'F235G', 'F235E', 'F235D', 'F235C', 'F235A', 'F235L', 'F236T', 'F236S', 'F236V', 'F236P', 'F236Y', 'F236R', 'F236W', 'F236Q', 'F236K', 'F236N', 'F236A', 'F236D', 'F236E', 'F236G', 'F236C', 'F236I', 'F236L', 'F236M', 'F236H', 'R237M', 'R237F', 'R237G', 'R237H', 'R237I', 'R237K', 'R237L', 'R237N', 'R237V', 'R237Q', 'R237S', 'R237T', 'R237E', 'R237W', 'R237Y', 'R237C', 'R237P', 'R237D', 'R237A', 'F238N', 'F238W', 'F238V', 'F238T', 'F238S', 'F238R', 'F238Q', 'F238P', 'F238M', 'F238L', 'F238K', 'F238I', 'F238H', 'F238G', 'F238E', 'F238D', 'F238C', 'F238A', 'F238Y', 'V239A', 'V239Y', 'V239L', 'V239E', 'V239F', 'V239G', 'V239H', 'V239I', 'V239K', 'V239M', 'V239C', 'V239N', 'V239Q', 'V239R', 'V239S', 'V239T', 'V239W', 'V239P', 'V239D', 'K240A', 'K240D', 'K240E', 'K240Y', 'K240C', 'K240G', 'K240H', 'K240I', 'K240L', 'K240M', 'K240N', 'K240P', 'K240F', 'K240R', 'K240S', 'K240T', 'K240V', 'K240W', 'K240Q', 'A241W', 'A241V', 'A241T', 'A241S', 'A241R', 'A241Q', 'A241P', 'A241L', 'A241M', 'A241K', 'A241I', 'A241H', 'A241G', 'A241F', 'A241Y', 'A241N', 'A241C', 'A241D', 'A241E', 'T243C', 'T243W', 'T243V', 'T243S', 'T243R', 'T243Q', 'T243P', 'T243A', 'T243N', 'T243K', 'T243I', 'T243H', 'T243G', 'T243F', 'T243E', 'T243D', 'T243M', 'T243Y', 'T243L', 'D244W', 'D244Y', 'D244A', 'D244C', 'D244E', 'D244G', 'D244H', 'D244I', 'D244K', 'D244F', 'D244M', 'D244L', 'D244T', 'D244S', 'D244R', 'D244V', 'D244P', 'D244N', 'D244Q', 'M245N', 'M245W', 'M245V', 'M245T', 'M245S', 'M245R', 'M245Q', 'M245P', 'M245Y', 'M245L', 'M245I', 'M245H', 'M245G', 'M245F', 'M245E', 'M245D', 'M245C', 'M245A', 'M245K', 'G246K', 'G246I', 'G246H', 'G246C', 'G246E', 'G246D', 'G246L', 'G246F', 'G246M', 'G246R', 'G246P', 'G246Q', 'G246S', 'G246T', 'G246V', 'G246W', 'G246Y', 'G246N', 'G246A', 'T248A', 'T248C', 'T248Y', 'T248W', 'T248V', 'T248S', 'T248R', 'T248P', 'T248N', 'T248M', 'T248Q', 'T248K', 'T248I', 'T248H', 'T248G', 'T248F', 'T248E', 'T248D', 'T248L', 'C249L', 'C249E', 'C249F', 'C249G', 'C249H', 'C249I', 'C249K', 'C249N', 'C249M', 'C249Q', 'C249R', 'C249S', 'C249T', 'C249V', 'C249W', 'C249Y', 'C249D', 'C249P', 'C249A', 'F256Y', 'F256C', 'F256W', 'F256V', 'F256T', 'F256S', 'F256R', 'F256P', 'F256N', 'F256M', 'F256Q', 'F256A', 'F256L', 'F256K', 'F256I', 'F256H', 'F256G', 'F256E', 'F256D', 'P257L', 'P257M', 'P257N', 'P257V', 'P257R', 'P257T', 'P257K', 'P257Q', 'P257S', 'P257I', 'P257G', 'P257H', 'P257Y', 'P257A', 'P257W', 'P257D', 'P257E', 'P257F', 'P257C', 'Q259W', 'Q259V', 'Q259T', 'Q259S', 'Q259R', 'Q259P', 'Q259N', 'Q259M', 'Q259K', 'Q259I', 'Q259H', 'Q259F', 'Q259E', 'Q259D', 'Q259C', 'Q259Y', 'Q259A', 'Q259L', 'Q259G', 'G260Y', 'G260A', 'G260W', 'G260V', 'G260T', 'G260S', 'G260R', 'G260Q', 'G260P', 'G260N', 'G260M', 'G260L', 'G260K', 'G260I', 'G260H', 'G260F', 'G260E', 'G260C', 'G260D', 'Y261K', 'Y261V', 'Y261E', 'Y261F', 'Y261G', 'Y261H', 'Y261I', 'Y261A', 'Y261W', 'Y261L', 'Y261N', 'Y261P', 'Y261Q', 'Y261R', 'Y261S', 'Y261T', 'Y261M', 'Y261D', 'Y261C', 'H262Y', 'H262W', 'H262S', 'H262R', 'H262Q', 'H262P', 'H262N', 'H262M', 'H262L', 'H262T', 'H262I', 'H262K', 'H262A', 'H262C', 'H262V', 'H262E', 'H262F', 'H262G', 'H262D', 'S263V', 'S263T', 'S263R', 'S263Q', 'S263P', 'S263N', 'S263M', 'S263K', 'S263I', 'S263H', 'S263F', 'S263E', 'S263D', 'S263C', 'S263A', 'S263L', 'S263G', 'S263Y', 'S263W', 'L264T', 'L264Q', 'L264P', 'L264N', 'L264M', 'L264K', 'L264I', 'L264H', 'L264S', 'L264G', 'L264E', 'L264D', 'L264C', 'L264A', 'L264Y', 'L264W', 'L264V', 'L264F', 'L264R', 'R265A', 'R265C', 'R265W', 'R265Y', 'R265D', 'R265E', 'R265F', 'R265G', 'R265I', 'R265K', 'R265L', 'R265H', 'R265N', 'R265P', 'R265Q', 'R265S', 'R265T', 'R265V', 'R265M', 'Q266D', 'Q266K', 'Q266E', 'Q266F', 'Q266G', 'Q266H', 'Q266I', 'Q266L', 'Q266Y', 'Q266N', 'Q266P', 'Q266R', 'Q266S', 'Q266T', 'Q266V', 'Q266W', 'Q266A', 'Q266M', 'Q266C', 'L267H', 'L267Y', 'L267C', 'L267D', 'L267E', 'L267F', 'L267G', 'L267I', 'L267A', 'L267K', 'L267N', 'L267P', 'L267Q', 'L267R', 'L267S', 'L267T', 'L267M', 'L267W', 'L267V', 'V268S', 'V268C', 'V268D', 'V268E', 'V268F', 'V268G', 'V268H', 'V268I', 'V268K', 'V268L', 'V268M', 'V268N', 'V268P', 'V268Q', 'V268R', 'V268Y', 'V268T', 'V268W', 'V268A', 'K269C', 'K269V', 'K269T', 'K269S', 'K269R', 'K269Q', 'K269P', 'K269W', 'K269N', 'K269L', 'K269I', 'K269H', 'K269G', 'K269F', 'K269E', 'K269M', 'K269A', 'K269D', 'K269Y', 'L270V', 'L270T', 'L270A', 'L270C', 'L270D', 'L270E', 'L270F', 'L270G', 'L270H', 'L270I', 'L270K', 'L270M', 'L270N', 'L270P', 'L270Q', 'L270R', 'L270Y', 'L270S', 'L270W', 'S271A', 'S271H', 'S271E', 'S271D', 'S271C', 'S271I', 'S271G', 'S271Y', 'S271W', 'S271V', 'S271F', 'S271R', 'S271Q', 'S271P', 'S271N', 'S271M', 'S271L', 'S271K', 'S271T', 'K272L', 'K272I', 'K272H', 'K272D', 'K272F', 'K272E', 'K272C', 'K272G', 'K272M', 'K272A', 'K272Q', 'K272N', 'K272Y', 'K272W', 'K272P', 'K272T', 'K272S', 'K272R', 'K272V', 'L273A', 'L273C', 'L273D', 'L273E', 'L273F', 'L273G', 'L273I', 'L273M', 'L273P', 'L273Q', 'L273R', 'L273S', 'L273H', 'L273N', 'L273K', 'L273T', 'L273V', 'L273W', 'L273Y', 'E274C', 'E274H', 'E274I', 'E274K', 'E274L', 'E274M', 'E274N', 'E274P', 'E274D', 'E274R', 'E274S', 'E274T', 'E274V', 'E274W', 'E274Y', 'E274Q', 'E274G', 'E274F', 'E274A', 'V275T', 'V275D', 'V275E', 'V275F', 'V275G', 'V275H', 'V275I', 'V275K', 'V275C', 'V275L', 'V275N', 'V275P', 'V275Q', 'V275R', 'V275S', 'V275W', 'V275Y', 'V275M', 'V275A', 'Q277D', 'Q277A', 'Q277C', 'Q277E', 'Q277F', 'Q277G', 'Q277H', 'Q277K', 'Q277L', 'Q277M', 'Q277I', 'Q277P', 'Q277R', 'Q277S', 'Q277T', 'Q277V', 'Q277W', 'Q277Y', 'Q277N', 'E278I', 'E278H', 'E278G', 'E278A', 'E278D', 'E278C', 'E278K', 'E278F', 'E278L', 'E278N', 'E278P', 'E278Q', 'E278R', 'E278S', 'E278T', 'E278V', 'E278W', 'E278Y', 'E278M', 'I279C', 'I279T', 'I279S', 'I279R', 'I279Q', 'I279P', 'I279N', 'I279G', 'I279L', 'I279K', 'I279H', 'I279F', 'I279E', 'I279D', 'I279M', 'I279A', 'I279Y', 'I279W', 'I279V', 'K280V', 'K280T', 'K280S', 'K280R', 'K280Q', 'K280P', 'K280N', 'K280M', 'K280W', 'K280H', 'K280G', 'K280F', 'K280E', 'K280D', 'K280C', 'K280A', 'K280I', 'K280L', 'K280Y', 'D281A', 'D281W', 'D281V', 'D281T', 'D281S', 'D281R', 'D281Q', 'D281Y', 'D281P', 'D281M', 'D281L', 'D281K', 'D281I', 'D281H', 'D281G', 'D281N', 'D281C', 'D281E', 'D281F', 'V282Y', 'V282E', 'V282F', 'V282G', 'V282H', 'V282I', 'V282K', 'V282L', 'V282M', 'V282P', 'V282Q', 'V282R', 'V282S', 'V282T', 'V282W', 'V282N', 'V282D', 'V282C', 'V282A', 'I283T', 'I283S', 'I283R', 'I283Q', 'I283P', 'I283N', 'I283M', 'I283L', 'I283K', 'I283H', 'I283G', 'I283F', 'I283E', 'I283D', 'I283C', 'I283V', 'I283Y', 'I283W', 'I283A', 'E284A', 'E284Y', 'E284W', 'E284V', 'E284T', 'E284S', 'E284R', 'E284Q', 'E284P', 'E284N', 'E284L', 'E284K', 'E284I', 'E284H', 'E284G', 'E284F', 'E284D', 'E284C', 'E284M', 'P285A', 'P285Y', 'P285W', 'P285V', 'P285T', 'P285S', 'P285Q', 'P285N', 'P285M', 'P285R', 'P285K', 'P285L', 'P285D', 'P285E', 'P285F', 'P285C', 'P285H', 'P285I', 'P285G', 'I286N', 'I286Y', 'I286V', 'I286T', 'I286S', 'I286R', 'I286Q', 'I286P', 'I286M', 'I286W', 'I286K', 'I286H', 'I286G', 'I286F', 'I286E', 'I286D', 'I286C', 'I286A', 'I286L', 'K287W', 'K287V', 'K287T', 'K287S', 'K287R', 'K287Q', 'K287I', 'K287N', 'K287M', 'K287L', 'K287H', 'K287G', 'K287F', 'K287P', 'K287Y', 'K287D', 'K287E', 'K287C', 'K287A', 'D288G', 'D288Y', 'D288W', 'D288V', 'D288T', 'D288S', 'D288R', 'D288Q', 'D288P', 'D288N', 'D288M', 'D288L', 'D288I', 'D288E', 'D288F', 'D288H', 'D288K', 'D288A', 'D288C', 'A291C', 'A291D', 'A291E', 'A291F', 'A291G', 'A291H', 'A291I', 'A291K', 'A291W', 'A291M', 'A291N', 'A291P', 'A291Q', 'A291R', 'A291S', 'A291T', 'A291V', 'A291Y', 'A291L', 'I293Y', 'I293D', 'I293C', 'I293A', 'I293W', 'I293T', 'I293S', 'I293R', 'I293Q', 'I293V', 'I293N', 'I293P', 'I293F', 'I293G', 'I293H', 'I293E', 'I293K', 'I293L', 'I293M', 'R294V', 'R294T', 'R294S', 'R294Q', 'R294P', 'R294N', 'R294M', 'R294L', 'R294W', 'R294Y', 'R294H', 'R294I', 'R294A', 'R294C', 'R294K', 'R294E', 'R294F', 'R294G', 'R294D', 'E299S', 'E299R', 'E299Q', 'E299P', 'E299N', 'E299M', 'E299L', 'E299H', 'E299G', 'E299D', 'E299C', 'E299A', 'E299W', 'E299Y', 'E299I', 'E299F', 'E299K', 'E299V', 'E299T', 'L300V', 'L300T', 'L300S', 'L300R', 'L300Q', 'L300P', 'L300N', 'L300W', 'L300A', 'L300I', 'L300H', 'L300G', 'L300F', 'L300E', 'L300D', 'L300C', 'L300K', 'L300M', 'L300Y', 'A301K', 'A301E', 'A301F', 'A301G', 'A301H', 'A301I', 'A301L', 'A301M', 'A301N', 'A301P', 'A301Q', 'A301R', 'A301S', 'A301T', 'A301V', 'A301W', 'A301D', 'A301C', 'A301Y', 'S303E', 'S303V', 'S303T', 'S303R', 'S303Q', 'S303P', 'S303A', 'S303N', 'S303M', 'S303L', 'S303K', 'S303I', 'S303H', 'S303G', 'S303F', 'S303D', 'S303W', 'S303Y', 'S303C', 'Q306N', 'Q306F', 'Q306G', 'Q306H', 'Q306I', 'Q306K', 'Q306L', 'Q306M', 'Q306C', 'Q306P', 'Q306R', 'Q306S', 'Q306T', 'Q306V', 'Q306W', 'Q306Y', 'Q306A', 'Q306E', 'Q306D', 'E307C', 'E307W', 'E307D', 'E307A', 'E307Y', 'E307V', 'E307T', 'E307R', 'E307Q', 'E307P', 'E307S', 'E307M', 'E307L', 'E307K', 'E307I', 'E307H', 'E307G', 'E307F', 'E307N', 'A310Q', 'A310Y', 'A310W', 'A310V', 'A310T', 'A310S', 'A310R', 'A310P', 'A310E', 'A310M', 'A310L', 'A310K', 'A310I', 'A310H', 'A310G', 'A310F', 'A310C', 'A310N', 'A310D', 'G312Y', 'G312W', 'G312A', 'G312C', 'G312D', 'G312E', 'G312F', 'G312H', 'G312I', 'G312K', 'G312V', 'G312M', 'G312N', 'G312P', 'G312Q', 'G312R', 'G312S', 'G312T', 'G312L', 'L313I', 'L313H', 'L313G', 'L313D', 'L313E', 'L313K', 'L313C', 'L313F', 'L313M', 'L313R', 'L313P', 'L313Q', 'L313S', 'L313T', 'L313V', 'L313W', 'L313A', 'L313N', 'L313Y', 'L317Y', 'L317W', 'L317A', 'L317C', 'L317D', 'L317E', 'L317F', 'L317H', 'L317I', 'L317K', 'L317G', 'L317N', 'L317P', 'L317Q', 'L317R', 'L317S', 'L317T', 'L317V', 'L317M', 'T321L', 'T321S', 'T321R', 'T321Q', 'T321P', 'T321N', 'T321M', 'T321K', 'T321W', 'T321H', 'T321G', 'T321F', 'T321E', 'T321D', 'T321C', 'T321A', 'T321V', 'T321I', 'T321Y', 'L322D', 'L322A', 'L322E', 'L322C', 'L322F', 'L322G', 'L322H', 'L322I', 'L322K', 'L322M', 'L322Y', 'L322P', 'L322Q', 'L322R', 'L322S', 'L322T', 'L322V', 'L322N', 'L322W', 'N323C', 'N323D', 'N323E', 'N323F', 'N323G', 'N323H', 'N323I', 'N323M', 'N323L', 'N323P', 'N323Q', 'N323R', 'N323S', 'N323V', 'N323W', 'N323K', 'N323T', 'N323Y', 'N323A', 'R324E', 'R324D', 'R324F', 'R324G', 'R324H', 'R324I', 'R324K', 'R324M', 'R324N', 'R324L', 'R324Q', 'R324A', 'R324P', 'R324Y', 'R324W', 'R324C', 'R324T', 'R324S', 'R324V', 'E325C', 'E325M', 'E325G', 'E325H', 'E325I', 'E325K', 'E325L', 'E325A', 'E325N', 'E325Q', 'E325R', 'E325S', 'E325T', 'E325V', 'E325W', 'E325D', 'E325Y', 'E325P', 'E325F', 'M326F', 'M326W', 'M326D', 'M326C', 'M326A', 'M326G', 'M326H', 'M326I', 'M326K', 'M326L', 'M326N', 'M326P', 'M326E', 'M326R', 'M326S', 'M326T', 'M326V', 'M326Y', 'M326Q', 'A327M', 'A327C', 'A327W', 'A327V', 'A327T', 'A327S', 'A327R', 'A327Q', 'A327P', 'A327N', 'A327Y', 'A327K', 'A327I', 'A327H', 'A327G', 'A327F', 'A327E', 'A327D', 'A327L', 'T328S', 'T328R', 'T328Q', 'T328P', 'T328N', 'T328M', 'T328L', 'T328F', 'T328H', 'T328G', 'T328E', 'T328D', 'T328C', 'T328A', 'T328V', 'T328I', 'T328Y', 'T328W', 'T328K', 'T329E', 'T329F', 'T329G', 'T329H', 'T329I', 'T329K', 'T329L', 'T329D', 'T329M', 'T329P', 'T329Q', 'T329R', 'T329S', 'T329V', 'T329W', 'T329A', 'T329N', 'T329Y', 'T329C', 'E330N', 'E330F', 'E330G', 'E330H', 'E330I', 'E330K', 'E330L', 'E330M', 'E330P', 'E330Q', 'E330R', 'E330S', 'E330T', 'E330V', 'E330W', 'E330Y', 'E330D', 'E330C', 'E330A', 'L332W', 'L332Y', 'L332E', 'L332F', 'L332G', 'L332H', 'L332I', 'L332K', 'L332M', 'L332A', 'L332C', 'L332D', 'L332T', 'L332S', 'L332V', 'L332Q', 'L332P', 'L332N', 'L332R', 'K333A', 'K333H', 'K333C', 'K333D', 'K333E', 'K333F', 'K333G', 'K333I', 'K333Q', 'K333M', 'K333N', 'K333P', 'K333R', 'K333S', 'K333T', 'K333V', 'K333Y', 'K333L', 'K333W', 'M337Y', 'M337I', 'M337W', 'M337A', 'M337C', 'M337D', 'M337E', 'M337F', 'M337V', 'M337G', 'M337T', 'M337K', 'M337L', 'M337N', 'M337P', 'M337Q', 'M337H', 'M337R', 'M337S', 'W338D', 'W338T', 'W338S', 'W338R', 'W338Q', 'W338P', 'W338N', 'W338C', 'W338M', 'W338K', 'W338I', 'W338H', 'W338G', 'W338F', 'W338E', 'W338L', 'W338A', 'W338Y', 'W338V', 'T339V', 'T339S', 'T339R', 'T339Q', 'T339P', 'T339N', 'T339M', 'T339L', 'T339K', 'T339I', 'T339H', 'T339G', 'T339F', 'T339E', 'T339D', 'T339C', 'T339A', 'T339Y', 'T339W', 'E340N', 'E340Y', 'E340A', 'E340C', 'E340D', 'E340F', 'E340H', 'E340I', 'E340K', 'E340G', 'E340M', 'E340L', 'E340V', 'E340T', 'E340S', 'E340W', 'E340Q', 'E340P', 'E340R', 'D341L', 'D341E', 'D341F', 'D341G', 'D341H', 'D341I', 'D341K', 'D341C', 'D341A', 'D341P', 'D341N', 'D341Y', 'D341W', 'D341V', 'D341M', 'D341S', 'D341R', 'D341Q', 'D341T', 'P342V', 'P342T', 'P342S', 'P342R', 'P342Q', 'P342N', 'P342Y', 'P342M', 'P342K', 'P342I', 'P342H', 'P342F', 'P342E', 'P342D', 'P342C', 'P342A', 'P342L', 'P342G', 'P342W', 'R343A', 'R343W', 'R343V', 'R343T', 'R343S', 'R343Q', 'R343P', 'R343N', 'R343M', 'R343C', 'R343L', 'R343K', 'R343I', 'R343H', 'R343G', 'R343F', 'R343E', 'R343D', 'R343Y', 'R344Y', 'R344L', 'R344A', 'R344E', 'R344F', 'R344G', 'R344H', 'R344I', 'R344K', 'R344D', 'R344M', 'R344P', 'R344Q', 'R344S', 'R344T', 'R344V', 'R344W', 'R344N', 'R344C', 'P345E', 'P345A', 'P345F', 'P345G', 'P345H', 'P345I', 'P345K', 'P345L', 'P345C', 'P345N', 'P345Q', 'P345R', 'P345S', 'P345T', 'P345V', 'P345W', 'P345Y', 'P345D', 'P345M', 'L346M', 'L346C', 'L346A', 'L346Y', 'L346W', 'L346V', 'L346T', 'L346R', 'L346Q', 'L346P', 'L346S', 'L346K', 'L346I', 'L346H', 'L346G', 'L346F', 'L346E', 'L346D', 'L346N', 'W348K', 'W348D', 'W348E', 'W348F', 'W348G', 'W348H', 'W348I', 'W348L', 'W348Y', 'W348N', 'W348P', 'W348Q', 'W348R', 'W348S', 'W348T', 'W348V', 'W348M', 'W348A', 'W348C', 'A349Y', 'A349W', 'A349C', 'A349D', 'A349E', 'A349F', 'A349H', 'A349I', 'A349K', 'A349G', 'A349M', 'A349N', 'A349P', 'A349Q', 'A349R', 'A349S', 'A349T', 'A349L', 'A349V', 'L350H', 'L350G', 'L350F', 'L350A', 'L350D', 'L350C', 'L350I', 'L350E', 'L350K', 'L350R', 'L350N', 'L350P', 'L350Q', 'L350S', 'L350T', 'L350V', 'L350W', 'L350Y', 'L350M', 'S351L', 'S351K', 'S351I', 'S351H', 'S351E', 'S351F', 'S351C', 'S351D', 'S351G', 'S351M', 'S351P', 'S351Q', 'S351R', 'S351T', 'S351V', 'S351W', 'S351Y', 'S351N', 'S351A', 'A352C', 'A352D', 'A352E', 'A352F', 'A352H', 'A352I', 'A352K', 'A352L', 'A352G', 'A352N', 'A352M', 'A352Y', 'A352V', 'A352T', 'A352W', 'A352R', 'A352Q', 'A352P', 'A352S', 'H353N', 'H353T', 'H353S', 'H353R', 'H353Q', 'H353P', 'H353M', 'H353F', 'H353K', 'H353I', 'H353G', 'H353E', 'H353D', 'H353C', 'H353A', 'H353L', 'H353Y', 'H353W', 'H353V', 'P354W', 'P354Y', 'P354C', 'P354D', 'P354E', 'P354F', 'P354G', 'P354H', 'P354I', 'P354A', 'P354L', 'P354M', 'P354N', 'P354Q', 'P354R', 'P354S', 'P354T', 'P354K', 'P354V', 'K355T', 'K355S', 'K355R', 'K355Q', 'K355P', 'K355N', 'K355M', 'K355G', 'K355I', 'K355H', 'K355V', 'K355F', 'K355E', 'K355D', 'K355C', 'K355A', 'K355Y', 'K355L', 'K355W', 'R356T', 'R356W', 'R356A', 'R356C', 'R356D', 'R356E', 'R356F', 'R356G', 'R356H', 'R356I', 'R356K', 'R356M', 'R356N', 'R356P', 'R356Q', 'R356S', 'R356V', 'R356Y', 'R356L', 'R357Y', 'R357W', 'R357T', 'R357S', 'R357Q', 'R357P', 'R357N', 'R357M', 'R357V', 'R357L', 'R357I', 'R357H', 'R357G', 'R357F', 'R357E', 'R357D', 'R357K', 'R357C', 'R357A', 'E358W', 'E358D', 'E358F', 'E358G', 'E358H', 'E358I', 'E358K', 'E358L', 'E358Y', 'E358M', 'E358P', 'E358Q', 'E358R', 'E358S', 'E358T', 'E358V', 'E358A', 'E358N', 'E358C', 'E359C', 'E359D', 'E359Y', 'E359W', 'E359V', 'E359T', 'E359S', 'E359R', 'E359Q', 'E359P', 'E359A', 'E359M', 'E359L', 'E359K', 'E359I', 'E359H', 'E359G', 'E359F', 'E359N', 'D360V', 'D360T', 'D360S', 'D360R', 'D360Q', 'D360P', 'D360N', 'D360M', 'D360H', 'D360K', 'D360I', 'D360G', 'D360F', 'D360E', 'D360C', 'D360Y', 'D360A', 'D360L', 'D360W', 'V361Y', 'V361L', 'V361S', 'V361R', 'V361Q', 'V361P', 'V361N', 'V361M', 'V361T', 'V361I', 'V361G', 'V361F', 'V361E', 'V361D', 'V361C', 'V361A', 'V361H', 'V361W', 'V361K', 'R362V', 'R362Q', 'R362P', 'R362N', 'R362M', 'R362L', 'R362K', 'R362I', 'R362H', 'R362G', 'R362F', 'R362E', 'R362D', 'R362C', 'R362A', 'R362W', 'R362T', 'R362S', 'R362Y', 'P363V', 'P363T', 'P363S', 'P363R', 'P363Q', 'P363N', 'P363M', 'P363L', 'P363E', 'P363I', 'P363H', 'P363G', 'P363F', 'P363C', 'P363D', 'P363W', 'P363K', 'P363Y', 'P363A', 'I364W', 'I364V', 'I364A', 'I364C', 'I364D', 'I364E', 'I364G', 'I364H', 'I364K', 'I364F', 'I364M', 'I364L', 'I364T', 'I364S', 'I364R', 'I364Y', 'I364P', 'I364N', 'I364Q', 'F365P', 'F365W', 'F365Y', 'F365V', 'F365T', 'F365S', 'F365R', 'F365Q', 'F365N', 'F365H', 'F365L', 'F365A', 'F365M', 'F365D', 'F365E', 'F365C', 'F365I', 'F365K', 'F365G', 'W366A', 'W366K', 'W366D', 'W366E', 'W366F', 'W366G', 'W366H', 'W366I', 'W366L', 'W366C', 'W366N', 'W366Y', 'W366V', 'W366T', 'W366M', 'W366R', 'W366Q', 'W366P', 'W366S', 'A367Y', 'A367W', 'A367V', 'A367T', 'A367S', 'A367C', 'A367D', 'A367E', 'A367G', 'A367H', 'A367Q', 'A367K', 'A367L', 'A367M', 'A367N', 'A367P', 'A367F', 'A367I', 'A367R', 'S368R', 'S368Q', 'S368N', 'S368T', 'S368V', 'S368C', 'S368M', 'S368L', 'S368K', 'S368P', 'S368I', 'S368G', 'S368F', 'S368W', 'S368E', 'S368D', 'S368Y', 'S368H', 'S368A', 'R369T', 'R369S', 'R369P', 'R369A', 'R369C', 'R369D', 'R369E', 'R369F', 'R369G', 'R369H', 'R369I', 'R369K', 'R369M', 'R369N', 'R369Q', 'R369Y', 'R369L', 'R369W', 'R369V', 'P370W', 'P370C', 'P370D', 'P370E', 'P370F', 'P370G', 'P370H', 'P370I', 'P370L', 'P370A', 'P370N', 'P370Q', 'P370R', 'P370S', 'P370T', 'P370V', 'P370Y', 'P370M', 'P370K', 'K371E', 'K371D', 'K371C', 'K371G', 'K371H', 'K371I', 'K371L', 'K371M', 'K371A', 'K371N', 'K371Q', 'K371R', 'K371S', 'K371T', 'K371V', 'K371W', 'K371Y', 'K371P', 'K371F', 'S372D', 'S372C', 'S372A', 'S372F', 'S372Y', 'S372W', 'S372V', 'S372T', 'S372R', 'S372Q', 'S372E', 'S372P', 'S372M', 'S372L', 'S372K', 'S372I', 'S372H', 'S372G', 'S372N', 'Y373T', 'Y373W', 'Y373A', 'Y373D', 'Y373E', 'Y373F', 'Y373G', 'Y373H', 'Y373I', 'Y373V', 'Y373K', 'Y373M', 'Y373N', 'Y373P', 'Y373Q', 'Y373R', 'Y373S', 'Y373L', 'Y373C', 'I374R', 'I374W', 'I374Q', 'I374P', 'I374N', 'I374M', 'I374L', 'I374K', 'I374S', 'I374H', 'I374F', 'I374E', 'I374D', 'I374C', 'I374A', 'I374T', 'I374G', 'I374V', 'I374Y', 'Y375K', 'Y375D', 'Y375E', 'Y375F', 'Y375G', 'Y375H', 'Y375I', 'Y375L', 'Y375C', 'Y375M', 'Y375P', 'Y375Q', 'Y375R', 'Y375S', 'Y375T', 'Y375V', 'Y375W', 'Y375N', 'Y375A', 'R376A', 'R376W', 'R376Y', 'R376D', 'R376E', 'R376F', 'R376G', 'R376H', 'R376I', 'R376K', 'R376C', 'R376M', 'R376N', 'R376P', 'R376Q', 'R376S', 'R376T', 'R376V', 'R376L', 'T377W', 'T377V', 'T377S', 'T377R', 'T377Q', 'T377P', 'T377N', 'T377M', 'T377H', 'T377K', 'T377I', 'T377G', 'T377F', 'T377E', 'T377D', 'T377Y', 'T377L', 'T377C', 'T377A', 'Q378E', 'Q378D', 'Q378A', 'Q378F', 'Q378G', 'Q378H', 'Q378I', 'Q378K', 'Q378L', 'Q378M', 'Q378C', 'Q378P', 'Q378R', 'Q378S', 'Q378T', 'Q378V', 'Q378W', 'Q378Y', 'Q378N', 'E379W', 'E379V', 'E379T', 'E379S', 'E379R', 'E379Q', 'E379P', 'E379K', 'E379M', 'E379L', 'E379I', 'E379H', 'E379G', 'E379F', 'E379D', 'E379N', 'E379Y', 'E379A', 'E379C', 'W380R', 'W380Y', 'W380A', 'W380C', 'W380D', 'W380E', 'W380F', 'W380G', 'W380H', 'W380S', 'W380K', 'W380L', 'W380M', 'W380N', 'W380P', 'W380Q', 'W380V', 'W380T', 'W380I', 'D381Y', 'D381W', 'D381V', 'D381T', 'D381S', 'D381R', 'D381Q', 'D381L', 'D381N', 'D381M', 'D381K', 'D381I', 'D381H', 'D381G', 'D381P', 'D381E', 'D381C', 'D381A', 'D381F', 'E382P', 'E382H', 'E382I', 'E382K', 'E382L', 'E382M', 'E382N', 'E382A', 'E382Q', 'E382R', 'E382S', 'E382T', 'E382V', 'E382W', 'E382Y', 'E382D', 'E382C', 'E382G', 'E382F', 'F383V', 'F383A', 'F383Y', 'F383T', 'F383L', 'F383G', 'F383C', 'F383D', 'F383E', 'F383H', 'F383I', 'F383M', 'F383K', 'F383P', 'F383Q', 'F383R', 'F383S', 'F383W', 'F383N', 'P384S', 'P384Q', 'P384R', 'P384T', 'P384L', 'P384W', 'P384Y', 'P384N', 'P384V', 'P384M', 'P384K', 'P384I', 'P384H', 'P384G', 'P384F', 'P384E', 'P384D', 'P384C', 'P384A', 'N385I', 'N385K', 'N385Y', 'N385W', 'N385V', 'N385C', 'N385S', 'N385E', 'N385T', 'N385D', 'N385L', 'N385M', 'N385P', 'N385A', 'N385F', 'N385G', 'N385H', 'N385Q', 'N385R', 'G386Y', 'G386N', 'G386V', 'G386T', 'G386S', 'G386R', 'G386P', 'G386W', 'G386M', 'G386Q', 'G386K', 'G386L', 'G386A', 'G386C', 'G386H', 'G386E', 'G386F', 'G386I', 'G386D', 'R387S', 'R387T', 'R387V', 'R387A', 'R387C', 'R387D', 'R387E', 'R387F', 'R387H', 'R387I', 'R387K', 'R387M', 'R387N', 'R387P', 'R387Q', 'R387W', 'R387Y', 'R387G', 'R387L', 'W388V', 'W388Q', 'W388T', 'W388S', 'W388R', 'W388P', 'W388M', 'W388L', 'W388K', 'W388I', 'W388N', 'W388G', 'W388F', 'W388E', 'W388D', 'W388C', 'W388A', 'W388Y', 'W388H', 'G389K', 'G389I', 'G389H', 'G389A', 'G389D', 'G389C', 'G389L', 'G389E', 'G389M', 'G389F', 'G389P', 'G389Y', 'G389W', 'G389V', 'G389T', 'G389N', 'G389S', 'G389R', 'G389Q', 'N390P', 'N390Y', 'N390W', 'N390V', 'N390T', 'N390S', 'N390R', 'N390Q', 'N390A', 'N390C', 'N390M', 'N390K', 'N390I', 'N390H', 'N390G', 'N390F', 'N390E', 'N390D', 'N390L', 'S391P', 'S391Q', 'S391R', 'S391N', 'S391V', 'S391W', 'S391Y', 'S391T', 'S391M', 'S391L', 'S391I', 'S391H', 'S391G', 'S391F', 'S391E', 'S391D', 'S391C', 'S391A', 'S391K', 'S392H', 'S392G', 'S392F', 'S392I', 'S392D', 'S392C', 'S392A', 'S392E', 'S392K', 'S392L', 'S392M', 'S392W', 'S392V', 'S392T', 'S392Y', 'S392Q', 'S392P', 'S392N', 'S392R', 'S393M', 'S393Y', 'S393W', 'S393V', 'S393T', 'S393R', 'S393Q', 'S393P', 'S393L', 'S393N', 'S393K', 'S393I', 'S393H', 'S393G', 'S393F', 'S393E', 'S393D', 'S393C', 'S393A', 'P394D', 'P394E', 'P394F', 'P394G', 'P394H', 'P394I', 'P394Q', 'P394L', 'P394M', 'P394N', 'P394S', 'P394R', 'P394C', 'P394K', 'P394A', 'P394V', 'P394W', 'P394Y', 'P394T', 'A395T', 'A395Y', 'A395W', 'A395V', 'A395S', 'A395R', 'A395Q', 'A395P', 'A395M', 'A395N', 'A395K', 'A395I', 'A395H', 'A395G', 'A395F', 'A395E', 'A395D', 'A395C', 'A395L', 'F396R', 'F396W', 'F396V', 'F396Y', 'F396T', 'F396S', 'F396Q', 'F396G', 'F396N', 'F396M', 'F396L', 'F396I', 'F396H', 'F396C', 'F396D', 'F396E', 'F396P', 'F396K', 'F396A', 'G397W', 'G397C', 'G397D', 'G397E', 'G397F', 'G397H', 'G397I', 'G397K', 'G397A', 'G397L', 'G397N', 'G397P', 'G397Q', 'G397R', 'G397S', 'G397T', 'G397V', 'G397M', 'G397Y', 'E398A', 'E398V', 'E398C', 'E398D', 'E398F', 'E398G', 'E398H', 'E398I', 'E398K', 'E398L', 'E398M', 'E398N', 'E398P', 'E398Q', 'E398R', 'E398S', 'E398T', 'E398W', 'E398Y', 'L399Y', 'L399M', 'L399C', 'L399D', 'L399E', 'L399F', 'L399G', 'L399H', 'L399A', 'L399I', 'L399N', 'L399P', 'L399Q', 'L399R', 'L399S', 'L399T', 'L399K', 'L399V', 'L399W', 'K400D', 'K400V', 'K400T', 'K400S', 'K400R', 'K400Q', 'K400P', 'K400W', 'K400N', 'K400M', 'K400L', 'K400I', 'K400H', 'K400G', 'K400F', 'K400E', 'K400C', 'K400A', 'K400Y', 'D401A', 'D401Y', 'D401W', 'D401V', 'D401T', 'D401R', 'D401Q', 'D401P', 'D401N', 'D401S', 'D401L', 'D401M', 'D401E', 'D401F', 'D401G', 'D401C', 'D401I', 'D401K', 'D401H', 'Y402K', 'Y402C', 'Y402D', 'Y402E', 'Y402F', 'Y402G', 'Y402H', 'Y402I', 'Y402A', 'Y402L', 'Y402N', 'Y402P', 'Y402Q', 'Y402R', 'Y402S', 'Y402T', 'Y402V', 'Y402W', 'Y402M', 'Y403N', 'Y403P', 'Y403Q', 'Y403V', 'Y403S', 'Y403T', 'Y403M', 'Y403R', 'Y403L', 'Y403G', 'Y403I', 'Y403H', 'Y403F', 'Y403E', 'Y403D', 'Y403C', 'Y403A', 'Y403K', 'Y403W', 'L404A', 'L404P', 'L404D', 'L404C', 'L404Y', 'L404W', 'L404V', 'L404T', 'L404R', 'L404Q', 'L404S', 'L404M', 'L404K', 'L404I', 'L404H', 'L404G', 'L404F', 'L404E', 'L404N', 'F405E', 'F405M', 'F405G', 'F405H', 'F405I', 'F405K', 'F405L', 'F405N', 'F405Y', 'F405Q', 'F405R', 'F405S', 'F405T', 'F405V', 'F405W', 'F405D', 'F405P', 'F405C', 'F405A', 'Y406C', 'Y406E', 'Y406F', 'Y406G', 'Y406H', 'Y406I', 'Y406K', 'Y406L', 'Y406M', 'Y406A', 'Y406N', 'Y406P', 'Y406Q', 'Y406R', 'Y406S', 'Y406T', 'Y406V', 'Y406W', 'Y406D', 'L407Y', 'L407A', 'L407E', 'L407W', 'L407V', 'L407T', 'L407S', 'L407R', 'L407Q', 'L407D', 'L407P', 'L407M', 'L407K', 'L407I', 'L407H', 'L407G', 'L407F', 'L407N', 'L407C', 'K408Y', 'K408V', 'K408S', 'K408R', 'K408Q', 'K408P', 'K408N', 'K408M', 'K408L', 'K408I', 'K408H', 'K408G', 'K408F', 'K408E', 'K408D', 'K408C', 'K408A', 'K408T', 'K408W', 'S409W', 'S409C', 'S409V', 'S409T', 'S409R', 'S409Q', 'S409P', 'S409N', 'S409M', 'S409L', 'S409K', 'S409I', 'S409H', 'S409G', 'S409F', 'S409E', 'S409D', 'S409Y', 'S409A', 'K410Y', 'K410A', 'K410E', 'K410F', 'K410G', 'K410H', 'K410I', 'K410L', 'K410D', 'K410N', 'K410Q', 'K410R', 'K410S', 'K410T', 'K410V', 'K410W', 'K410P', 'K410C', 'K410M', 'S411A', 'S411D', 'S411E', 'S411F', 'S411G', 'S411H', 'S411I', 'S411K', 'S411Y', 'S411L', 'S411N', 'S411P', 'S411Q', 'S411R', 'S411T', 'S411V', 'S411W', 'S411M', 'S411C', 'P412A', 'P412W', 'P412V', 'P412T', 'P412S', 'P412R', 'P412Q', 'P412N', 'P412M', 'P412L', 'P412Y', 'P412K', 'P412I', 'P412H', 'P412G', 'P412F', 'P412E', 'P412D', 'P412C', 'K413W', 'K413C', 'K413D', 'K413E', 'K413F', 'K413G', 'K413H', 'K413N', 'K413L', 'K413M', 'K413P', 'K413Q', 'K413R', 'K413S', 'K413Y', 'K413I', 'K413A', 'K413V', 'K413T', 'E414C', 'E414V', 'E414T', 'E414S', 'E414R', 'E414Q', 'E414P', 'E414N', 'E414M', 'E414K', 'E414I', 'E414H', 'E414G', 'E414F', 'E414D', 'E414A', 'E414L', 'E414Y', 'E414W', 'E415A', 'E415Y', 'E415W', 'E415V', 'E415T', 'E415S', 'E415R', 'E415Q', 'E415P', 'E415N', 'E415L', 'E415K', 'E415I', 'E415H', 'E415G', 'E415F', 'E415D', 'E415C', 'E415M', 'L416D', 'L416A', 'L416R', 'L416W', 'L416V', 'L416T', 'L416S', 'L416Y', 'L416Q', 'L416P', 'L416N', 'L416M', 'L416K', 'L416I', 'L416H', 'L416G', 'L416F', 'L416E', 'L416C', 'L417Q', 'L417P', 'L417R', 'L417N', 'L417T', 'L417V', 'L417S', 'L417M', 'L417A', 'L417I', 'L417H', 'L417G', 'L417F', 'L417E', 'L417D', 'L417C', 'L417W', 'L417K', 'L417Y', 'K418W', 'K418Y', 'K418V', 'K418T', 'K418S', 'K418R', 'K418Q', 'K418P', 'K418N', 'K418L', 'K418I', 'K418H', 'K418G', 'K418F', 'K418E', 'K418D', 'K418C', 'K418A', 'K418M', 'M419C', 'M419A', 'M419Y', 'M419W', 'M419V', 'M419T', 'M419S', 'M419Q', 'M419P', 'M419N', 'M419R', 'M419K', 'M419I', 'M419H', 'M419G', 'M419F', 'M419E', 'M419D', 'M419L', 'W420P', 'W420Y', 'W420V', 'W420T', 'W420S', 'W420R', 'W420Q', 'W420N', 'W420M', 'W420C', 'W420K', 'W420I', 'W420H', 'W420G', 'W420F', 'W420E', 'W420D', 'W420A', 'W420L', 'G421Q', 'G421R', 'G421Y', 'G421T', 'G421V', 'G421P', 'G421S', 'G421N', 'G421E', 'G421K', 'G421I', 'G421H', 'G421F', 'G421D', 'G421C', 'G421A', 'G421L', 'G421W', 'G421M', 'E422C', 'E422F', 'E422G', 'E422H', 'E422I', 'E422K', 'E422L', 'E422M', 'E422D', 'E422N', 'E422Q', 'E422R', 'E422S', 'E422T', 'E422V', 'E422W', 'E422Y', 'E422P', 'E422A', 'E423C', 'E423F', 'E423Y', 'E423W', 'E423V', 'E423T', 'E423S', 'E423R', 'E423A', 'E423Q', 'E423N', 'E423M', 'E423L', 'E423K', 'E423I', 'E423H', 'E423G', 'E423P', 'E423D', 'L424W', 'L424Y', 'L424A', 'L424V', 'L424T', 'L424S', 'L424R', 'L424P', 'L424N', 'L424M', 'L424Q', 'L424I', 'L424H', 'L424G', 'L424F', 'L424E', 'L424D', 'L424C', 'L424K', 'T425K', 'T425I', 'T425H', 'T425A', 'T425F', 'T425E', 'T425L', 'T425G', 'T425M', 'T425Y', 'T425P', 'T425Q', 'T425R', 'T425S', 'T425V', 'T425W', 'T425C', 'T425N', 'T425D', 'S426C', 'S426A', 'S426Q', 'S426W', 'S426V', 'S426T', 'S426R', 'S426P', 'S426N', 'S426Y', 'S426M', 'S426K', 'S426I', 'S426H', 'S426G', 'S426F', 'S426E', 'S426D', 'S426L', 'E427H', 'E427G', 'E427F', 'E427Y', 'E427C', 'E427A', 'E427I', 'E427D', 'E427K', 'E427V', 'E427M', 'E427N', 'E427P', 'E427Q', 'E427R', 'E427S', 'E427T', 'E427W', 'E427L', 'E428A', 'E428C', 'E428R', 'E428W', 'E428Y', 'E428V', 'E428T', 'E428S', 'E428Q', 'E428P', 'E428N', 'E428L', 'E428K', 'E428I', 'E428H', 'E428G', 'E428F', 'E428D', 'E428M', 'S429F', 'S429G', 'S429E', 'S429W', 'S429C', 'S429A', 'S429H', 'S429D', 'S429I', 'S429P', 'S429L', 'S429K', 'S429V', 'S429T', 'S429R', 'S429Y', 'S429N', 'S429M', 'S429Q', 'V430N', 'V430Y', 'V430W', 'V430T', 'V430S', 'V430R', 'V430Q', 'V430P', 'V430M', 'V430C', 'V430K', 'V430L', 'V430D', 'V430E', 'V430A', 'V430G', 'V430H', 'V430I', 'V430F', 'F431T', 'F431M', 'F431S', 'F431R', 'F431Q', 'F431P', 'F431N', 'F431L', 'F431G', 'F431I', 'F431H', 'F431E', 'F431D', 'F431C', 'F431Y', 'F431W', 'F431K', 'F431V', 'F431A', 'E432A', 'E432Y', 'E432W', 'E432V', 'E432T', 'E432S', 'E432R', 'E432Q', 'E432P', 'E432C', 'E432M', 'E432D', 'E432F', 'E432G', 'E432N', 'E432I', 'E432K', 'E432L', 'E432H', 'V433I', 'V433C', 'V433D', 'V433E', 'V433F', 'V433G', 'V433H', 'V433A', 'V433K', 'V433M', 'V433L', 'V433Y', 'V433W', 'V433T', 'V433Q', 'V433R', 'V433P', 'V433N', 'V433S', 'F434L', 'F434C', 'F434D', 'F434E', 'F434G', 'F434H', 'F434I', 'F434K', 'F434A', 'F434M', 'F434P', 'F434Q', 'F434R', 'F434S', 'F434T', 'F434V', 'F434W', 'F434Y', 'F434N', 'V435D', 'V435Y', 'V435W', 'V435T', 'V435S', 'V435R', 'V435Q', 'V435L', 'V435N', 'V435M', 'V435C', 'V435K', 'V435I', 'V435H', 'V435G', 'V435P', 'V435A', 'V435E', 'V435F', 'L436W', 'L436C', 'L436D', 'L436E', 'L436F', 'L436G', 'L436H', 'L436Y', 'L436I', 'L436M', 'L436N', 'L436P', 'L436Q', 'L436R', 'L436T', 'L436V', 'L436K', 'L436S', 'L436A', 'Y437W', 'Y437D', 'Y437E', 'Y437F', 'Y437G', 'Y437H', 'Y437I', 'Y437K', 'Y437L', 'Y437M', 'Y437N', 'Y437P', 'Y437Q', 'Y437R', 'Y437S', 'Y437T', 'Y437C', 'Y437V', 'Y437A', 'L438Y', 'L438E', 'L438F', 'L438G', 'L438H', 'L438I', 'L438K', 'L438M', 'L438N', 'L438P', 'L438Q', 'L438R', 'L438S', 'L438T', 'L438V', 'L438W', 'L438A', 'L438D', 'L438C', 'S439Y', 'S439T', 'S439R', 'S439Q', 'S439P', 'S439N', 'S439M', 'S439L', 'S439V', 'S439K', 'S439H', 'S439G', 'S439F', 'S439E', 'S439D', 'S439C', 'S439A', 'S439I', 'S439W', 'G440C', 'G440Y', 'G440T', 'G440S', 'G440R', 'G440Q', 'G440P', 'G440N', 'G440V', 'G440A', 'G440L', 'G440K', 'G440I', 'G440H', 'G440F', 'G440E', 'G440M', 'G440W', 'G440D', 'E441W', 'E441T', 'E441S', 'E441R', 'E441Q', 'E441P', 'E441N', 'E441M', 'E441Y', 'E441L', 'E441I', 'E441H', 'E441G', 'E441F', 'E441D', 'E441C', 'E441A', 'E441K', 'E441V', 'P442I', 'P442C', 'P442W', 'P442V', 'P442T', 'P442S', 'P442R', 'P442Q', 'P442A', 'P442N', 'P442L', 'P442H', 'P442G', 'P442F', 'P442E', 'P442D', 'P442M', 'P442Y', 'P442K', 'N443M', 'N443Y', 'N443C', 'N443V', 'N443T', 'N443S', 'N443R', 'N443W', 'N443Q', 'N443D', 'N443L', 'N443K', 'N443I', 'N443H', 'N443G', 'N443P', 'N443F', 'N443A', 'N443E', 'R444D', 'R444E', 'R444F', 'R444G', 'R444H', 'R444I', 'R444W', 'R444V', 'R444M', 'R444N', 'R444P', 'R444Q', 'R444S', 'R444T', 'R444L', 'R444K', 'R444A', 'R444Y', 'R444C', 'N445Q', 'N445P', 'N445M', 'N445L', 'N445K', 'N445I', 'N445H', 'N445G', 'N445F', 'N445D', 'N445C', 'N445A', 'N445T', 'N445V', 'N445W', 'N445Y', 'N445E', 'N445R', 'N445S', 'G446L', 'G446H', 'G446I', 'G446K', 'G446D', 'G446M', 'G446N', 'G446P', 'G446Q', 'G446R', 'G446S', 'G446T', 'G446V', 'G446W', 'G446Y', 'G446C', 'G446A', 'G446E', 'G446F', 'H447W', 'H447V', 'H447T', 'H447S', 'H447R', 'H447Q', 'H447D', 'H447L', 'H447K', 'H447I', 'H447G', 'H447F', 'H447E', 'H447N', 'H447P', 'H447M', 'H447C', 'H447Y', 'H447A', 'K448F', 'K448Y', 'K448C', 'K448A', 'K448W', 'K448V', 'K448T', 'K448S', 'K448R', 'K448Q', 'K448E', 'K448N', 'K448M', 'K448L', 'K448I', 'K448H', 'K448G', 'K448D', 'K448P', 'V449F', 'V449P', 'V449T', 'V449D', 'V449C', 'V449A', 'V449S', 'V449R', 'V449Q', 'V449W', 'V449Y', 'V449M', 'V449E', 'V449L', 'V449K', 'V449I', 'V449H', 'V449G', 'V449N', 'T450E', 'T450W', 'T450D', 'T450S', 'T450R', 'T450Q', 'T450P', 'T450N', 'T450M', 'T450K', 'T450I', 'T450H', 'T450G', 'T450F', 'T450V', 'T450L', 'T450Y', 'T450C', 'T450A', 'C451F', 'C451G', 'C451H', 'C451I', 'C451K', 'C451L', 'C451M', 'C451N', 'C451P', 'C451Q', 'C451R', 'C451S', 'C451T', 'C451V', 'C451W', 'C451D', 'C451A', 'C451Y', 'C451E', 'L452A', 'L452C', 'L452E', 'L452F', 'L452G', 'L452H', 'L452I', 'L452K', 'L452M', 'L452D', 'L452N', 'L452P', 'L452Q', 'L452R', 'L452S', 'L452T', 'L452Y', 'L452W', 'L452V', 'P453C', 'P453A', 'P453E', 'P453Y', 'P453W', 'P453V', 'P453T', 'P453S', 'P453R', 'P453Q', 'P453N', 'P453M', 'P453L', 'P453K', 'P453I', 'P453H', 'P453G', 'P453F', 'P453D', 'W454Y', 'W454C', 'W454T', 'W454D', 'W454E', 'W454F', 'W454G', 'W454H', 'W454I', 'W454V', 'W454K', 'W454A', 'W454N', 'W454P', 'W454Q', 'W454R', 'W454S', 'W454L', 'W454M', 'N455Y', 'N455E', 'N455C', 'N455V', 'N455T', 'N455S', 'N455R', 'N455Q', 'N455D', 'N455P', 'N455L', 'N455K', 'N455I', 'N455H', 'N455G', 'N455F', 'N455W', 'N455M', 'N455A', 'D456N', 'D456Y', 'D456W', 'D456V', 'D456S', 'D456R', 'D456Q', 'D456P', 'D456M', 'D456T', 'D456K', 'D456I', 'D456H', 'D456G', 'D456F', 'D456E', 'D456C', 'D456A', 'D456L', 'E457D', 'E457F', 'E457G', 'E457H', 'E457I', 'E457K', 'E457P', 'E457M', 'E457N', 'E457T', 'E457S', 'E457C', 'E457Q', 'E457L', 'E457A', 'E457R', 'E457W', 'E457Y', 'E457V', 'P458T', 'P458A', 'P458C', 'P458D', 'P458E', 'P458F', 'P458G', 'P458V', 'P458H', 'P458W', 'P458M', 'P458N', 'P458Q', 'P458R', 'P458S', 'P458I', 'P458K', 'P458L', 'P458Y', 'L459W', 'L459V', 'L459T', 'L459S', 'L459R', 'L459Q', 'L459P', 'L459N', 'L459Y', 'L459A', 'L459K', 'L459I', 'L459H', 'L459G', 'L459F', 'L459E', 'L459D', 'L459C', 'L459M', 'A460K', 'A460I', 'A460N', 'A460E', 'A460D', 'A460V', 'A460T', 'A460S', 'A460R', 'A460Q', 'A460P', 'A460Y', 'A460M', 'A460C', 'A460L', 'A460F', 'A460G', 'A460H', 'A460W', 'A461D', 'A461Y', 'A461W', 'A461N', 'A461E', 'A461Q', 'A461C', 'A461R', 'A461T', 'A461V', 'A461P', 'A461S', 'A461L', 'A461K', 'A461I', 'A461H', 'A461G', 'A461F', 'A461M', 'E462Q', 'E462R', 'E462S', 'E462A', 'E462C', 'E462D', 'E462F', 'E462H', 'E462G', 'E462K', 'E462L', 'E462M', 'E462N', 'E462P', 'E462Y', 'E462W', 'E462V', 'E462I', 'E462T', 'T463I', 'T463H', 'T463D', 'T463E', 'T463K', 'T463G', 'T463L', 'T463F', 'T463N', 'T463Y', 'T463M', 'T463W', 'T463C', 'T463V', 'T463S', 'T463R', 'T463P', 'T463Q', 'T463A', 'S464A', 'S464C', 'S464D', 'S464E', 'S464F', 'S464G', 'S464H', 'S464I', 'S464L', 'S464M', 'S464N', 'S464Q', 'S464R', 'S464T', 'S464V', 'S464Y', 'S464W', 'S464K', 'S464P', 'L465A', 'L465F', 'L465G', 'L465E', 'L465D', 'L465C', 'L465H', 'L465K', 'L465M', 'L465N', 'L465I', 'L465Q', 'L465R', 'L465Y', 'L465W', 'L465S', 'L465V', 'L465T', 'L465P', 'L466V', 'L466W', 'L466D', 'L466E', 'L466F', 'L466I', 'L466H', 'L466K', 'L466M', 'L466N', 'L466P', 'L466R', 'L466S', 'L466T', 'L466G', 'L466Q', 'L466Y', 'L466A', 'L466C', 'K467S', 'K467Y', 'K467W', 'K467V', 'K467T', 'K467R', 'K467Q', 'K467P', 'K467N', 'K467M', 'K467L', 'K467A', 'K467H', 'K467G', 'K467F', 'K467E', 'K467D', 'K467C', 'K467I', 'E468D', 'E468F', 'E468G', 'E468Y', 'E468W', 'E468V', 'E468T', 'E468S', 'E468N', 'E468Q', 'E468P', 'E468M', 'E468L', 'E468K', 'E468H', 'E468I', 'E468A', 'E468R', 'E468C', 'E469I', 'E469A', 'E469V', 'E469T', 'E469S', 'E469Q', 'E469P', 'E469N', 'E469W', 'E469M', 'E469K', 'E469H', 'E469G', 'E469F', 'E469D', 'E469C', 'E469L', 'E469Y', 'E469R', 'L470E', 'L470A', 'L470C', 'L470Y', 'L470W', 'L470D', 'L470T', 'L470S', 'L470R', 'L470Q', 'L470V', 'L470N', 'L470F', 'L470P', 'L470H', 'L470G', 'L470K', 'L470M', 'L470I', 'L471P', 'L471C', 'L471Y', 'L471W', 'L471V', 'L471T', 'L471S', 'L471R', 'L471Q', 'L471N', 'L471A', 'L471K', 'L471I', 'L471H', 'L471G', 'L471F', 'L471E', 'L471D', 'L471M', 'R472Y', 'R472W', 'R472V', 'R472T', 'R472Q', 'R472P', 'R472N', 'R472M', 'R472S', 'R472K', 'R472L', 'R472A', 'R472D', 'R472E', 'R472C', 'R472G', 'R472H', 'R472I', 'R472F', 'V473T', 'V473S', 'V473R', 'V473Q', 'V473P', 'V473N', 'V473M', 'V473H', 'V473K', 'V473I', 'V473G', 'V473F', 'V473E', 'V473D', 'V473W', 'V473L', 'V473Y', 'V473C', 'V473A', 'N474A', 'N474Y', 'N474W', 'N474V', 'N474T', 'N474S', 'N474Q', 'N474P', 'N474M', 'N474L', 'N474K', 'N474I', 'N474H', 'N474G', 'N474F', 'N474E', 'N474D', 'N474C', 'N474R', 'R475M', 'R475Y', 'R475C', 'R475D', 'R475E', 'R475F', 'R475G', 'R475H', 'R475I', 'R475K', 'R475L', 'R475N', 'R475P', 'R475Q', 'R475S', 'R475T', 'R475V', 'R475A', 'R475W', 'Q476Y', 'Q476L', 'Q476D', 'Q476E', 'Q476F', 'Q476G', 'Q476H', 'Q476I', 'Q476K', 'Q476C', 'Q476A', 'Q476N', 'Q476P', 'Q476R', 'Q476S', 'Q476T', 'Q476V', 'Q476W', 'Q476M', 'G477A', 'G477W', 'G477T', 'G477R', 'G477Q', 'G477P', 'G477N', 'G477M', 'G477L', 'G477K', 'G477S', 'G477Y', 'G477I', 'G477H', 'G477F', 'G477E', 'G477D', 'G477C', 'G477V', 'I478W', 'I478C', 'I478V', 'I478T', 'I478S', 'I478R', 'I478Q', 'I478P', 'I478N', 'I478M', 'I478K', 'I478H', 'I478G', 'I478F', 'I478E', 'I478D', 'I478A', 'I478L', 'I478Y', 'L479D', 'L479A', 'L479F', 'L479G', 'L479H', 'L479I', 'L479K', 'L479M', 'L479C', 'L479N', 'L479Q', 'L479R', 'L479S', 'L479T', 'L479V', 'L479W', 'L479Y', 'L479P', 'L479E', 'T480S', 'T480A', 'T480R', 'T480Q', 'T480P', 'T480N', 'T480M', 'T480L', 'T480Y', 'T480K', 'T480H', 'T480G', 'T480F', 'T480E', 'T480D', 'T480C', 'T480I', 'T480W', 'T480V', 'I481V', 'I481R', 'I481Q', 'I481P', 'I481N', 'I481M', 'I481L', 'I481K', 'I481S', 'I481H', 'I481F', 'I481E', 'I481D', 'I481C', 'I481A', 'I481Y', 'I481W', 'I481G', 'I481T', 'N482E', 'N482Y', 'N482F', 'N482G', 'N482H', 'N482I', 'N482K', 'N482L', 'N482D', 'N482M', 'N482Q', 'N482R', 'N482S', 'N482T', 'N482V', 'N482W', 'N482P', 'N482C', 'N482A', 'S483M', 'S483C', 'S483W', 'S483V', 'S483T', 'S483R', 'S483Q', 'S483P', 'S483N', 'S483A', 'S483L', 'S483K', 'S483I', 'S483H', 'S483G', 'S483F', 'S483E', 'S483D', 'S483Y', 'Q484W', 'Q484Y', 'Q484F', 'Q484D', 'Q484E', 'Q484G', 'Q484H', 'Q484I', 'Q484K', 'Q484A', 'Q484L', 'Q484N', 'Q484P', 'Q484R', 'Q484S', 'Q484T', 'Q484V', 'Q484M', 'Q484C', 'P485W', 'P485V', 'P485C', 'P485T', 'P485S', 'P485R', 'P485A', 'P485N', 'P485M', 'P485L', 'P485Q', 'P485I', 'P485K', 'P485Y', 'P485D', 'P485G', 'P485F', 'P485H', 'P485E', 'N486K', 'N486A', 'N486D', 'N486E', 'N486F', 'N486G', 'N486H', 'N486I', 'N486C', 'N486L', 'N486P', 'N486Q', 'N486R', 'N486S', 'N486T', 'N486V', 'N486W', 'N486Y', 'N486M', 'I487D', 'I487E', 'I487F', 'I487H', 'I487K', 'I487L', 'I487M', 'I487R', 'I487P', 'I487Q', 'I487C', 'I487S', 'I487T', 'I487V', 'I487W', 'I487N', 'I487A', 'I487G', 'I487Y', 'N488A', 'N488C', 'N488Y', 'N488D', 'N488E', 'N488F', 'N488G', 'N488H', 'N488I', 'N488W', 'N488L', 'N488M', 'N488P', 'N488Q', 'N488R', 'N488V', 'N488T', 'N488S', 'N488K', 'G489A', 'G489C', 'G489D', 'G489E', 'G489F', 'G489H', 'G489I', 'G489N', 'G489L', 'G489M', 'G489P', 'G489Q', 'G489R', 'G489S', 'G489V', 'G489W', 'G489Y', 'G489K', 'G489T', 'K490A', 'K490N', 'K490V', 'K490T', 'K490S', 'K490R', 'K490Q', 'K490P', 'K490M', 'K490L', 'K490W', 'K490I', 'K490G', 'K490F', 'K490E', 'K490D', 'K490C', 'K490H', 'K490Y', 'P491T', 'P491S', 'P491R', 'P491Q', 'P491Y', 'P491V', 'P491W', 'P491A', 'P491N', 'P491M', 'P491L', 'P491K', 'P491I', 'P491H', 'P491G', 'P491F', 'P491E', 'P491D', 'P491C', 'S492V', 'S492T', 'S492R', 'S492Q', 'S492P', 'S492N', 'S492L', 'S492G', 'S492I', 'S492H', 'S492W', 'S492F', 'S492E', 'S492D', 'S492C', 'S492A', 'S492K', 'S492Y', 'S492M', 'S493C', 'S493A', 'S493W', 'S493V', 'S493T', 'S493R', 'S493Q', 'S493P', 'S493N', 'S493M', 'S493Y', 'S493K', 'S493I', 'S493H', 'S493G', 'S493F', 'S493E', 'S493D', 'S493L', 'D494F', 'D494A', 'D494C', 'D494G', 'D494H', 'D494I', 'D494K', 'D494L', 'D494Q', 'D494N', 'D494P', 'D494R', 'D494S', 'D494T', 'D494V', 'D494Y', 'D494W', 'D494M', 'D494E', 'P495A', 'P495Y', 'P495C', 'P495E', 'P495F', 'P495G', 'P495H', 'P495I', 'P495K', 'P495D', 'P495L', 'P495N', 'P495Q', 'P495R', 'P495S', 'P495T', 'P495V', 'P495M', 'P495W', 'I496D', 'I496C', 'I496F', 'I496G', 'I496H', 'I496K', 'I496L', 'I496M', 'I496A', 'I496N', 'I496Q', 'I496R', 'I496S', 'I496T', 'I496V', 'I496W', 'I496P', 'I496E', 'I496Y', 'V497F', 'V497E', 'V497A', 'V497C', 'V497D', 'V497Y', 'V497W', 'V497S', 'V497R', 'V497T', 'V497P', 'V497N', 'V497M', 'V497L', 'V497K', 'V497I', 'V497H', 'V497G', 'V497Q', 'G498M', 'G498H', 'G498I', 'G498K', 'G498L', 'G498N', 'G498W', 'G498Q', 'G498R', 'G498S', 'G498T', 'G498V', 'G498F', 'G498Y', 'G498P', 'G498E', 'G498A', 'G498C', 'G498D', 'W499Y', 'W499V', 'W499A', 'W499C', 'W499D', 'W499E', 'W499F', 'W499G', 'W499H', 'W499K', 'W499L', 'W499M', 'W499P', 'W499Q', 'W499R', 'W499S', 'W499T', 'W499I', 'W499N', 'G500A', 'G500C', 'G500W', 'G500V', 'G500T', 'G500S', 'G500R', 'G500Q', 'G500P', 'G500Y', 'G500N', 'G500L', 'G500K', 'G500I', 'G500H', 'G500F', 'G500E', 'G500D', 'G500M', 'P501A', 'P501C', 'P501Q', 'P501Y', 'P501W', 'P501V', 'P501T', 'P501S', 'P501R', 'P501N', 'P501M', 'P501L', 'P501K', 'P501I', 'P501G', 'P501F', 'P501E', 'P501D', 'P501H', 'S502W', 'S502A', 'S502C', 'S502D', 'S502E', 'S502F', 'S502G', 'S502Y', 'S502I', 'S502K', 'S502H', 'S502L', 'S502M', 'S502N', 'S502P', 'S502Q', 'S502R', 'S502T', 'S502V', 'G503A', 'G503F', 'G503H', 'G503I', 'G503K', 'G503L', 'G503E', 'G503N', 'G503M', 'G503Q', 'G503R', 'G503S', 'G503T', 'G503V', 'G503W', 'G503Y', 'G503D', 'G503P', 'G503C', 'G504A', 'G504Y', 'G504W', 'G504V', 'G504T', 'G504S', 'G504Q', 'G504P', 'G504N', 'G504M', 'G504R', 'G504K', 'G504I', 'G504H', 'G504F', 'G504E', 'G504D', 'G504C', 'G504L', 'Y505K', 'Y505I', 'Y505H', 'Y505D', 'Y505F', 'Y505E', 'Y505L', 'Y505G', 'Y505M', 'Y505C', 'Y505P', 'Y505Q', 'Y505R', 'Y505S', 'Y505T', 'Y505V', 'Y505W', 'Y505A', 'Y505N', 'V506D', 'V506E', 'V506N', 'V506T', 'V506S', 'V506R', 'V506Q', 'V506P', 'V506M', 'V506L', 'V506K', 'V506I', 'V506H', 'V506G', 'V506F', 'V506C', 'V506A', 'V506W', 'V506Y', 'F507H', 'F507Y', 'F507W', 'F507D', 'F507V', 'F507T', 'F507S', 'F507R', 'F507Q', 'F507P', 'F507N', 'F507M', 'F507C', 'F507A', 'F507I', 'F507L', 'F507G', 'F507E', 'F507K', 'Q508Y', 'Q508S', 'Q508R', 'Q508P', 'Q508N', 'Q508M', 'Q508L', 'Q508K', 'Q508I', 'Q508H', 'Q508F', 'Q508E', 'Q508D', 'Q508C', 'Q508A', 'Q508W', 'Q508G', 'Q508V', 'Q508T', 'K509C', 'K509A', 'K509D', 'K509E', 'K509F', 'K509G', 'K509H', 'K509L', 'K509M', 'K509I', 'K509P', 'K509Q', 'K509R', 'K509S', 'K509T', 'K509V', 'K509W', 'K509Y', 'K509N', 'A510E', 'A510D', 'A510C', 'A510H', 'A510I', 'A510K', 'A510L', 'A510R', 'A510N', 'A510P', 'A510Q', 'A510S', 'A510T', 'A510V', 'A510W', 'A510M', 'A510Y', 'A510G', 'A510F', 'Y511W', 'Y511V', 'Y511A', 'Y511C', 'Y511D', 'Y511E', 'Y511F', 'Y511H', 'Y511I', 'Y511K', 'Y511G', 'Y511M', 'Y511N', 'Y511P', 'Y511Q', 'Y511R', 'Y511S', 'Y511T', 'Y511L', 'L512Q', 'L512R', 'L512S', 'L512W', 'L512V', 'L512Y', 'L512P', 'L512T', 'L512N', 'L512A', 'L512K', 'L512I', 'L512H', 'L512G', 'L512F', 'L512E', 'L512D', 'L512C', 'L512M', 'E513V', 'E513W', 'E513Y', 'E513I', 'E513F', 'E513H', 'E513D', 'E513K', 'E513L', 'E513M', 'E513N', 'E513P', 'E513Q', 'E513R', 'E513S', 'E513T', 'E513C', 'E513A', 'E513G', 'F514W', 'F514Q', 'F514P', 'F514N', 'F514M', 'F514L', 'F514K', 'F514I', 'F514H', 'F514G', 'F514E', 'F514D', 'F514C', 'F514R', 'F514V', 'F514T', 'F514S', 'F514Y', 'F514A', 'F515A', 'F515C', 'F515D', 'F515E', 'F515G', 'F515H', 'F515I', 'F515K', 'F515L', 'F515M', 'F515N', 'F515P', 'F515V', 'F515R', 'F515S', 'F515T', 'F515Q', 'F515W', 'F515Y', 'T516L', 'T516Y', 'T516W', 'T516V', 'T516S', 'T516R', 'T516Q', 'T516P', 'T516A', 'T516D', 'T516E', 'T516F', 'T516G', 'T516N', 'T516I', 'T516K', 'T516M', 'T516H', 'T516C', 'S517D', 'S517C', 'S517A', 'S517Y', 'S517W', 'S517V', 'S517T', 'S517R', 'S517Q', 'S517P', 'S517N', 'S517M', 'S517L', 'S517K', 'S517I', 'S517H', 'S517G', 'S517F', 'S517E', 'R518A', 'R518W', 'R518Y', 'R518V', 'R518T', 'R518S', 'R518Q', 'R518P', 'R518N', 'R518M', 'R518L', 'R518K', 'R518I', 'R518H', 'R518G', 'R518F', 'R518E', 'R518D', 'R518C', 'E519Y', 'E519W', 'E519A', 'E519D', 'E519C', 'E519F', 'E519G', 'E519I', 'E519K', 'E519L', 'E519H', 'E519N', 'E519P', 'E519Q', 'E519R', 'E519S', 'E519T', 'E519V', 'E519M', 'T520I', 'T520A', 'T520C', 'T520D', 'T520E', 'T520F', 'T520G', 'T520K', 'T520H', 'T520M', 'T520N', 'T520P', 'T520Q', 'T520R', 'T520S', 'T520W', 'T520Y', 'T520L', 'T520V', 'A521F', 'A521Y', 'A521G', 'A521H', 'A521I', 'A521K', 'A521L', 'A521M', 'A521C', 'A521P', 'A521Q', 'A521R', 'A521S', 'A521T', 'A521V', 'A521W', 'A521E', 'A521D', 'A521N', 'E522V', 'E522L', 'E522T', 'E522S', 'E522Q', 'E522P', 'E522N', 'E522M', 'E522W', 'E522K', 'E522H', 'E522G', 'E522F', 'E522D', 'E522C', 'E522A', 'E522I', 'E522Y', 'E522R', 'A523T', 'A523Y', 'A523W', 'A523C', 'A523D', 'A523E', 'A523F', 'A523V', 'A523H', 'A523I', 'A523G', 'A523L', 'A523M', 'A523N', 'A523P', 'A523Q', 'A523R', 'A523S', 'A523K', 'L524D', 'L524M', 'L524E', 'L524F', 'L524G', 'L524H', 'L524I', 'L524N', 'L524K', 'L524Q', 'L524R', 'L524S', 'L524T', 'L524V', 'L524W', 'L524Y', 'L524C', 'L524P', 'L524A', 'L525Y', 'L525W', 'L525V', 'L525T', 'L525S', 'L525R', 'L525Q', 'L525N', 'L525M', 'L525K', 'L525P', 'L525H', 'L525G', 'L525F', 'L525E', 'L525D', 'L525C', 'L525A', 'L525I', 'Q526M', 'Q526N', 'Q526P', 'Q526T', 'Q526W', 'Q526V', 'Q526L', 'Q526Y', 'Q526K', 'Q526R', 'Q526H', 'Q526G', 'Q526F', 'Q526E', 'Q526D', 'Q526C', 'Q526A', 'Q526S', 'Q526I', 'V527D', 'V527E', 'V527L', 'V527G', 'V527F', 'V527Y', 'V527W', 'V527T', 'V527S', 'V527R', 'V527Q', 'V527A', 'V527N', 'V527H', 'V527P', 'V527I', 'V527K', 'V527M', 'V527C', 'L528D', 'L528W', 'L528V', 'L528T', 'L528S', 'L528Q', 'L528C', 'L528P', 'L528M', 'L528A', 'L528I', 'L528H', 'L528G', 'L528F', 'L528E', 'L528N', 'L528K', 'L528R', 'L528Y', 'K529C', 'K529A', 'K529D', 'K529E', 'K529F', 'K529G', 'K529I', 'K529L', 'K529M', 'K529H', 'K529P', 'K529Q', 'K529R', 'K529S', 'K529T', 'K529V', 'K529W', 'K529Y', 'K529N', 'K530R', 'K530Y', 'K530W', 'K530V', 'K530T', 'K530S', 'K530Q', 'K530A', 'K530N', 'K530M', 'K530L', 'K530I', 'K530H', 'K530G', 'K530F', 'K530D', 'K530P', 'K530E', 'K530C', 'Y531V', 'Y531W', 'Y531A', 'Y531C', 'Y531D', 'Y531E', 'Y531F', 'Y531G', 'Y531H', 'Y531I', 'Y531K', 'Y531L', 'Y531M', 'Y531N', 'Y531P', 'Y531Q', 'Y531R', 'Y531S', 'Y531T', 'E532Y', 'E532W', 'E532C', 'E532T', 'E532V', 'E532D', 'E532F', 'E532G', 'E532H', 'E532K', 'E532I', 'E532M', 'E532N', 'E532P', 'E532S', 'E532Q', 'E532R', 'E532L', 'E532A', 'L533C', 'L533W', 'L533V', 'L533Y', 'L533T', 'L533S', 'L533R', 'L533Q', 'L533A', 'L533N', 'L533M', 'L533P', 'L533I', 'L533H', 'L533G', 'L533F', 'L533E', 'L533D', 'L533K', 'R534P', 'R534Y', 'R534W', 'R534V', 'R534T', 'R534S', 'R534Q', 'R534N', 'R534E', 'R534L', 'R534M', 'R534A', 'R534D', 'R534F', 'R534C', 'R534H', 'R534I', 'R534K', 'R534G', 'V535A', 'V535Q', 'V535Y', 'V535W', 'V535T', 'V535S', 'V535R', 'V535P', 'V535I', 'V535M', 'V535L', 'V535K', 'V535H', 'V535G', 'V535F', 'V535E', 'V535C', 'V535N', 'V535D', 'N536Y', 'N536Q', 'N536I', 'N536A', 'N536C', 'N536E', 'N536F', 'N536G', 'N536H', 'N536K', 'N536L', 'N536M', 'N536P', 'N536R', 'N536S', 'N536T', 'N536V', 'N536W', 'N536D', 'Y537A', 'Y537C', 'Y537T', 'Y537S', 'Y537R', 'Y537Q', 'Y537P', 'Y537N', 'Y537W', 'Y537V', 'Y537L', 'Y537K', 'Y537I', 'Y537H', 'Y537G', 'Y537F', 'Y537E', 'Y537M', 'Y537D', 'H538Q', 'H538R', 'H538S', 'H538W', 'H538V', 'H538P', 'H538Y', 'H538T', 'H538N', 'H538I', 'H538L', 'H538K', 'H538G', 'H538F', 'H538E', 'H538D', 'H538A', 'H538M', 'H538C', 'L539A', 'L539W', 'L539H', 'L539C', 'L539D', 'L539E', 'L539Y', 'L539F', 'L539G', 'L539T', 'L539V', 'L539K', 'L539M', 'L539N', 'L539P', 'L539Q', 'L539R', 'L539S', 'L539I', 'V540E', 'V540F', 'V540G', 'V540I', 'V540K', 'V540L', 'V540M', 'V540N', 'V540P', 'V540Q', 'V540R', 'V540S', 'V540T', 'V540W', 'V540Y', 'V540A', 'V540C', 'V540H', 'V540D', 'N541T', 'N541V', 'N541G', 'N541K', 'N541S', 'N541R', 'N541Q', 'N541P', 'N541M', 'N541L', 'N541I', 'N541H', 'N541F', 'N541E', 'N541D', 'N541C', 'N541W', 'N541Y', 'N541A', 'V542E', 'V542Y', 'V542W', 'V542T', 'V542S', 'V542R', 'V542Q', 'V542D', 'V542F', 'V542P', 'V542M', 'V542L', 'V542K', 'V542I', 'V542H', 'V542G', 'V542A', 'V542N', 'V542C', 'K543F', 'K543A', 'K543G', 'K543H', 'K543I', 'K543L', 'K543M', 'K543N', 'K543D', 'K543Q', 'K543R', 'K543S', 'K543T', 'K543V', 'K543W', 'K543Y', 'K543C', 'K543E', 'K543P', 'G544C', 'G544R', 'G544V', 'G544W', 'G544Y', 'G544T', 'G544S', 'G544Q', 'G544P', 'G544M', 'G544N', 'G544K', 'G544I', 'G544H', 'G544F', 'G544E', 'G544D', 'G544A', 'G544L', 'E545G', 'E545H', 'E545F', 'E545W', 'E545C', 'E545A', 'E545I', 'E545D', 'E545L', 'E545K', 'E545N', 'E545P', 'E545Q', 'E545R', 'E545S', 'E545T', 'E545V', 'E545Y', 'E545M', 'N546L', 'N546M', 'N546P', 'N546W', 'N546R', 'N546Y', 'N546K', 'N546V', 'N546Q', 'N546I', 'N546E', 'N546G', 'N546F', 'N546D', 'N546C', 'N546A', 'N546T', 'N546H', 'N546S', 'I547S', 'I547Y', 'I547W', 'I547T', 'I547Q', 'I547P', 'I547N', 'I547M', 'I547L', 'I547K', 'I547H', 'I547G', 'I547R', 'I547E', 'I547D', 'I547C', 'I547A', 'I547V', 'I547F', 'T548G', 'T548H', 'T548I', 'T548K', 'T548L', 'T548D', 'T548N', 'T548P', 'T548S', 'T548R', 'T548V', 'T548W', 'T548Y', 'T548E', 'T548F', 'T548C', 'T548A', 'T548Q', 'T548M', 'N549C', 'N549W', 'N549D', 'N549E', 'N549F', 'N549G', 'N549H', 'N549I', 'N549K', 'N549L', 'N549M', 'N549P', 'N549Q', 'N549R', 'N549S', 'N549T', 'N549V', 'N549Y', 'N549A', 'A550D', 'A550G', 'A550H', 'A550F', 'A550I', 'A550K', 'A550L', 'A550N', 'A550P', 'A550Q', 'A550M', 'A550R', 'A550E', 'A550Y', 'A550W', 'A550C', 'A550T', 'A550S', 'A550V', 'P551L', 'P551D', 'P551E', 'P551F', 'P551G', 'P551H', 'P551I', 'P551K', 'P551M', 'P551C', 'P551Q', 'P551R', 'P551S', 'P551T', 'P551V', 'P551W', 'P551Y', 'P551N', 'P551A', 'E552A', 'E552D', 'E552P', 'E552C', 'E552V', 'E552T', 'E552S', 'E552R', 'E552Q', 'E552W', 'E552N', 'E552L', 'E552K', 'E552I', 'E552H', 'E552G', 'E552F', 'E552M', 'E552Y', 'L553C', 'L553W', 'L553D', 'L553E', 'L553F', 'L553G', 'L553H', 'L553I', 'L553A', 'L553K', 'L553N', 'L553P', 'L553Q', 'L553R', 'L553S', 'L553V', 'L553M', 'L553Y', 'L553T', 'Q554V', 'Q554Y', 'Q554W', 'Q554T', 'Q554S', 'Q554R', 'Q554P', 'Q554M', 'Q554L', 'Q554K', 'Q554N', 'Q554H', 'Q554G', 'Q554F', 'Q554E', 'Q554D', 'Q554C', 'Q554A', 'Q554I', 'P555K', 'P555A', 'P555D', 'P555E', 'P555F', 'P555G', 'P555I', 'P555L', 'P555H', 'P555N', 'P555Q', 'P555R', 'P555S', 'P555Y', 'P555C', 'P555W', 'P555M', 'P555V', 'P555T', 'N556H', 'N556Y', 'N556D', 'N556E', 'N556F', 'N556G', 'N556C', 'N556I', 'N556W', 'N556A', 'N556K', 'N556M', 'N556P', 'N556Q', 'N556R', 'N556S', 'N556T', 'N556V', 'N556L', 'A557Q', 'A557R', 'A557D', 'A557W', 'A557P', 'A557V', 'A557S', 'A557C', 'A557E', 'A557M', 'A557L', 'A557K', 'A557I', 'A557H', 'A557G', 'A557F', 'A557T', 'A557Y', 'A557N', 'V558Y', 'V558S', 'V558A', 'V558W', 'V558R', 'V558Q', 'V558P', 'V558N', 'V558M', 'V558T', 'V558L', 'V558I', 'V558G', 'V558F', 'V558E', 'V558D', 'V558C', 'V558K', 'V558H', 'T559I', 'T559W', 'T559H', 'T559G', 'T559F', 'T559E', 'T559D', 'T559K', 'T559A', 'T559L', 'T559N', 'T559P', 'T559Q', 'T559R', 'T559S', 'T559V', 'T559Y', 'T559M', 'T559C', 'W560A', 'W560C', 'W560G', 'W560E', 'W560D', 'W560F', 'W560I', 'W560K', 'W560L', 'W560M', 'W560N', 'W560P', 'W560H', 'W560R', 'W560S', 'W560T', 'W560V', 'W560Y', 'W560Q', 'G561V', 'G561A', 'G561R', 'G561Q', 'G561P', 'G561N', 'G561M', 'G561I', 'G561K', 'G561H', 'G561F', 'G561E', 'G561D', 'G561C', 'G561L', 'G561W', 'G561T', 'G561Y', 'G561S', 'I562V', 'I562W', 'I562S', 'I562R', 'I562Q', 'I562P', 'I562N', 'I562M', 'I562A', 'I562T', 'I562K', 'I562H', 'I562G', 'I562F', 'I562Y', 'I562E', 'I562D', 'I562C', 'I562L', 'F563V', 'F563T', 'F563S', 'F563R', 'F563Q', 'F563P', 'F563E', 'F563M', 'F563L', 'F563K', 'F563H', 'F563G', 'F563D', 'F563N', 'F563I', 'F563W', 'F563A', 'F563Y', 'F563C', 'P564Y', 'P564W', 'P564V', 'P564T', 'P564S', 'P564R', 'P564D', 'P564N', 'P564M', 'P564L', 'P564K', 'P564I', 'P564H', 'P564G', 'P564F', 'P564E', 'P564Q', 'P564C', 'P564A', 'G565S', 'G565Y', 'G565W', 'G565V', 'G565T', 'G565R', 'G565Q', 'G565P', 'G565N', 'G565M', 'G565L', 'G565I', 'G565H', 'G565F', 'G565E', 'G565D', 'G565C', 'G565A', 'G565K', 'R566Y', 'R566W', 'R566V', 'R566T', 'R566S', 'R566Q', 'R566P', 'R566N', 'R566E', 'R566L', 'R566K', 'R566D', 'R566I', 'R566H', 'R566C', 'R566A', 'R566G', 'R566M', 'R566F', 'E567A', 'E567R', 'E567Y', 'E567W', 'E567V', 'E567T', 'E567S', 'E567Q', 'E567P', 'E567M', 'E567N', 'E567K', 'E567I', 'E567H', 'E567G', 'E567F', 'E567D', 'E567C', 'E567L', 'I568F', 'I568K', 'I568G', 'I568E', 'I568H', 'I568C', 'I568A', 'I568L', 'I568D', 'I568M', 'I568T', 'I568P', 'I568Q', 'I568R', 'I568S', 'I568Y', 'I568V', 'I568W', 'I568N', 'I569K', 'I569H', 'I569G', 'I569C', 'I569E', 'I569D', 'I569L', 'I569F', 'I569M', 'I569N', 'I569P', 'I569Q', 'I569R', 'I569S', 'I569T', 'I569V', 'I569W', 'I569Y', 'I569A', 'Q570V', 'Q570T', 'Q570S', 'Q570R', 'Q570P', 'Q570N', 'Q570K', 'Q570L', 'Q570W', 'Q570I', 'Q570H', 'Q570G', 'Q570M', 'Q570F', 'Q570E', 'Q570A', 'Q570C', 'Q570D', 'Q570Y', 'P571Q', 'P571D', 'P571A', 'P571C', 'P571G', 'P571H', 'P571M', 'P571E', 'P571K', 'P571Y', 'P571W', 'P571F', 'P571V', 'P571L', 'P571T', 'P571S', 'P571R', 'P571I', 'P571N', 'T572I', 'T572W', 'T572S', 'T572R', 'T572Q', 'T572P', 'T572N', 'T572M', 'T572L', 'T572K', 'T572H', 'T572G', 'T572F', 'T572E', 'T572D', 'T572C', 'T572A', 'T572V', 'T572Y', 'V573S', 'V573W', 'V573Q', 'V573P', 'V573N', 'V573M', 'V573L', 'V573K', 'V573I', 'V573H', 'V573G', 'V573F', 'V573E', 'V573D', 'V573C', 'V573A', 'V573Y', 'V573R', 'V573T', 'V574T', 'V574R', 'V574A', 'V574C', 'V574D', 'V574E', 'V574F', 'V574G', 'V574H', 'V574I', 'V574K', 'V574L', 'V574M', 'V574N', 'V574P', 'V574Q', 'V574W', 'V574S', 'V574Y', 'D575C', 'D575Y', 'D575W', 'D575S', 'D575R', 'D575Q', 'D575P', 'D575A', 'D575N', 'D575L', 'D575K', 'D575I', 'D575H', 'D575G', 'D575F', 'D575E', 'D575M', 'D575T', 'D575V', 'P576Q', 'P576S', 'P576R', 'P576N', 'P576M', 'P576L', 'P576K', 'P576V', 'P576I', 'P576G', 'P576F', 'P576E', 'P576D', 'P576C', 'P576A', 'P576H', 'P576W', 'P576T', 'P576Y', 'V577R', 'V577W', 'V577T', 'V577S', 'V577A', 'V577C', 'V577Y', 'V577D', 'V577E', 'V577F', 'V577G', 'V577I', 'V577K', 'V577L', 'V577M', 'V577N', 'V577P', 'V577Q', 'V577H', 'S578Q', 'S578R', 'S578T', 'S578C', 'S578W', 'S578Y', 'S578P', 'S578V', 'S578N', 'S578M', 'S578L', 'S578K', 'S578I', 'S578A', 'S578H', 'S578G', 'S578E', 'S578D', 'S578F', 'F579D', 'F579C', 'F579H', 'F579I', 'F579K', 'F579L', 'F579M', 'F579R', 'F579P', 'F579Q', 'F579E', 'F579S', 'F579T', 'F579V', 'F579W', 'F579Y', 'F579A', 'F579N', 'F579G', 'M580E', 'M580F', 'M580C', 'M580Y', 'M580W', 'M580V', 'M580T', 'M580S', 'M580D', 'M580R', 'M580P', 'M580N', 'M580L', 'M580I', 'M580H', 'M580G', 'M580Q', 'M580K', 'M580A', 'F581P', 'F581W', 'F581S', 'F581R', 'F581Q', 'F581V', 'F581N', 'F581M', 'F581L', 'F581K', 'F581Y', 'F581I', 'F581G', 'F581E', 'F581D', 'F581C', 'F581A', 'F581T', 'F581H', 'W582V', 'W582Y', 'W582N', 'W582M', 'W582F', 'W582C', 'W582D', 'W582P', 'W582E', 'W582L', 'W582G', 'W582H', 'W582A', 'W582Q', 'W582K', 'W582S', 'W582T', 'W582I', 'W582R', 'K583V', 'K583T', 'K583S', 'K583R', 'K583Q', 'K583P', 'K583M', 'K583L', 'K583I', 'K583G', 'K583F', 'K583E', 'K583D', 'K583N', 'K583H', 'K583A', 'K583C', 'K583Y', 'K583W', 'D584V', 'D584W', 'D584Y', 'D584C', 'D584A', 'D584E', 'D584G', 'D584H', 'D584I', 'D584F', 'D584L', 'D584T', 'D584K', 'D584R', 'D584Q', 'D584S', 'D584N', 'D584M', 'D584P', 'E585Q', 'E585Y', 'E585W', 'E585V', 'E585T', 'E585S', 'E585R', 'E585P', 'E585D', 'E585M', 'E585L', 'E585K', 'E585I', 'E585H', 'E585G', 'E585F', 'E585A', 'E585N', 'E585C', 'A586D', 'A586C', 'A586F', 'A586G', 'A586W', 'A586V', 'A586T', 'A586S', 'A586R', 'A586Q', 'A586Y', 'A586N', 'A586P', 'A586H', 'A586I', 'A586E', 'A586L', 'A586M', 'A586K', 'F587W', 'F587V', 'F587S', 'F587R', 'F587Q', 'F587P', 'F587N', 'F587M', 'F587L', 'F587I', 'F587G', 'F587E', 'F587D', 'F587Y', 'F587C', 'F587A', 'F587K', 'F587H', 'F587T', 'A588F', 'A588Q', 'A588V', 'A588R', 'A588S', 'A588T', 'A588M', 'A588L', 'A588K', 'A588I', 'A588H', 'A588G', 'A588E', 'A588D', 'A588C', 'A588W', 'A588Y', 'A588P', 'A588N', 'L589E', 'L589C', 'L589A', 'L589F', 'L589W', 'L589Y', 'L589G', 'L589H', 'L589I', 'L589K', 'L589M', 'L589S', 'L589P', 'L589Q', 'L589R', 'L589D', 'L589T', 'L589V', 'L589N', 'W590T', 'W590S', 'W590R', 'W590Q', 'W590P', 'W590N', 'W590M', 'W590L', 'W590G', 'W590I', 'W590H', 'W590F', 'W590E', 'W590D', 'W590A', 'W590V', 'W590Y', 'W590K', 'W590C', 'I591W', 'I591H', 'I591S', 'I591R', 'I591Q', 'I591P', 'I591N', 'I591M', 'I591T', 'I591L', 'I591G', 'I591F', 'I591E', 'I591D', 'I591C', 'I591A', 'I591K', 'I591V', 'I591Y', 'E592F', 'E592A', 'E592D', 'E592W', 'E592V', 'E592T', 'E592S', 'E592R', 'E592Q', 'E592C', 'E592P', 'E592G', 'E592H', 'E592I', 'E592Y', 'E592L', 'E592M', 'E592N', 'E592K', 'R593W', 'R593V', 'R593T', 'R593S', 'R593Q', 'R593N', 'R593M', 'R593L', 'R593I', 'R593H', 'R593Y', 'R593F', 'R593E', 'R593D', 'R593C', 'R593A', 'R593K', 'R593G', 'R593P', 'W594G', 'W594R', 'W594S', 'W594T', 'W594N', 'W594M', 'W594L', 'W594K', 'W594I', 'W594H', 'W594P', 'W594F', 'W594E', 'W594D', 'W594C', 'W594A', 'W594Y', 'W594V', 'W594Q', 'G595C', 'G595A', 'G595T', 'G595S', 'G595E', 'G595D', 'G595W', 'G595Y', 'G595R', 'G595Q', 'G595P', 'G595V', 'G595M', 'G595L', 'G595K', 'G595I', 'G595H', 'G595F', 'G595N', 'K596A', 'K596C', 'K596D', 'K596E', 'K596F', 'K596G', 'K596H', 'K596I', 'K596P', 'K596M', 'K596N', 'K596Q', 'K596R', 'K596S', 'K596V', 'K596W', 'K596Y', 'K596L', 'K596T', 'L597C', 'L597Y', 'L597A', 'L597W', 'L597V', 'L597T', 'L597S', 'L597Q', 'L597P', 'L597N', 'L597R', 'L597K', 'L597I', 'L597H', 'L597G', 'L597F', 'L597E', 'L597D', 'L597M', 'Y598I', 'Y598D', 'Y598E', 'Y598F', 'Y598G', 'Y598H', 'Y598K', 'Y598S', 'Y598M', 'Y598N', 'Y598P', 'Y598Q', 'Y598R', 'Y598T', 'Y598C', 'Y598L', 'Y598A', 'Y598W', 'Y598V', 'E599W', 'E599D', 'E599C', 'E599F', 'E599G', 'E599H', 'E599I', 'E599K', 'E599M', 'E599N', 'E599Q', 'E599R', 'E599S', 'E599T', 'E599V', 'E599Y', 'E599P', 'E599L', 'E599A', 'E600V', 'E600T', 'E600S', 'E600R', 'E600Q', 'E600P', 'E600N', 'E600W', 'E600L', 'E600I', 'E600H', 'E600G', 'E600F', 'E600D', 'E600C', 'E600A', 'E600K', 'E600Y', 'E600M', 'E601D', 'E601H', 'E601I', 'E601K', 'E601L', 'E601M', 'E601N', 'E601P', 'E601G', 'E601Q', 'E601S', 'E601T', 'E601V', 'E601W', 'E601A', 'E601C', 'E601Y', 'E601R', 'E601F', 'S602W', 'S602Q', 'S602V', 'S602A', 'S602C', 'S602D', 'S602E', 'S602F', 'S602Y', 'S602T', 'S602H', 'S602I', 'S602K', 'S602L', 'S602M', 'S602N', 'S602P', 'S602G', 'S602R', 'P603Y', 'P603W', 'P603G', 'P603C', 'P603A', 'P603D', 'P603E', 'P603F', 'P603H', 'P603I', 'P603K', 'P603M', 'P603N', 'P603Q', 'P603R', 'P603S', 'P603T', 'P603V', 'P603L', 'S604F', 'S604G', 'S604E', 'S604K', 'S604C', 'S604H', 'S604D', 'S604I', 'S604W', 'S604M', 'S604N', 'S604P', 'S604Q', 'S604R', 'S604T', 'S604V', 'S604L', 'S604A', 'S604Y', 'R605A', 'R605Y', 'R605W', 'R605V', 'R605T', 'R605S', 'R605Q', 'R605P', 'R605N', 'R605M', 'R605C', 'R605K', 'R605I', 'R605H', 'R605G', 'R605F', 'R605E', 'R605D', 'R605L', 'T606P', 'T606Q', 'T606R', 'T606W', 'T606V', 'T606N', 'T606Y', 'T606S', 'T606M', 'T606I', 'T606K', 'T606H', 'T606G', 'T606F', 'T606E', 'T606A', 'T606C', 'T606L', 'T606D', 'I607W', 'I607V', 'I607Y', 'I607A', 'I607C', 'I607D', 'I607E', 'I607F', 'I607H', 'I607K', 'I607G', 'I607M', 'I607N', 'I607P', 'I607Q', 'I607R', 'I607S', 'I607T', 'I607L', 'I608L', 'I608R', 'I608Q', 'I608P', 'I608N', 'I608M', 'I608K', 'I608T', 'I608G', 'I608F', 'I608E', 'I608D', 'I608C', 'I608A', 'I608S', 'I608H', 'I608W', 'I608V', 'I608Y', 'Q609D', 'Q609A', 'Q609C', 'Q609V', 'Q609T', 'Q609S', 'Q609R', 'Q609P', 'Q609N', 'Q609M', 'Q609L', 'Q609K', 'Q609I', 'Q609H', 'Q609G', 'Q609F', 'Q609E', 'Q609W', 'Q609Y', 'Y610F', 'Y610C', 'Y610G', 'Y610H', 'Y610K', 'Y610L', 'Y610M', 'Y610N', 'Y610A', 'Y610P', 'Y610R', 'Y610S', 'Y610T', 'Y610V', 'Y610W', 'Y610E', 'Y610Q', 'Y610D', 'Y610I', 'I611A', 'I611C', 'I611D', 'I611E', 'I611F', 'I611G', 'I611H', 'I611K', 'I611M', 'I611N', 'I611L', 'I611Q', 'I611P', 'I611W', 'I611V', 'I611Y', 'I611S', 'I611R', 'I611T', 'H612D', 'H612L', 'H612C', 'H612E', 'H612F', 'H612G', 'H612I', 'H612K', 'H612M', 'H612V', 'H612P', 'H612N', 'H612A', 'H612W', 'H612Y', 'H612S', 'H612R', 'H612Q', 'H612T', 'D613Q', 'D613Y', 'D613W', 'D613V', 'D613T', 'D613S', 'D613R', 'D613A', 'D613C', 'D613M', 'D613E', 'D613N', 'D613F', 'D613G', 'D613P', 'D613I', 'D613H', 'D613K', 'D613L', 'N614V', 'N614Y', 'N614W', 'N614R', 'N614Q', 'N614P', 'N614M', 'N614L', 'N614S', 'N614K', 'N614H', 'N614G', 'N614F', 'N614E', 'N614D', 'N614C', 'N614A', 'N614I', 'N614T', 'Y615R', 'Y615A', 'Y615C', 'Y615T', 'Y615E', 'Y615F', 'Y615G', 'Y615H', 'Y615I', 'Y615K', 'Y615L', 'Y615M', 'Y615N', 'Y615P', 'Y615Q', 'Y615S', 'Y615D', 'Y615W', 'Y615V', 'F616C', 'F616A', 'F616H', 'F616I', 'F616K', 'F616L', 'F616E', 'F616G', 'F616M', 'F616P', 'F616Q', 'F616R', 'F616S', 'F616T', 'F616V', 'F616W', 'F616Y', 'F616D', 'F616N', 'L617T', 'L617S', 'L617Y', 'L617W', 'L617R', 'L617Q', 'L617P', 'L617N', 'L617M', 'L617K', 'L617I', 'L617H', 'L617G', 'L617F', 'L617E', 'L617D', 'L617C', 'L617A', 'L617V', 'V618F', 'V618A', 'V618C', 'V618G', 'V618H', 'V618I', 'V618D', 'V618L', 'V618M', 'V618E', 'V618N', 'V618Q', 'V618R', 'V618S', 'V618T', 'V618W', 'V618Y', 'V618P', 'V618K', 'N619A', 'N619V', 'N619Y', 'N619S', 'N619R', 'N619Q', 'N619P', 'N619M', 'N619L', 'N619K', 'N619I', 'N619H', 'N619G', 'N619F', 'N619E', 'N619D', 'N619C', 'N619W', 'N619T', 'L620C', 'L620V', 'L620T', 'L620S', 'L620R', 'L620Q', 'L620P', 'L620N', 'L620K', 'L620I', 'L620H', 'L620G', 'L620F', 'L620E', 'L620D', 'L620A', 'L620M', 'L620Y', 'L620W', 'V621A', 'V621C', 'V621D', 'V621E', 'V621F', 'V621G', 'V621H', 'V621I', 'V621K', 'V621M', 'V621N', 'V621P', 'V621Q', 'V621R', 'V621S', 'V621T', 'V621W', 'V621L', 'V621Y', 'D622A', 'D622C', 'D622G', 'D622H', 'D622I', 'D622K', 'D622L', 'D622M', 'D622N', 'D622E', 'D622P', 'D622R', 'D622S', 'D622T', 'D622V', 'D622W', 'D622Y', 'D622Q', 'D622F', 'N623C', 'N623D', 'N623Y', 'N623F', 'N623G', 'N623H', 'N623I', 'N623K', 'N623L', 'N623A', 'N623M', 'N623Q', 'N623R', 'N623S', 'N623T', 'N623V', 'N623W', 'N623P', 'N623E', 'D624A', 'D624C', 'D624Y', 'D624F', 'D624G', 'D624H', 'D624I', 'D624K', 'D624L', 'D624M', 'D624N', 'D624P', 'D624Q', 'D624R', 'D624S', 'D624T', 'D624V', 'D624W', 'D624E', 'F625A', 'F625Y', 'F625E', 'F625W', 'F625V', 'F625T', 'F625S', 'F625R', 'F625Q', 'F625D', 'F625P', 'F625M', 'F625L', 'F625K', 'F625I', 'F625H', 'F625G', 'F625N', 'F625C', 'P626K', 'P626A', 'P626Y', 'P626W', 'P626V', 'P626T', 'P626S', 'P626R', 'P626Q', 'P626N', 'P626M', 'P626L', 'P626I', 'P626H', 'P626G', 'P626F', 'P626E', 'P626D', 'P626C', 'L627S', 'L627W', 'L627T', 'L627R', 'L627V', 'L627A', 'L627C', 'L627D', 'L627E', 'L627F', 'L627Y', 'L627G', 'L627I', 'L627K', 'L627M', 'L627N', 'L627P', 'L627Q', 'L627H', 'D628E', 'D628C', 'D628G', 'D628F', 'D628H', 'D628I', 'D628K', 'D628L', 'D628M', 'D628N', 'D628P', 'D628Q', 'D628R', 'D628S', 'D628T', 'D628V', 'D628W', 'D628Y', 'D628A', 'N629C', 'N629F', 'N629G', 'N629H', 'N629I', 'N629K', 'N629L', 'N629Y', 'N629M', 'N629Q', 'N629R', 'N629S', 'N629T', 'N629V', 'N629W', 'N629P', 'N629D', 'N629E', 'N629A', 'C630D', 'C630A', 'C630E', 'C630F', 'C630G', 'C630I', 'C630K', 'C630L', 'C630M', 'C630H', 'C630P', 'C630Q', 'C630R', 'C630S', 'C630T', 'C630V', 'C630W', 'C630N', 'C630Y', 'L631P', 'L631Q', 'L631R', 'L631Y', 'L631T', 'L631V', 'L631N', 'L631S', 'L631M', 'L631W', 'L631I', 'L631H', 'L631G', 'L631F', 'L631E', 'L631D', 'L631C', 'L631K', 'L631A', 'W632R', 'W632Q', 'W632P', 'W632N', 'W632M', 'W632L', 'W632K', 'W632F', 'W632H', 'W632G', 'W632S', 'W632E', 'W632D', 'W632C', 'W632A', 'W632I', 'W632T', 'W632V', 'W632Y', 'Q633A', 'Q633Y', 'Q633W', 'Q633V', 'Q633T', 'Q633S', 'Q633R', 'Q633P', 'Q633N', 'Q633M', 'Q633K', 'Q633I', 'Q633H', 'Q633G', 'Q633F', 'Q633D', 'Q633C', 'Q633L', 'Q633E', 'V634W', 'V634A', 'V634D', 'V634E', 'V634F', 'V634G', 'V634H', 'V634I', 'V634K', 'V634L', 'V634M', 'V634N', 'V634P', 'V634Q', 'V634R', 'V634S', 'V634T', 'V634C', 'V634Y', 'V635T', 'V635Y', 'V635S', 'V635R', 'V635Q', 'V635P', 'V635N', 'V635M', 'V635L', 'V635K', 'V635I', 'V635H', 'V635G', 'V635F', 'V635E', 'V635D', 'V635C', 'V635W', 'V635A', 'E636C', 'E636W', 'E636D', 'E636F', 'E636G', 'E636H', 'E636I', 'E636K', 'E636L', 'E636A', 'E636N', 'E636M', 'E636V', 'E636T', 'E636Y', 'E636R', 'E636Q', 'E636P', 'E636S', 'D637G', 'D637H', 'D637I', 'D637K', 'D637L', 'D637M', 'D637N', 'D637P', 'D637R', 'D637S', 'D637V', 'D637W', 'D637Y', 'D637C', 'D637F', 'D637A', 'D637Q', 'D637T', 'D637E', 'T638W', 'T638Y', 'T638A', 'T638C', 'T638D', 'T638E', 'T638G', 'T638H', 'T638I', 'T638F', 'T638L', 'T638K', 'T638S', 'T638R', 'T638Q', 'T638V', 'T638N', 'T638M', 'T638P', 'L639N', 'L639V', 'L639T', 'L639S', 'L639R', 'L639Q', 'L639P', 'L639M', 'L639A', 'L639I', 'L639H', 'L639G', 'L639F', 'L639E', 'L639D', 'L639C', 'L639W', 'L639K', 'L639Y', 'E640C', 'E640Y', 'E640W', 'E640V', 'E640T', 'E640S', 'E640R', 'E640P', 'E640N', 'E640M', 'E640Q', 'E640K', 'E640I', 'E640H', 'E640G', 'E640F', 'E640D', 'E640A', 'E640L', 'L641N', 'L641V', 'L641T', 'L641S', 'L641R', 'L641Q', 'L641P', 'L641A', 'L641C', 'L641K', 'L641I', 'L641H', 'L641G', 'L641F', 'L641E', 'L641D', 'L641Y', 'L641M', 'L641W', 'L642N', 'L642D', 'L642Y', 'L642W', 'L642V', 'L642T', 'L642S', 'L642R', 'L642C', 'L642Q', 'L642M', 'L642I', 'L642H', 'L642G', 'L642F', 'L642E', 'L642P', 'L642K', 'L642A', 'N643Y', 'N643C', 'N643D', 'N643E', 'N643F', 'N643G', 'N643H', 'N643I', 'N643A', 'N643K', 'N643M', 'N643P', 'N643Q', 'N643R', 'N643S', 'N643T', 'N643V', 'N643W', 'N643L', 'R644W', 'R644Y', 'R644V', 'R644D', 'R644S', 'R644A', 'R644C', 'R644E', 'R644F', 'R644H', 'R644G', 'R644K', 'R644L', 'R644M', 'R644N', 'R644P', 'R644Q', 'R644I', 'R644T', 'P645G', 'P645I', 'P645H', 'P645Y', 'P645E', 'P645A', 'P645C', 'P645F', 'P645L', 'P645K', 'P645N', 'P645M', 'P645W', 'P645V', 'P645T', 'P645D', 'P645R', 'P645Q', 'P645S', 'T646A', 'T646C', 'T646D', 'T646E', 'T646F', 'T646G', 'T646H', 'T646I', 'T646K', 'T646L', 'T646N', 'T646Q', 'T646R', 'T646S', 'T646V', 'T646W', 'T646Y', 'T646M', 'T646P', 'Q647S', 'Q647W', 'Q647Y', 'Q647R', 'Q647V', 'Q647P', 'Q647N', 'Q647M', 'Q647L', 'Q647K', 'Q647I', 'Q647H', 'Q647G', 'Q647F', 'Q647E', 'Q647D', 'Q647C', 'Q647A', 'Q647T', 'N648C', 'N648Y', 'N648E', 'N648W', 'N648V', 'N648T', 'N648S', 'N648R', 'N648Q', 'N648A', 'N648P', 'N648L', 'N648K', 'N648I', 'N648H', 'N648G', 'N648F', 'N648M', 'N648D', 'A649C', 'A649E', 'A649D', 'A649F', 'A649G', 'A649H', 'A649I', 'A649K', 'A649L', 'A649M', 'A649P', 'A649Q', 'A649R', 'A649S', 'A649T', 'A649V', 'A649W', 'A649N', 'A649Y', 'R650W', 'R650Y', 'R650S', 'R650Q', 'R650P', 'R650N', 'R650M', 'R650L', 'R650H', 'R650I', 'R650V', 'R650G', 'R650F', 'R650E', 'R650D', 'R650C', 'R650A', 'R650K', 'R650T', 'E651A', 'E651D', 'E651F', 'E651Y', 'E651W', 'E651V', 'E651T', 'E651S', 'E651R', 'E651C', 'E651Q', 'E651N', 'E651M', 'E651L', 'E651I', 'E651H', 'E651G', 'E651P', 'E651K', 'T652D', 'T652E', 'T652C', 'T652Y', 'T652W', 'T652V', 'T652S', 'T652R', 'T652Q', 'T652A', 'T652P', 'T652M', 'T652L', 'T652K', 'T652I', 'T652H', 'T652G', 'T652F', 'T652N', 'E653W', 'E653D', 'E653N', 'E653C', 'E653F', 'E653G', 'E653H', 'E653I', 'E653K', 'E653L', 'E653M', 'E653P', 'E653Q', 'E653R', 'E653S', 'E653T', 'E653V', 'E653Y', 'E653A', 'A654E', 'A654F', 'A654G', 'A654Y', 'A654W', 'A654V', 'A654T', 'A654S', 'A654R', 'A654C', 'A654Q', 'A654N', 'A654M', 'A654L', 'A654K', 'A654I', 'A654H', 'A654P', 'A654D', 'P655C', 'P655Y', 'P655D', 'P655E', 'P655F', 'P655G', 'P655H', 'P655I', 'P655K', 'P655L', 'P655M', 'P655N', 'P655Q', 'P655R', 'P655S', 'P655T', 'P655V', 'P655A', 'P655W']\n"
     ]
    }
   ],
   "source": [
    "df_test_mutants = list(df_test[\"mutant\"])\n",
    "print(df_test_mutants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11224\n"
     ]
    }
   ],
   "source": [
    "df_test_no_query1 = list(set(df_test_mutants) - set(query1_mutants))\n",
    "print(len(df_test_no_query1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1D</td>\n",
       "      <td>MDNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V1Y</td>\n",
       "      <td>MYNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V1C</td>\n",
       "      <td>MCNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1A</td>\n",
       "      <td>MANEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V1E</td>\n",
       "      <td>MENEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11319</th>\n",
       "      <td>P655S</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11320</th>\n",
       "      <td>P655T</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>P655V</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>P655A</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>P655W</td>\n",
       "      <td>MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11224 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant                                           sequence\n",
       "0        V1D  MDNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "1        V1Y  MYNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "2        V1C  MCNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "3        V1A  MANEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "4        V1E  MENEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "...      ...                                                ...\n",
       "11319  P655S  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11320  P655T  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11321  P655V  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11322  P655A  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "11323  P655W  MVNEARGNSSLNPCLEGSASSGSESSKDSSRCSTPGLDPERHERLR...\n",
       "\n",
       "[11224 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_active_learn = df_test[df_test['mutant'].isin(df_test_no_query1)]\n",
    "df_test_active_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mutant, sequence]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_active_learn[df_test_active_learn[\"mutant\"] == \"S77V\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6451460057437225\n",
      "0.560067133348446\n",
      "0.6916470280178059\n",
      "0.7136465882304719\n",
      "0.6842313955690316\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(cor_esm(RNN_ensemble[i],val_sets[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_predict(model, test_dataset):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        for row in test_dataset:\n",
    "            mut,esm = row\n",
    "            esm = esm.to(device)\n",
    "            #mut = mut.to(device)\n",
    "            preds.append(model(esm.unsqueeze(0)))\n",
    "    preds = [float(x) for x in preds]\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for creating a dataset to predict with \n",
    "class ProteinTestESMDataset(Dataset):\n",
    "    def __init__(self, mutant_list, emb_dir):\n",
    "        super().__init__()\n",
    "        self.mutants = mutant_list\n",
    "        self.embeddings = []\n",
    "        for mut in tqdm(mutant_list, desc='Loading esm embeddings'):\n",
    "            name = mut\n",
    "            emb_file = os.path.join(emb_dir, f'{name}.pt')\n",
    "            emb = torch.load(emb_file)['mean_representations'][33]\n",
    "            self.embeddings.append(emb)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        emb = self.embeddings[index]\n",
    "        mut = self.mutants[index]\n",
    "        return mut, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 11224/11224 [00:34<00:00, 327.10it/s]\n"
     ]
    }
   ],
   "source": [
    "mutant_list = list(df_test_active_learn[\"mutant\"])\n",
    "test_active_learn = ProteinTestESMDataset(mutant_list, \"esm_embeddings_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_test_predicts = []\n",
    "for i in range(5):\n",
    "    RNN_preds = RNN_predict(RNN_ensemble[i], test_active_learn)\n",
    "    ensemble_test_predicts.append(RNN_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_and_sd(pred_list):\n",
    "    mean = []\n",
    "    sd = []\n",
    "    for i in range(len(pred_list[0])):\n",
    "        calc_list = []\n",
    "        for j in range(len(pred_list)):\n",
    "            calc_list.append(pred_list[j][i])\n",
    "        mean.append(np.mean(calc_list))\n",
    "        sd.append(np.std(calc_list))\n",
    "    return (mean,sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_RNN_ensemble, sds_RNN_ensemble = calc_mean_and_sd(ensemble_test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>RNN_model_1</th>\n",
       "      <th>RNN_model_2</th>\n",
       "      <th>RNN_model_3</th>\n",
       "      <th>RNN_model_4</th>\n",
       "      <th>RNN_model_5</th>\n",
       "      <th>Mean_DMS</th>\n",
       "      <th>SD_DMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1D</td>\n",
       "      <td>0.182941</td>\n",
       "      <td>0.191655</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.126397</td>\n",
       "      <td>0.794630</td>\n",
       "      <td>0.329374</td>\n",
       "      <td>0.244356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V1Y</td>\n",
       "      <td>0.192862</td>\n",
       "      <td>0.237899</td>\n",
       "      <td>0.527609</td>\n",
       "      <td>0.109419</td>\n",
       "      <td>0.748183</td>\n",
       "      <td>0.363194</td>\n",
       "      <td>0.238479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V1C</td>\n",
       "      <td>0.172023</td>\n",
       "      <td>0.155978</td>\n",
       "      <td>0.370862</td>\n",
       "      <td>0.103844</td>\n",
       "      <td>0.769396</td>\n",
       "      <td>0.314421</td>\n",
       "      <td>0.244913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1A</td>\n",
       "      <td>0.206846</td>\n",
       "      <td>0.417425</td>\n",
       "      <td>0.627325</td>\n",
       "      <td>0.179057</td>\n",
       "      <td>0.915782</td>\n",
       "      <td>0.469287</td>\n",
       "      <td>0.275729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V1E</td>\n",
       "      <td>0.187640</td>\n",
       "      <td>0.332994</td>\n",
       "      <td>0.480051</td>\n",
       "      <td>0.164003</td>\n",
       "      <td>0.889675</td>\n",
       "      <td>0.410872</td>\n",
       "      <td>0.264907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11219</th>\n",
       "      <td>P655S</td>\n",
       "      <td>0.155549</td>\n",
       "      <td>0.085672</td>\n",
       "      <td>0.147302</td>\n",
       "      <td>0.143114</td>\n",
       "      <td>0.745580</td>\n",
       "      <td>0.255443</td>\n",
       "      <td>0.246312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11220</th>\n",
       "      <td>P655T</td>\n",
       "      <td>0.172705</td>\n",
       "      <td>0.165946</td>\n",
       "      <td>0.235787</td>\n",
       "      <td>0.132652</td>\n",
       "      <td>0.804672</td>\n",
       "      <td>0.302352</td>\n",
       "      <td>0.253365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221</th>\n",
       "      <td>P655V</td>\n",
       "      <td>0.162603</td>\n",
       "      <td>0.214961</td>\n",
       "      <td>0.343043</td>\n",
       "      <td>0.145074</td>\n",
       "      <td>0.802657</td>\n",
       "      <td>0.333667</td>\n",
       "      <td>0.244524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11222</th>\n",
       "      <td>P655A</td>\n",
       "      <td>0.170669</td>\n",
       "      <td>0.222135</td>\n",
       "      <td>0.392505</td>\n",
       "      <td>0.166648</td>\n",
       "      <td>0.869379</td>\n",
       "      <td>0.364267</td>\n",
       "      <td>0.265582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223</th>\n",
       "      <td>P655W</td>\n",
       "      <td>0.199818</td>\n",
       "      <td>0.303189</td>\n",
       "      <td>0.515806</td>\n",
       "      <td>0.148764</td>\n",
       "      <td>0.764381</td>\n",
       "      <td>0.386392</td>\n",
       "      <td>0.227079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11224 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant  RNN_model_1  RNN_model_2  RNN_model_3  RNN_model_4  RNN_model_5  \\\n",
       "0        V1D     0.182941     0.191655     0.351250     0.126397     0.794630   \n",
       "1        V1Y     0.192862     0.237899     0.527609     0.109419     0.748183   \n",
       "2        V1C     0.172023     0.155978     0.370862     0.103844     0.769396   \n",
       "3        V1A     0.206846     0.417425     0.627325     0.179057     0.915782   \n",
       "4        V1E     0.187640     0.332994     0.480051     0.164003     0.889675   \n",
       "...      ...          ...          ...          ...          ...          ...   \n",
       "11219  P655S     0.155549     0.085672     0.147302     0.143114     0.745580   \n",
       "11220  P655T     0.172705     0.165946     0.235787     0.132652     0.804672   \n",
       "11221  P655V     0.162603     0.214961     0.343043     0.145074     0.802657   \n",
       "11222  P655A     0.170669     0.222135     0.392505     0.166648     0.869379   \n",
       "11223  P655W     0.199818     0.303189     0.515806     0.148764     0.764381   \n",
       "\n",
       "       Mean_DMS    SD_DMS  \n",
       "0      0.329374  0.244356  \n",
       "1      0.363194  0.238479  \n",
       "2      0.314421  0.244913  \n",
       "3      0.469287  0.275729  \n",
       "4      0.410872  0.264907  \n",
       "...         ...       ...  \n",
       "11219  0.255443  0.246312  \n",
       "11220  0.302352  0.253365  \n",
       "11221  0.333667  0.244524  \n",
       "11222  0.364267  0.265582  \n",
       "11223  0.386392  0.227079  \n",
       "\n",
       "[11224 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learn_df = pd.DataFrame({\"mutant\": mutant_list, \"RNN_model_1\": ensemble_test_predicts[0], \n",
    "                                \"RNN_model_2\": ensemble_test_predicts[1], \"RNN_model_3\": ensemble_test_predicts[2],\n",
    "                                \"RNN_model_4\": ensemble_test_predicts[3], \"RNN_model_5\": ensemble_test_predicts[4],\n",
    "                                \"Mean_DMS\" : means_RNN_ensemble , \"SD_DMS\" : sds_RNN_ensemble})\n",
    "active_learn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learn_df.to_csv(\"query2_active_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learn(active_learn_df, beta):\n",
    "    alpha_list = []\n",
    "    for i in range(len(active_learn_df)):\n",
    "        alpha = active_learn_df[\"Mean_DMS\"][i] + (beta**(1/2) * active_learn_df[\"SD_DMS\"][i])\n",
    "        alpha_list.append(alpha)\n",
    "    active_learn_df[\"Alpha_Score\"] = alpha_list\n",
    "    return (active_learn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learn_df = active_learn(active_learn_df,4)\n",
    "active_learn_df\n",
    "active_learn_df.to_csv(\"query2_active_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>RNN_model_1</th>\n",
       "      <th>RNN_model_2</th>\n",
       "      <th>RNN_model_3</th>\n",
       "      <th>RNN_model_4</th>\n",
       "      <th>RNN_model_5</th>\n",
       "      <th>Mean_DMS</th>\n",
       "      <th>SD_DMS</th>\n",
       "      <th>Alpha_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10055</th>\n",
       "      <td>R593A</td>\n",
       "      <td>0.470846</td>\n",
       "      <td>0.914777</td>\n",
       "      <td>0.215206</td>\n",
       "      <td>0.232565</td>\n",
       "      <td>0.825227</td>\n",
       "      <td>0.531724</td>\n",
       "      <td>0.291979</td>\n",
       "      <td>1.115683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>R131G</td>\n",
       "      <td>0.462670</td>\n",
       "      <td>0.913696</td>\n",
       "      <td>0.224873</td>\n",
       "      <td>0.246490</td>\n",
       "      <td>0.832928</td>\n",
       "      <td>0.536131</td>\n",
       "      <td>0.288726</td>\n",
       "      <td>1.113583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>R133P</td>\n",
       "      <td>0.475836</td>\n",
       "      <td>0.913461</td>\n",
       "      <td>0.222046</td>\n",
       "      <td>0.237987</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>0.534311</td>\n",
       "      <td>0.288231</td>\n",
       "      <td>1.110774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>R5N</td>\n",
       "      <td>0.467331</td>\n",
       "      <td>0.916028</td>\n",
       "      <td>0.221987</td>\n",
       "      <td>0.241014</td>\n",
       "      <td>0.821470</td>\n",
       "      <td>0.533566</td>\n",
       "      <td>0.288521</td>\n",
       "      <td>1.110608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>M245A</td>\n",
       "      <td>0.465566</td>\n",
       "      <td>0.914376</td>\n",
       "      <td>0.228136</td>\n",
       "      <td>0.224811</td>\n",
       "      <td>0.818960</td>\n",
       "      <td>0.530369</td>\n",
       "      <td>0.289709</td>\n",
       "      <td>1.109787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>K467A</td>\n",
       "      <td>0.471034</td>\n",
       "      <td>0.886453</td>\n",
       "      <td>0.205352</td>\n",
       "      <td>0.236715</td>\n",
       "      <td>0.810032</td>\n",
       "      <td>0.521917</td>\n",
       "      <td>0.282857</td>\n",
       "      <td>1.087630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>C451E</td>\n",
       "      <td>0.469920</td>\n",
       "      <td>0.888264</td>\n",
       "      <td>0.214475</td>\n",
       "      <td>0.239881</td>\n",
       "      <td>0.812152</td>\n",
       "      <td>0.524938</td>\n",
       "      <td>0.281130</td>\n",
       "      <td>1.087198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>R324M</td>\n",
       "      <td>0.353368</td>\n",
       "      <td>0.886423</td>\n",
       "      <td>0.190478</td>\n",
       "      <td>0.221431</td>\n",
       "      <td>0.812049</td>\n",
       "      <td>0.492750</td>\n",
       "      <td>0.297099</td>\n",
       "      <td>1.086948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>R133N</td>\n",
       "      <td>0.467440</td>\n",
       "      <td>0.892058</td>\n",
       "      <td>0.217041</td>\n",
       "      <td>0.238809</td>\n",
       "      <td>0.808047</td>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.281035</td>\n",
       "      <td>1.086749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>K211A</td>\n",
       "      <td>0.473555</td>\n",
       "      <td>0.883685</td>\n",
       "      <td>0.208941</td>\n",
       "      <td>0.238326</td>\n",
       "      <td>0.813196</td>\n",
       "      <td>0.523541</td>\n",
       "      <td>0.281575</td>\n",
       "      <td>1.086691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant  RNN_model_1  RNN_model_2  RNN_model_3  RNN_model_4  RNN_model_5  \\\n",
       "10055  R593A     0.470846     0.914777     0.215206     0.232565     0.825227   \n",
       "2342   R131G     0.462670     0.913696     0.224873     0.246490     0.832928   \n",
       "2373   R133P     0.475836     0.913461     0.222046     0.237987     0.822225   \n",
       "88       R5N     0.467331     0.916028     0.221987     0.241014     0.821470   \n",
       "4114   M245A     0.465566     0.914376     0.228136     0.224811     0.818960   \n",
       "...      ...          ...          ...          ...          ...          ...   \n",
       "7671   K467A     0.471034     0.886453     0.205352     0.236715     0.810032   \n",
       "7375   C451E     0.469920     0.888264     0.214475     0.239881     0.812152   \n",
       "5066   R324M     0.353368     0.886423     0.190478     0.221431     0.812049   \n",
       "2367   R133N     0.467440     0.892058     0.217041     0.238809     0.808047   \n",
       "3589   K211A     0.473555     0.883685     0.208941     0.238326     0.813196   \n",
       "\n",
       "       Mean_DMS    SD_DMS  Alpha_Score  \n",
       "10055  0.531724  0.291979     1.115683  \n",
       "2342   0.536131  0.288726     1.113583  \n",
       "2373   0.534311  0.288231     1.110774  \n",
       "88     0.533566  0.288521     1.110608  \n",
       "4114   0.530369  0.289709     1.109787  \n",
       "...         ...       ...          ...  \n",
       "7671   0.521917  0.282857     1.087630  \n",
       "7375   0.524938  0.281130     1.087198  \n",
       "5066   0.492750  0.297099     1.086948  \n",
       "2367   0.524679  0.281035     1.086749  \n",
       "3589   0.523541  0.281575     1.086691  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learn_df.sort_values(\"Alpha_Score\", ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>RNN_model_1</th>\n",
       "      <th>RNN_model_2</th>\n",
       "      <th>RNN_model_3</th>\n",
       "      <th>RNN_model_4</th>\n",
       "      <th>RNN_model_5</th>\n",
       "      <th>Mean_DMS</th>\n",
       "      <th>SD_DMS</th>\n",
       "      <th>Alpha_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>R131G</td>\n",
       "      <td>0.462670</td>\n",
       "      <td>0.913696</td>\n",
       "      <td>0.224873</td>\n",
       "      <td>0.246490</td>\n",
       "      <td>0.832928</td>\n",
       "      <td>0.536131</td>\n",
       "      <td>0.288726</td>\n",
       "      <td>1.113583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>R518A</td>\n",
       "      <td>0.477493</td>\n",
       "      <td>0.906072</td>\n",
       "      <td>0.222484</td>\n",
       "      <td>0.244554</td>\n",
       "      <td>0.827644</td>\n",
       "      <td>0.535649</td>\n",
       "      <td>0.285890</td>\n",
       "      <td>1.107429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>R133P</td>\n",
       "      <td>0.475836</td>\n",
       "      <td>0.913461</td>\n",
       "      <td>0.222046</td>\n",
       "      <td>0.237987</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>0.534311</td>\n",
       "      <td>0.288231</td>\n",
       "      <td>1.110774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>R5N</td>\n",
       "      <td>0.467331</td>\n",
       "      <td>0.916028</td>\n",
       "      <td>0.221987</td>\n",
       "      <td>0.241014</td>\n",
       "      <td>0.821470</td>\n",
       "      <td>0.533566</td>\n",
       "      <td>0.288521</td>\n",
       "      <td>1.110608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>K198Q</td>\n",
       "      <td>0.477034</td>\n",
       "      <td>0.904346</td>\n",
       "      <td>0.221316</td>\n",
       "      <td>0.244529</td>\n",
       "      <td>0.818463</td>\n",
       "      <td>0.533138</td>\n",
       "      <td>0.283860</td>\n",
       "      <td>1.100857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>S25K</td>\n",
       "      <td>0.448754</td>\n",
       "      <td>0.863414</td>\n",
       "      <td>0.195269</td>\n",
       "      <td>0.224780</td>\n",
       "      <td>0.780522</td>\n",
       "      <td>0.502548</td>\n",
       "      <td>0.276392</td>\n",
       "      <td>1.055332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10624</th>\n",
       "      <td>D624A</td>\n",
       "      <td>0.464275</td>\n",
       "      <td>0.825124</td>\n",
       "      <td>0.200790</td>\n",
       "      <td>0.238139</td>\n",
       "      <td>0.784319</td>\n",
       "      <td>0.502529</td>\n",
       "      <td>0.263015</td>\n",
       "      <td>1.028559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9887</th>\n",
       "      <td>D584N</td>\n",
       "      <td>0.467553</td>\n",
       "      <td>0.832310</td>\n",
       "      <td>0.195092</td>\n",
       "      <td>0.240766</td>\n",
       "      <td>0.776574</td>\n",
       "      <td>0.502459</td>\n",
       "      <td>0.263862</td>\n",
       "      <td>1.030184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>K287A</td>\n",
       "      <td>0.461335</td>\n",
       "      <td>0.835180</td>\n",
       "      <td>0.194085</td>\n",
       "      <td>0.237968</td>\n",
       "      <td>0.783714</td>\n",
       "      <td>0.502456</td>\n",
       "      <td>0.267039</td>\n",
       "      <td>1.036534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10594</th>\n",
       "      <td>D622N</td>\n",
       "      <td>0.465356</td>\n",
       "      <td>0.828975</td>\n",
       "      <td>0.201593</td>\n",
       "      <td>0.239382</td>\n",
       "      <td>0.776958</td>\n",
       "      <td>0.502453</td>\n",
       "      <td>0.261942</td>\n",
       "      <td>1.026337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant  RNN_model_1  RNN_model_2  RNN_model_3  RNN_model_4  RNN_model_5  \\\n",
       "2342   R131G     0.462670     0.913696     0.224873     0.246490     0.832928   \n",
       "8621   R518A     0.477493     0.906072     0.222484     0.244554     0.827644   \n",
       "2373   R133P     0.475836     0.913461     0.222046     0.237987     0.822225   \n",
       "88       R5N     0.467331     0.916028     0.221987     0.241014     0.821470   \n",
       "3357   K198Q     0.477034     0.904346     0.221316     0.244529     0.818463   \n",
       "...      ...          ...          ...          ...          ...          ...   \n",
       "466     S25K     0.448754     0.863414     0.195269     0.224780     0.780522   \n",
       "10624  D624A     0.464275     0.825124     0.200790     0.238139     0.784319   \n",
       "9887   D584N     0.467553     0.832310     0.195092     0.240766     0.776574   \n",
       "4739   K287A     0.461335     0.835180     0.194085     0.237968     0.783714   \n",
       "10594  D622N     0.465356     0.828975     0.201593     0.239382     0.776958   \n",
       "\n",
       "       Mean_DMS    SD_DMS  Alpha_Score  \n",
       "2342   0.536131  0.288726     1.113583  \n",
       "8621   0.535649  0.285890     1.107429  \n",
       "2373   0.534311  0.288231     1.110774  \n",
       "88     0.533566  0.288521     1.110608  \n",
       "3357   0.533138  0.283860     1.100857  \n",
       "...         ...       ...          ...  \n",
       "466    0.502548  0.276392     1.055332  \n",
       "10624  0.502529  0.263015     1.028559  \n",
       "9887   0.502459  0.263862     1.030184  \n",
       "4739   0.502456  0.267039     1.036534  \n",
       "10594  0.502453  0.261942     1.026337  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learn_df.sort_values(\"Mean_DMS\", ascending=False).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>RNN_model_1</th>\n",
       "      <th>RNN_model_2</th>\n",
       "      <th>RNN_model_3</th>\n",
       "      <th>RNN_model_4</th>\n",
       "      <th>RNN_model_5</th>\n",
       "      <th>Mean_DMS</th>\n",
       "      <th>SD_DMS</th>\n",
       "      <th>Alpha_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>P34V</td>\n",
       "      <td>0.122858</td>\n",
       "      <td>0.834310</td>\n",
       "      <td>0.150564</td>\n",
       "      <td>0.123836</td>\n",
       "      <td>0.780246</td>\n",
       "      <td>0.402363</td>\n",
       "      <td>0.331203</td>\n",
       "      <td>1.064769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>P34I</td>\n",
       "      <td>0.126094</td>\n",
       "      <td>0.834952</td>\n",
       "      <td>0.151882</td>\n",
       "      <td>0.117739</td>\n",
       "      <td>0.777460</td>\n",
       "      <td>0.401625</td>\n",
       "      <td>0.331030</td>\n",
       "      <td>1.063685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>P34L</td>\n",
       "      <td>0.139693</td>\n",
       "      <td>0.848730</td>\n",
       "      <td>0.176091</td>\n",
       "      <td>0.144827</td>\n",
       "      <td>0.786397</td>\n",
       "      <td>0.419148</td>\n",
       "      <td>0.326140</td>\n",
       "      <td>1.071428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>P34T</td>\n",
       "      <td>0.131012</td>\n",
       "      <td>0.824373</td>\n",
       "      <td>0.152367</td>\n",
       "      <td>0.133915</td>\n",
       "      <td>0.766578</td>\n",
       "      <td>0.401649</td>\n",
       "      <td>0.322160</td>\n",
       "      <td>1.045969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>P34D</td>\n",
       "      <td>0.112605</td>\n",
       "      <td>0.790250</td>\n",
       "      <td>0.136292</td>\n",
       "      <td>0.111777</td>\n",
       "      <td>0.756968</td>\n",
       "      <td>0.381578</td>\n",
       "      <td>0.320386</td>\n",
       "      <td>1.022350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>D103P</td>\n",
       "      <td>0.252813</td>\n",
       "      <td>0.833864</td>\n",
       "      <td>0.165538</td>\n",
       "      <td>0.211090</td>\n",
       "      <td>0.786249</td>\n",
       "      <td>0.449911</td>\n",
       "      <td>0.295735</td>\n",
       "      <td>1.041380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364</th>\n",
       "      <td>Y610K</td>\n",
       "      <td>0.199439</td>\n",
       "      <td>0.755065</td>\n",
       "      <td>0.113587</td>\n",
       "      <td>0.167564</td>\n",
       "      <td>0.765987</td>\n",
       "      <td>0.400329</td>\n",
       "      <td>0.295398</td>\n",
       "      <td>0.991125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7230</th>\n",
       "      <td>R444I</td>\n",
       "      <td>0.304534</td>\n",
       "      <td>0.871377</td>\n",
       "      <td>0.200796</td>\n",
       "      <td>0.220550</td>\n",
       "      <td>0.806391</td>\n",
       "      <td>0.480730</td>\n",
       "      <td>0.295216</td>\n",
       "      <td>1.071161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>L436R</td>\n",
       "      <td>0.247674</td>\n",
       "      <td>0.811360</td>\n",
       "      <td>0.154619</td>\n",
       "      <td>0.181429</td>\n",
       "      <td>0.775458</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.295146</td>\n",
       "      <td>1.024401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>G21I</td>\n",
       "      <td>0.239122</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.158167</td>\n",
       "      <td>0.137560</td>\n",
       "      <td>0.771057</td>\n",
       "      <td>0.417548</td>\n",
       "      <td>0.295018</td>\n",
       "      <td>1.007584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant  RNN_model_1  RNN_model_2  RNN_model_3  RNN_model_4  RNN_model_5  \\\n",
       "634     P34V     0.122858     0.834310     0.150564     0.123836     0.780246   \n",
       "625     P34I     0.126094     0.834952     0.151882     0.117739     0.777460   \n",
       "636     P34L     0.139693     0.848730     0.176091     0.144827     0.786397   \n",
       "626     P34T     0.131012     0.824373     0.152367     0.133915     0.766578   \n",
       "620     P34D     0.112605     0.790250     0.136292     0.111777     0.756968   \n",
       "...      ...          ...          ...          ...          ...          ...   \n",
       "1854   D103P     0.252813     0.833864     0.165538     0.211090     0.786249   \n",
       "10364  Y610K     0.199439     0.755065     0.113587     0.167564     0.765987   \n",
       "7230   R444I     0.304534     0.871377     0.200796     0.220550     0.806391   \n",
       "7086   L436R     0.247674     0.811360     0.154619     0.181429     0.775458   \n",
       "389     G21I     0.239122     0.781832     0.158167     0.137560     0.771057   \n",
       "\n",
       "       Mean_DMS    SD_DMS  Alpha_Score  \n",
       "634    0.402363  0.331203     1.064769  \n",
       "625    0.401625  0.331030     1.063685  \n",
       "636    0.419148  0.326140     1.071428  \n",
       "626    0.401649  0.322160     1.045969  \n",
       "620    0.381578  0.320386     1.022350  \n",
       "...         ...       ...          ...  \n",
       "1854   0.449911  0.295735     1.041380  \n",
       "10364  0.400329  0.295398     0.991125  \n",
       "7230   0.480730  0.295216     1.071161  \n",
       "7086   0.434108  0.295146     1.024401  \n",
       "389    0.417548  0.295018     1.007584  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learn_df.sort_values(\"SD_DMS\", ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mutants = list(active_learn_df.sort_values(\"Alpha_Score\", ascending=False).head(100)[\"mutant\"])\n",
    "query_mutants_str = \"\\n\".join(query_mutants)\n",
    "with open(\"query2_run2_mutant.txt\", 'w') as file:\n",
    "    file.write(query_mutants_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 11324/11324 [00:08<00:00, 1367.30it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mutant_list = list(df_test[\"mutant\"])\n",
    "test = ProteinTestESMDataset(test_mutant_list, \"esm_embeddings_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_test_predicts_full = []\n",
    "for i in range(5):\n",
    "    RNN_preds = RNN_predict(RNN_ensemble[i], test)\n",
    "    ensemble_test_predicts_full.append(RNN_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>DMS_score_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1D</td>\n",
       "      <td>0.113374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V1Y</td>\n",
       "      <td>0.113965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V1C</td>\n",
       "      <td>0.114098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1A</td>\n",
       "      <td>0.122715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V1E</td>\n",
       "      <td>0.117648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11319</th>\n",
       "      <td>P655S</td>\n",
       "      <td>0.114440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11320</th>\n",
       "      <td>P655T</td>\n",
       "      <td>0.114683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>P655V</td>\n",
       "      <td>0.116050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>P655A</td>\n",
       "      <td>0.119691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>P655W</td>\n",
       "      <td>0.115947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11324 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant  DMS_score_predicted\n",
       "0        V1D             0.113374\n",
       "1        V1Y             0.113965\n",
       "2        V1C             0.114098\n",
       "3        V1A             0.122715\n",
       "4        V1E             0.117648\n",
       "...      ...                  ...\n",
       "11319  P655S             0.114440\n",
       "11320  P655T             0.114683\n",
       "11321  P655V             0.116050\n",
       "11322  P655A             0.119691\n",
       "11323  P655W             0.115947\n",
       "\n",
       "[11324 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_RNN_preds = RNN_predict(RNN_ensemble[0], test)\n",
    "best_RNN_ensemble_pred_df = pd.DataFrame({'mutant': test_mutant_list, 'DMS_score_predicted': best_RNN_preds})\n",
    "best_RNN_ensemble_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_RNN_ensemble_pred_df.to_csv(\"RNN_red_from_ensemble_predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11324"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RNN_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMS_Score_test, _ = calc_mean_and_sd(ensemble_test_predicts_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11324 11324\n"
     ]
    }
   ],
   "source": [
    "print(len(DMS_Score_test), len(test_mutant_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutant</th>\n",
       "      <th>DMS_score_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1D</td>\n",
       "      <td>0.168579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V1Y</td>\n",
       "      <td>0.186914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V1C</td>\n",
       "      <td>0.169218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1A</td>\n",
       "      <td>0.232668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V1E</td>\n",
       "      <td>0.202389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11319</th>\n",
       "      <td>P655S</td>\n",
       "      <td>0.150979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11320</th>\n",
       "      <td>P655T</td>\n",
       "      <td>0.163953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>P655V</td>\n",
       "      <td>0.187040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>P655A</td>\n",
       "      <td>0.208302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>P655W</td>\n",
       "      <td>0.185119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11324 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutant  DMS_score_predicted\n",
       "0        V1D             0.168579\n",
       "1        V1Y             0.186914\n",
       "2        V1C             0.169218\n",
       "3        V1A             0.232668\n",
       "4        V1E             0.202389\n",
       "...      ...                  ...\n",
       "11319  P655S             0.150979\n",
       "11320  P655T             0.163953\n",
       "11321  P655V             0.187040\n",
       "11322  P655A             0.208302\n",
       "11323  P655W             0.185119\n",
       "\n",
       "[11324 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_ensemble_pred_df = pd.DataFrame({'mutant': test_mutant_list, 'DMS_score_predicted': DMS_Score_test})\n",
    "RNN_ensemble_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_ensemble_pred_df.to_csv(\"RNN_5_query2_upd_ensemble_predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mutant  DMS_score_predicted\n",
      "556     R30F             0.332197\n",
      "553     R30Y             0.309540\n",
      "540     S29F             0.309469\n",
      "597     S32L             0.308339\n",
      "601     S32F             0.307381\n",
      "550     S29Y             0.304780\n",
      "10142  R593A             0.303232\n",
      "3390   K198L             0.303015\n",
      "560     R30L             0.300149\n",
      "539     S29L             0.299632\n",
      "['R30F', 'R30Y', 'S29F', 'S32L', 'S32F', 'S29Y', 'R593A', 'K198L', 'R30L', 'S29L']\n"
     ]
    }
   ],
   "source": [
    "top_ten_mutants = RNN_ensemble_pred_df.sort_values('DMS_score_predicted', ascending=False).head(10)\n",
    "print(RNN_ensemble_pred_df.sort_values('DMS_score_predicted', ascending=False).head(10))\n",
    "top_ten_list = list(top_ten_mutants[\"mutant\"])\n",
    "print(top_ten_list)\n",
    "top_ten_string = \"\\n\".join(top_ten_list)\n",
    "with open(\"top10query2_2.txt\", 'w') as file:\n",
    "    file.write(top_ten_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutant_list = list(df_test[\"mutant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 11324/11324 [00:25<00:00, 437.78it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_list = []\n",
    "for mutant in tqdm(mutant_list, desc='Loading esm embeddings'):\n",
    "    name = mutant\n",
    "    emb_file = os.path.join(\"esm_embeddings_test\", f'{name}.pt')\n",
    "    emb_test = torch.load(emb_file)['mean_representations'][33]\n",
    "    emb_list.append(emb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading esm embeddings: 100%|██████████| 11324/11324 [00:13<00:00, 829.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0269, -0.0786,  0.0247,  ..., -0.1781,  0.0061,  0.1503],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ProteinTestESMDataset(mutant_list, \"esm_embeddings_test\")\n",
    "test.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsS5lPPIM8k5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
